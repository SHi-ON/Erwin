\documentclass[12pt]{report}

\usepackage[utf8]{inputenc}

\usepackage[dvipsnames]{xcolor}

\usepackage{array}
\newcolumntype{L}{>{\centering\arraybackslash}m{3cm}}

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{caption}

\usepackage{graphicx}
\usepackage{csquotes}

\renewcommand{\bibname}{References}


\title{Advanced Machine Learning\\Final Report}

\author{Shayan Amani}

\date{\today}

\begin{document}
\maketitle

\chapter{Problem Definition}
Presenting an optimum policy to control a specific invasive species, Frangula alnus so called, glossy buckthorn. For the sake of simplicity, a grid environment of 3x3 has been considered. 

Simulator-generated samples of history of measures and consequences are provided in a dataset. Dataset is consisted of the following columns:
\begin{itemize}
    \item \textbf{V1}: year number of the sample. from 1 to 200 which means simulator run through a span of 200 years.
    \item: \textbf{pop\_1} to \textbf{pop\_9}: population of the plant in each cells.
    \item: \textbf{sbank\_1} to \textbf{sbank\_9}: number of plant's seeds in each cells.
    \item: \textbf{actions}: taken action for each year of samples.
    \item: \textbf{rewards}: observed reward after applying the action.
\end{itemize}

\chapter{Previous Works}
We have two different point of view to look at this problem. Firstly a batch reinforcement learning problem and secondly 

\chapter{Proposed Methods}
As a problem in which the model (transition probabilities) is not provided, model-free design is what may first come to mind. Different options were available to choose among and each have their own pros and cons. From least squares temporal difference (LSTD) or least squares policy iteration (LSPI) algorithms to more sophisticated one such as asynchronous advantage actor-critic (A3C) \cite{Mnih2016b} are all available

\chapter{Proposed and Implemented Method}
Considering this sample-based problem I have picked and implemented a deep reinforcement learning approach, namely Deep Q-Networks (DQN). DQN is mostly relied on a stabilization technique with neural networks, called Experience replay.
 The elements of this method are explained in the following sections.

This needs to be mentioned that I have examined different method which are excelled in the literature. One of these methods is Deep Deterministic Policy Gradient (DDPG). According to \citet{} such a methods are more applicable to continues action space problems which is not the case here. I have provided a one-to-one comparison chart in Table \ref{tab:dqn vs. ddpg} to discuss particular aspects of these two methods.


\begin{table}
    \centering
    \begin{tabular}{|c|L|L|L|L|}
        \hline
        \textbf{Method}  &    \textbf{Model Dependence}    &   \textbf{Policy Dependence}   &   \textbf{Action Continuity}  & \textbf{Parent Algorithm(s)}\\
        \hline
        \hline
        DQN &   model-free  &   on-policy   &   discrete    &   Q-learning, DL\\
        \hline
        DDPG    &   model-free  &   on-policy   &   continuous  &   Actor-critic\\
        \hline
    \end{tabular}
    \caption{DRL methods comparison chart}
    \label{tab:dqn vs. ddpg}
\end{table}

\section{Algorithm}
The whole algorithm is described in Algorithm \ref{alg:DQN}.

\begin{algorithm}[H]
\caption{DQN algorithm in batch mode}
\label{alg:DQN}
\begin{algorithmic}[1]
    \State Init D \Comment{\textcolor{BlueViolet}{replay memory}}
    \State Init Q \Comment{\textcolor{BlueViolet}{Q-table w/ random weights}}
    \State \textcolor{OrangeRed}{Get} or Observe $s_0$ \Comment{\textcolor{BlueViolet}{the initial state}}
    \For{each episode}
        \For{samples in each episode}
            \State \textit{\textcolor{OrangeRed}{- skip $\epsilon$-greedy exploration}}
            \State $a = argmax_a Q(s,a)$
            \State \textit{\textcolor{OrangeRed}{- skip applying action $a$ to the environment}}
            \rlap{\smash{$\left.\begin{array}{@{}c@{}}\\{}\\{}\end{array}\color{BlueViolet}\right\}%
              \color{BlueViolet} experience$}}
            \State \textcolor{OrangeRed}{Get} or Observe $r, s'$
            \State Store the \textbf{experience} $<s, a, r, s'>$ in replay memory $D$
            \State \textit{Random} \textbf{sampling} from $D$ $<ss, aa, rr, ss'>$  \Comment{\textcolor{BlueViolet}{[mini]-batch}}
            \If{$ss' \neq $ terminal state} \Comment{\textcolor{BlueViolet}{target for each mini-batch}}
                \State $tt = rr + \gamma max_{aa'} Q(ss', aa')$
            \Else
                \State $tt = rr$
            \EndIf
            \State Train the network
        \EndFor    
    \EndFor
\end{algorithmic}
\end{algorithm}

\section{Experience Replay}
DQN is mostly relied on a stabilization technique with neural networks, called Experience replay. Experience replay is a memory abstraction which $n$ past samples are stored in this memory and then we use it later to draw samples from it on a randomized basis. 



% \includegraphics[width=1\columnwidth]{elements.png}

\bibliographystyle{nips}
\bibliography{library}


\end{document}