\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2019

% ready for submission
% \usepackage{neurips_2019}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2019}

% to compile a camera-ready version, add the [final] option, e.g.:
%  \usepackage[final]{neurips_2019}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

% SHi-ON packages
% argmin argmax begin
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
% argmin argmax end
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}        % norm using amsmath


\title{Adversarial State Aggregation}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
Authors \\ %\thanks{Use footnote for providing further information
%    about author (webpage, alternative address)---\emph{not} for acknowledging
%    funding agencies.}%
Department of Computer Science\\
University of New Hampshire \\
\texttt{\{authors\}@cs.unh.edu} \\
% examples of more authors
% \And
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email} \\
% \AND
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email} \\
% \And
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email} \\
% \And
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email} \\
}

\begin{document}

    \maketitle

    \begin{abstract}
        Promising advances in deep reinforcement learning suggests several application to the algorithms. However
        robustness of these algorithms remains doubts to trust these algorithms in practice. In this paper we study
        resistance of the state-of-the-art algorithms to environment dynamics. We then show the amount of robustness
        we can achieve as a guarantee of performance in presence of those changes.
    \end{abstract}


    \section{Introduction}
    Real-world problems embody high-dimensional state spaces where in computational issues
    arise.
    Various approximation methods are proposed in the literature to reduce the resulted value function
    dimensionality \cite{Sutton1998, Francois-Lavet2018}.
    State aggregation is one of the simplest and most analytically transparent state representation approximation.

    Although state aggregation may not generalize as well as neural network-based methods, the techniques has both
    analytical and practical advantages \cite{Lagoudakis2003}.
    In comparison, state aggregation takes less computation power to calculate representative features.
    Moreover, analysis and troubleshooting in a state aggregation process is more obvious than a black-box neural
    network.
    Approximation is a corner stone in all successful RL methods to hold a "good" mapping between a large state
    space to a much smaller one where in value function is computable or at least feasible.
    These "good" approximators can end up with "good" features to represent the projected state space.

    The cross-fertilization in using state aggregation and deep neural networks in solving immense state
    space problems in RL is also profound.
    While the newly emerged methods are growing at a staggering pace, a parallel area of research has been formed
    wherein a variety of techniques have been utilized to make these methods contravene their promised functionality.
    These efforts would be served to develop robustness in those spectacular models and reduce their
    susceptibility to data tampering or dynamics of the environment.

    In this work, we scrutinize state-of-the-art deep RL algorithms resiliency to environment parameters alterations
    and then introduced our approach to overcome those. Then a guarantee of performance to these attacks is introduced.

    \section{Markov Decision Processes}
    Reinforcement learning problems can be formulated as an Markov decision process (MDP)\cite{Puterman1994}.
    Discrete discounted reward criterion is the center of focus for ease of exposition in this paper.
    For an environment and a decision maker, state space $S$ and action space $A$ are defined, respectively.
    The transition probabilities $T$ describe how likely is the next state $s'$ by taking a specific action $a$ by
    the agent in a certain state $s$ which will receive $r$ as reward.
    For a taken action $a_i$ in the state $s_i$, the decision maker will be ended up in state $s_{i+1}$ by the
    transition probabilities $P : S \times A \times S \rightarrow [0, 1]$ and wil receive reward $r_{i+1}$ from the
    rewards $R : S \times A \rightarrow \mathbb{R}$: \\
    \begin{equation}
        s_{i+1} = T(s_{i+1}|s_i, a_i)
    \end{equation}

    \begin{equation}
        r_{i+1} = R(s_i, a_i)
    \end{equation}


    \section{State Aggregation}
    Value function approximation alleviates the computational hurdle of a large state-space problem.
    We introduce an approximate policy iteration method to solve an aggregate problem.
    State aggregation is an approximation approach to make a problem more tractable by a reduction in dimensionality in
    state space \footnote{aggregation in action space although is possible, has less significance comparatively.}.
    State aggregation, in particular, is a parametric feature-based approximation where the features are membership
    functions of 0-1 form.
    Aggregation groups similar states together, so called representative states, which lets the \textit{aggregate}
    problem to be solved by an exact method.
    The computed value function from the exact solution of the aggregate problem can be then used in the original
    problem.


    Now we formulate the aggregated problem by introducing the dynamics of the aggregate problem.
    The transition probabilities between two representative states:

    \begin{equation}
        \hat{P}_{s s^\prime}(a)=\sum_{j=1}^{n} P_{s j}(a) \phi_{j y},
    \end{equation}

    where $p_ {s j}$ is the transition probabilities from a representative state, $s$, to an original state, $j$
    and $\phi_{j s^\prime}$ is the aggregation probabilities from an original state, $j$, to a representative state,
    $s^\prime$.
    The rewards are defined as:

    \begin{equation}
        \hat{r}(s, a)=\sum_{j=1}^{n} P_{s j}(a) r(s, a, j),
    \end{equation}

    where $\hat{r}(s, a)$ is the received reward at state $s$ by taking action $a$.


    Aggregation then helps us to derive the value function.
    Approximated optimal value function is a weighted sum of optimal rewards and can be computed by
    equation\ref{approx val func} where $r_{s^\prime}^{\ast}$ is optimal reward at represented state $s$.

    \begin{equation}
        \label{approx val func}
        \tilde{V}(j)=\sum_{s^\prime \in \mathcal{U}} \phi_{j s^\prime} r_{s^\prime}^{\ast},
    \end{equation}

    In this paper, we lay down the aggregation framework and then solve the aggregate problem by a tabular method
    such as value iteration (VI), policy iteration (PI).

    \section{Method}
    For the sake of clarity, we elaborate on some preliminary definitions here to have a consistent set of notations
    to the end of this paper. We use model-free LSPI and LSTDQ \citep{Lagoudakis2003} to update and
    evaluate the policy of actions, respectively. CRAAM, on the other hand, as a exact method tries to approximate
    the value function by dynamic programming optimization.

    We developed an aggregation scheme to lower the state space dimensionality by assigning same action to the state
    in a neighbourhood, so called nearest neighbour/ piecewise linear aggregation.
    We then solve directly the lower dimension problem which relatively takes less computation.

    \subsection{Aggregation}
    Find the best policy then find optimal number of buckets.
    Train the agent and retrieve the best policy then by running the agent with the best policy \textit{generate}
    samples.
    These collected samples are not based on a random policy but the best trained policy (baseline).
    Then we can test our hypothesis on the collected samples to see how discretization affect another trained agent.
    One way to measure the effect of discretization is average reward received by the trained agent.


    \subsection{Adversarial attacks}
    The designed attack may have different levels of access to the underlying training policy which have motivated
    the community to classify adversarial attacks into two distinct categories, namely \textit{white-box} or
    \textit{black-box} attacks. White-box attacks illustrating the scenario in which the... Adversarial attacks fall
    in either one of the following categories based upon the stage, which in the adversary perturbs the model:
    \begin{itemize}
        \item Train time: Data poisoning.
        \item Evaluation time: Adversarial examples.
        \item Deploy time: Black-box attacks.
    \end{itemize}


    \subsection{Objective}
    The aggregate problem is stochastic even if the original problem is deterministic. Once we lay out the aggregate
    problem framework we can solve the problem by any exact methods either value- or policy-space solutions.


    The ultimate goal is deceiving the model either in choosing the worst possible action in an RL task or misclassifying an input in a classification task in a way which is not perceptible for unaided eye. In terms of a Markovian RL task we show how minimum set of worst-case actions are needed to result in the minimum final reward. We try to show that the model is not robust to other configuration of each benchmarks with different dynamics.

    The main goal of adversarial training and examples can be expressed briefly in form of the following equation:
    %    \begin{equation}
    %        \begin{aligned}
    %            & \underset{\theta}{\min}
    %            & & \sum_{x,y \in S} \underset{\delta \in \Delta}{\max}
    %            & & & \textbb{Loss}(x + \delta, y; \theta)
    %        \end{aligned}
    %    \end{equation}


    \section{Experiment}
    We apply the approach to two MDPs proposed by\cite{Strehl2004}. \textit{RiverSwim} an MDP consists of six states and \textit{SixArms} one with seven states. In \textit{RiverSwim} flow of the river is to left and the swimmer picks probabilistically equal either state 1 or 2 as the start point. All states but two terminal states have zero reward to land at while the right most one rewards substantially higher than the other one. The agent in SixArms selects between six distinct actions which pulls different arms of a multi-armed bandit. By pulling an arm the agent traverses to a new state. Although the agent does not obtain reward by pulling the arms but going to the connected state to that arm is highly rewarding in which the transition probability is in inverse relationship with the reward value.

    As both problems favor keeping the agent in states with lower reward, exploration is paramount to maximize the accumulated reward. This is also confirmed by the authors in \cite{Strehl2004} that the agent will fail to learn when a rudimentary $\epsilon$-greedy algorithm.

    Our implementation uses Deep Q-Network (DQN)\cite{Mnih2015} as the policy optimizer. OpenAI Gym \cite{Brockman2016} is the testbed in the benchmarks in this paper. In training phase, models are trained in the environment. Online methods such as DQN learn by interacting directly in the environment, however, CRAAM

    % \begin{figure}
    %     \centering
    %     \includegraphics[width=1\columnwidth]{minutia/cartpole-v1_batchsize-32_episodes-100_full-step.png}
    %     \caption{Average reward of DQN training to 100 episodes in cartpole.}
    %     \label{fig:my_label}
    % \end{figure}



    \section{Future Works}
    We developed the presented approach with having reproducibility in mind. It could be served as a launching pad for the future contributors who want to surf this newly emerged field deeper. Future ideas to pursue is a generative model to be able to generate adversaries from a network.

    \section*{Appendix}

    \subsection{Implementation note}
    * Reshape is needed after getting each state from Gym models.




    \bibliographystyle{plain}
    \bibliography{library}
    % \section*{References}



    % [1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms for
    % connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and T.K.\ Leen
    % (eds.), {\it Advances in Neural Information Processing Systems 7},
    % pp.\ 609--616. Cambridge, MA: MIT Press.

    % [2] Bower, J.M.\ \& Beeman, D.\ (1995) {\it The Book of GENESIS: Exploring
    %   Realistic Neural Models with the GEneral NEural SImulation System.}  New York:
    % TELOS/Springer--Verlag.

    % [3] Hasselmo, M.E., Schnell, E.\ \& Barkai, E.\ (1995) Dynamics of learning and
    % recall at excitatory recurrent synapses and cholinergic modulation in rat
    % hippocampal region CA3. {\it Journal of Neuroscience} {\bf 15}(7):5249-5262.

\end{document}
