\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2019

% ready for submission
% \usepackage{neurips_2019}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2019}

% to compile a camera-ready version, add the [final] option, e.g.:
%  \usepackage[final]{neurips_2019}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

% SHi-ON packages
% argmin argmax begin
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
% argmin argmax end
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}        % norm using amsmath


\title{Distributional State Aggregation}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
    Authors \\ %\thanks{Use footnote for providing further information
%    about author (webpage, alternative address)---\emph{not} for acknowledging
%    funding agencies.}%
    Department of Computer Science\\
    University of New Hampshire \\
    \texttt{\{authors\}@cs.unh.edu} \\
% examples of more authors
% \And
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email} \\
% \AND
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email} \\
% \And
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email} \\
% \And
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email} \\
}

\begin{document}

    \maketitle

    \begin{abstract}
        We present a state aggregation algorithm and a learning algorithm based on the former to solve a stochastic
        problem.
        The presented aggregation algorithm uses range of collected samples to provide the proper aggregation
        \textit{buckets}.
        Our approach is a batch learning by which we detect the \textit{splitting policy} using inter-quartile range
        of data for each feature.
        We then show the amount of robustness we can achieve as a guarantee of performance in presence of those
        aggregation policies.
    \end{abstract}


    \section{Introduction}
    Real-world problems embody high-dimensional state spaces where in computational issues
    arise.
    Various approximation methods are proposed in the literature to reduce the resulted value function
    dimensionality\cite{Sutton1998, Francois-Lavet2018}.
    State aggregation is one of the simplest and most analytically transparent state representation approximation.

    Although state aggregation may not generalize as well as neural network-based methods, the techniques has both
    analytical and practical advantages\cite{Lagoudakis2003}.
    In comparison, state aggregation takes less computation power to calculate representative features.
    Moreover, analysis and troubleshooting in a state aggregation process is more obvious than a black-box neural
    network.
    Approximation is a corner stone in all successful RL methods to hold a "good" mapping between a large state
    space to a much smaller one where in value function is computable or at least feasible.
    These "good" approximators can end up with "good" features to represent the projected state space.

    The cross-fertilization in using state aggregation and deep neural networks in solving immense state
    space problems in RL is also profound.
    While the newly emerged methods are growing at a staggering pace, a parallel area of research has been formed
    wherein a variety of techniques have been utilized to make these methods contravene their promised functionality.
    These efforts would be served to develop robustness in those spectacular models and reduce their
    susceptibility to data tampering or dynamics of the environment.


    \section{Markov Decision Processes}
    Reinforcement learning problems can be formulated using a Markov Decision Processes (MDP)
    framework\cite{Puterman1994, Bertsekas}.
    MDPs facilitate handling sequential decision-making under uncertainty.
    Discrete discounted reward criterion is the center of focus for ease of exposition in this paper.
    For an environment and a decision maker, state space $S$ and action space $A$ are defined, respectively.
    The transition probabilities $T$ describe how likely is the next state $s'$ by taking a specific action $a$ by
    the agent in a certain state $s$ which will receive $r$ as reward.
    For a taken action $a_i$ in the state $s_i$, the decision maker will be ended up in state $s_{i+1}$ by the
    transition probabilities $P : S \times A \times S \rightarrow [0, 1]$ and wil receive reward $r_{i+1}$ from the
    rewards $R : S \times A \rightarrow \mathbb{R}$: \\
    \begin{equation}
        s_{i+1} = T(s_{i+1}|s_i, a_i)
    \end{equation}

    \begin{equation}
        r_{i+1} = R(s_i, a_i)
    \end{equation}


    \section{State Aggregation}
    Value function approximation alleviates the computational hurdle of a large state-space problem.
    We introduce an approximate policy iteration method to solve an aggregate problem.
    State aggregation is an approximation approach to make a problem more tractable by a reduction in dimensionality in
    state space \footnote{aggre
    gation in action space although is possible, has less significance comparatively.}.
    State aggregation, in particular, is a parametric feature-based approximation where the features are membership
    functions of 0-1 form.
    Aggregation groups similar states together, so called representative states, which lets the \textit{aggregate}
    problem to be solved by an exact method.
    The computed value function from the exact solution of the aggregate problem can be then used in the original
    problem.


    Now we formulate the aggregated problem by introducing the dynamics of the aggregate problem.
    The transition probabilities between
    two representative states:

    \begin{equation}
        \hat{P}_{s s^\prime}(a)=\sum_{j=1}^{n} P_{s j}(a) \phi_{j y},
    \end{equation}

    where $p_ {s j}$ is the transition probabilities from a representative state, $s$, to an original state, $j$
    and $\phi_{j s^\prime}$ is the aggregation probabilities from an original state, $j$, to a representative state,
    $s^\prime$.
    The rewards are defined as:

    \begin{equation}
        \hat{r}(s, a)=\sum_{j=1}^{n} P_{s j}(a) r(s, a, j),
    \end{equation}

    where $\hat{r}(s, a)$ is the received reward at state $s$ by taking action $a$.


    Aggregation then helps us to derive the value function.
    Approximated optimal value function is a weighted sum of optimal rewards and can be computed by
    equation\ref{approx val func} where $r_{s^\prime}^{\ast}$ is optimal reward at represented state $s$.

    \begin{equation}
        \label{approx val func}
        \tilde{V}(j)=\sum_{s^\prime \in \mathcal{U}} \phi_{j s^\prime} r_{s^\prime}^{\ast},
    \end{equation}

    In this paper, we lay down the aggregation framework and then solve the aggregate problem by a tabular method
    such as value iteration (VI), policy iteration (PI).


    \section{Method}
    Splitting the observations into neighborhoods of similar observations is one way to interpret an aggregation
    problem.
    Inspired by histograms and the statistics behind finding the proper number of bins for a given distribution, we
    build our proposal on top of such basics.
    Finding proper number of bins for a given distribution could be calculated by Freedman-Diaconis rule. As the rule
    states:

    \begin{equation}
        |S_{agg}| = 2 \frac{\operatorname{IQR}(x)}{\sqrt[3]{n}}
    \end{equation}

    where n is number of observations and $IQR(x)$ is the inter-quartile range of data.f

    By having the inter-quartile range of data for each feature, a discretization policy can be calculated.


    For the sake of clarity, we elaborate on some preliminary definitions here to have a consistent set of notations
    to the end of this paper. We use model-free LSPI and LSTDQ \citep{Lagoudakis2003} to update and
    evaluate the policy of actions, respectively.

    We developed an aggregation scheme to lower the state space dimensionality by assigning same action to the state
    in a neighbourhood, so called nearest neighbour/ piecewise linear aggregation.
    We then solve directly the lower dimension problem which relatively takes less computation.

    The aggregate problem is stochastic even if the original problem is deterministic.
    Once we lay out the aggregate problem framework we can solve the problem by any exact methods either value- or
    policy-space solutions.


    The objective is finding an optimal aggregate model which falls in the span of spectrum from a fine-grid model
    to a rigid aggregate model.
    Our presented method calculates a discretization policy for which we calculate a feasible value function.
    Return of a policy shows the quality of the gauged policy.
    Hence, by calculating the accumulative return (eq.\ref{eq:return}) over run of the \textit{true} model and in a
    comparison with the original return we can measure the improvement.

    \begin{equation}
        R = \sum_{k=0}^{\infty} \gamma^{k} r_{k+1}. \label{eq:return}
    \end{equation}


    \subsection{Aggregation}
    The original states with the closest \textit{calculated} values forgather to build aggregate states.
    For this purpose, state values are clustered by K-means analysis to calculate the relevant aggregate states.
    State aggregation merge similar transitions based on a triple $<s, s', a>$ where $s$ indicates the starting state,
    $s'$ is the ending state, and the take action in that transition represented by $a$.
    In the process of stacking similar transitions, we here presume all inbound transitions as outbound transition of
    the ending state.
    This assumption render all transitions as outbound ones.


    \section{Experiment}
    We apply the approach to two MDPs proposed by\cite{Strehl2004}.
    \textit{RiverSwim} an MDP consists of six states and \textit{SixArms} one with seven states.
    In \textit{RiverSwim} flow of the river is to left and the swimmer picks probabilistically equal either state 1
    or 2 as the start point.
    All states but two terminal states have zero reward to land at while the right most one rewards substantially
    higher than the other one.
    The agent in SixArms selects between six distinct actions which pulls different arms of a multi-armed bandit.
    By pulling an arm the agent traverses to a new state.
    Although the agent does not obtain reward by pulling the arms but going to the connected state to that arm is
    highly rewarding in which the transition probability is in inverse relationship with the reward value.

    As both problems favor keeping the agent in states with lower reward, exploration is paramount to maximize the
    accumulated reward.
    This is also confirmed by the authors in \cite{Strehl2004} that the agent will fail to learn when a rudimentary
    $\epsilon$-greedy algorithm.

    We use a randomized policy to simulate trajectories and collect samples to attenuate bias in action selection
    process.
    As it has shown the literature, $\epsilon$-greedy has a bias in solving exploration-exploitation dilemma, we
    explore the environment by following a randomized policy with a uniform distribution.

    \section{Future Works}
    We developed the presented approach with having reproducibility in mind. It could be served as a launching pad for the future contributors who want to surf this newly emerged field deeper. Future ideas to pursue is a generative model to be able to generate adversaries from a network.

    \section*{Appendix}

    \subsection{Implementation note}
    * Reshape is needed after getting each state from Gym models.


    \bibliographystyle{plain}
    \bibliography{library}
    % \section*{References}


    % [1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms for
    % connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and T.K.\ Leen
    % (eds.), {\it Advances in Neural Information Processing Systems 7},
    % pp.\ 609--616. Cambridge, MA: MIT Press.

    % [2] Bower, J.M.\ \& Beeman, D.\ (1995) {\it The Book of GENESIS: Exploring
    %   Realistic Neural Models with the GEneral NEural SImulation System.}  New York:
    % TELOS/Springer--Verlag.

    % [3] Hasselmo, M.E., Schnell, E.\ \& Barkai, E.\ (1995) Dynamics of learning and
    % recall at excitatory recurrent synapses and cholinergic modulation in rat
    % hippocampal region CA3. {\it Journal of Neuroscience} {\bf 15}(7):5249-5262.

\end{document}
