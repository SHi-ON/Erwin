\documentclass[letterpaper,12pt]{article}
\usepackage[utf8]{inputenc}

% CSL style files for Mendeley and other referencing tools:
% https://github.com/citation-style-language/styles

%
% For alternative styles, see the biblatex manual:
% http://mirrors.ctan.org/macros/latex/contrib/biblatex/doc/biblatex.pdf
%
% The 'verbose' family of styles produces full citations in footnotes, 
% with and a variety of options for ibidem abbreviations.
%

\usepackage{graphicx}
\usepackage{csquotes}
\usepackage{amsfonts} % \mathbb in equations
% \usepackage[style=verbose-ibid,backend=bibtex]{biblatex}

\usepackage{cite}
% \usepackage{natbib}

\usepackage[dvipsnames]{xcolor}

\usepackage{array}
\newcolumntype{L}{>{\centering\arraybackslash}m{3cm}}

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{caption}

\usepackage{amsmath} % \mathbb and \aligned
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}


% \title{Markovian Approach to Data-driven Problems}
\title{Robust Deluding Adversarial Examples}

\author{Shayan Amani}

\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
We propose a new algorithm to create robust adversarial examples to fool the decision-making models. We then show the amount of robustness we can achieve. Finally, an evaluation is provided to evaluate our proposed method.
\end{abstract}

\section{Introduction}
Deep reinforcement learning (Deep RL) achievements \cite{Mnih2013PlayingLearning} absorbed attentions to propose variety of algorithms \cite{Haarnoja2018SoftActor, Espeholt2018Impala:Architectures, Pong2018TemporalControl}. The promising methods ---similar to other aspects of machine learning--- have its own uncertainties which need to be addressed before the implementation. Robustness---as a property---elevates assurance about method performance and reliability to the foreground. We, in this paper, try to scrutinize state-of-the-art deep RL algorithms from a robustness point of view utilizing adversarial attacks in order to come up with a guarantee that we make sure which definitely works in the claimed criterion.

While the newly emerged methods are growing at a staggering pace, a parallel area of research has been formed wherein a variety of techniques have been utilized to make these methods contravene their promised functionality. These efforts would be served to develop robustness in those spectacular models and reduce their susceptibility to data tampering or dynamics of the environment.

\section{Related Work}
\citet{Goodfellow2014} stated and consequently depicted that the root cause of vulnerability to adversarial examples laid behind the piecewise linearity attributes of neural network models.

\section{Definition}
For the sake of clarity, we elaborate on some preliminary definitions here to have a consistent set of notations to the end of this paper.

\subsection{Adversarial attacks}
The designed attack may have different levels of access to the underlying training policy which have motivated the community to classify adversarial attacks into two distinct categories, namely \textit{white-box} or \textit{black-box} attacks. White-box attacks illustrating the scenario in which the... Adversarial attacks fall in either one of the following categories based upon the stage which in the adversary perturbs the model:
\begin{itemize}
    \item Train time: Data poisoning.
    \item Evaluation time: Adversarial examples.
    \item Deploy time: Black-box attacks.
\end{itemize}


\section{Devising Adversarial Attacks}
Neural networks are vulnerable to adversarial attacks either in supervised learning or reinforcement learning settings \cite{Huang2017}. Such weakness could possibly result in unrecognizable differences for the unaided eye in the presence of perturbation which leads to misclassification for neural network based model \cite{Szegedy2014, Goodfellow2014}. 

\subsection{Objective}
The ultimate goal is deceiving the model either in choosing the worst possible action in an RL task or misclassifying an input in a classification task in a way which is not perceptible for unaided eye. In terms of a Markovian RL task we show how minimum set of worst-case actions are needed to result in the minimum final reward. We try to show that the model is not robust to other configuration of each benchmarks with different dynamics.


\subsection{Attacks Based upon Fast Gradient Sign Method}

The main goal of adversarial training and examples can be expressed briefly in form of the following equation:
\begin{equation}
\begin{aligned}
& \underset{\theta}{\min}
& & \sum_{x,y \in S} \underset{\delta \in \Delta}{\max}
& & & \textbb{Loss}(x + \delta, y; \theta)
\end{aligned}
\end{equation}

where in we trying to find an adversarial example (inner maximization) in order to adversarially train a robust model (outer minimization).

We define $s_\eta$ as the perturbed observation:
\begin{equation}
    s_\eta = (1 + \epsilon) s
\end{equation}


Let $s_\eta$ be the perturbed observation and $s_c$ scaled observation, so we are trying to find the following:
\begin{equation}
\begin{aligned}
& \underset{s}{\argmin}
& & {\norm{s_\eta - s_c}}
\end{aligned}
\end{equation}
which is the state observation that have the minimum distance between real-world [scaled] observation and the perturbed one.

In order to expand the applicability of designed adversarial attacks to an assortment of algorithms [models], we rely on \textit{transferability property} of RL-specific adversarial attacks \cite{Szegedy2014, Papernot2016, Goodfellow2014a}. \citet{Huang2017} clearly assert [or asserts?!] attacks are deployable to a group of trained RL models aimed at the same task.  

\subsection{Reward Perturbation Attacks}
In an adversarial environment we try to perturb reward to interfere in learning process of the RL agent. We considered this attack also in scenarios with a single reward which we will show the process later.

\section{Adversarial attacks on different methodologies}
Here we review adversarial attacks on a variety of value function based state-of-art algorithms proposed by the community so far. As \citet{Pattanaik2018} pointed out, the concept of achieving a successful adversarial attack is unalike in reinforcement learning tasks compared to image classification tasks. Worst \textit{action} is tangible and feasible but not worst \textit{image}.

\subsection{Deep Q Learning (DQN)}
Utilizing experience replay and the leveraging target network, DQN take advantage of them to gain more stability \cite{Pattanaik2018}.

\section{Experiment}
In order to achieve more robustness, we need to omit loosely correlated features in training the model.

\subsection{Benchmarks}
When it comes to benchmarking, an acute shortage of a comprehensive testbed is felt. Two available evaluation frameworks are as follows:
\begin{itemize}
    \item CleverHans \cite{Papernot2016}
    \item Robust ML 
\end{itemize}
 

\section{Future Works}
We developed the presented approach with having reproducibility in mind. It could be served as a launching pad for the future contributors who want to surf this newly emerged field deeper. Future ideas to pursue is a generative model to be able to generate adversaries from a network.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{nips}
\bibliography{library}

\end{document}
