Automatically generated by Mendeley Desktop 1.19.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@misc{Devlin2018,
author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina and Google, Kristina Toutanova and Language, A I and Toutanova, Kristina},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Devlin et al. - 2018 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding(2).pdf:pdf},
month = {oct},
title = {{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}},
url = {http://arxiv.org/abs/1810.04805 https://github.com/tensorflow/tensor2tensor},
year = {2018}
}
@article{Nogueira2019,
abstract = {Recently, neural models pretrained on a language modeling task, such as ELMo (Peters et al., 2017), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et al., 2018), have achieved impressive results on various natural language processing tasks such as question-answering and natural language inference. In this paper, we describe a simple re-implementation of BERT for query-based passage re-ranking. Our system is the state of the art on the TREC-CAR dataset and the top entry in the leaderboard of the MS MARCO passage retrieval task, outperforming the previous state of the art by 27{\%} (relative) in MRR@10. The code to reproduce our results is available at https://github.com/nyu-dl/dl4marco-bert},
archivePrefix = {arXiv},
arxivId = {1901.04085},
author = {Nogueira, Rodrigo and Cho, Kyunghyun},
eprint = {1901.04085},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nogueira, Cho - 2019 - Passage Re-ranking with BERT(2).pdf:pdf},
month = {jan},
title = {{Passage Re-ranking with BERT}},
url = {http://arxiv.org/abs/1901.04085},
year = {2019}
}
@misc{bojarski2016a,
author = {Bojarski, M and Testa, D.Del and Dworakowski, D and Firner, B and Flepp, B and Goyal, P and Jackel, L D and Monfort, M and Muller, U and Zhang, J},
edition = {arXiv prep},
title = {{End to end learning for self-driving cars}}
}
@misc{finn2017a,
author = {Finn, C and Abbeel, P and Levine, S},
edition = {arXiv prep},
title = {{Model-agnostic metalearning for fast adaptation of deep networks}}
}
@misc{klambauer2017a,
author = {Klambauer, G and Unterthiner, T and Mayr, A and Hochreiter, S},
edition = {arXiv prep},
title = {{SelfNormalizing Neural Networks}}
}
@incollection{browne2012a,
author = {Browne, C B and Powley, E and Whitehouse, D and Lucas, S M and Cowling, P I and Rohlfshagen, P and Tavener, S and Perez, D and Samothrakis, S and Colton, S},
booktitle = {IEEE Trans. Comput. Intell. AI games},
number = {1},
pages = {1--43},
title = {{A survey of monte carlo tree search methods}},
volume = {4}
}
@article{samuel1959a,
author = {Samuel, A L},
journal = {IBM J. Res. Dev.},
number = {3},
pages = {210--229},
title = {{Some studies in machine learning using the game of checkers}},
volume = {3}
}
@article{lee2012a,
author = {Lee, D and Seo, H and Jung, M W},
journal = {Annu. Rev. Neurosci.},
pages = {287--308},
title = {{Neural basis of reinforcement learning and decision making}},
volume = {35}
}
@misc{bellemare2016a,
author = {Bellemare, M G and Srinivasan, S and Ostrovski, G and Schaul, T and Saxton, D and Munos, R},
edition = {arXiv prep},
title = {{Unifying Count-Based Exploration and Intrinsic Motivation}}
}
@incollection{rescorla1972a,
author = {Rescorla, R A and Wagner, A R},
booktitle = {Class. Cond. II Curr. Res. theory. 2},
pages = {64--99},
title = {{A theory of Pavlovian conditioning: Variations in the effectiveness of reinforcement and nonreinforcement}}
}
@misc{kahneman2011a,
author = {Kahneman, D},
title = {{No Title}}
}
@article{davis2017a,
author = {Davis, K.Waugh and Johanson, M and Bowling, M},
journal = {Science (80-. ).},
number = {6337},
pages = {508--513},
title = {{DeepStack: Expert-level artificial intelligence in heads-up no-limit poker}},
volume = {356}
}
@article{piketty2013a,
author = {Piketty, T},
journal = {In: IJCAI},
pages = {1025--1032},
title = {{Capital in the Twenty-First Century}},
volume = {3}
}
@article{pedregosa2011a,
author = {Pedregosa, F and Varoquaux, G and Gramfort, A and Michel, V and Thirion, B and Grisel, O and Blondel, M and Prettenhofer, P and Weiss, R and Dubourg, V},
journal = {J. Mach. Learn. Res.},
pages = {2825--2830},
title = {{Scikit-learn: Machine learning in Python}},
volume = {12}
}
@article{camerer2005a,
author = {Camerer, C and Loewenstein, G and Prelec, D},
journal = {J. Econ. Lit.},
number = {1},
pages = {9--64},
title = {{Neuroeconomics: How neuroscience can inform economics}},
volume = {43}
}
@misc{farquhar2017a,
author = {Farquhar, G and Rockt{\"{a}}schel, T and Igl, M and Whiteson, S},
edition = {arXiv prep},
title = {{TreeQN and ATreeC: Differentiable Tree Planning for Deep Reinforcement Learning}}
}
@inproceedings{morimura2010a,
author = {Morimura, T and Sugiyama, M and Kashima, H and Hachiya, H and Tanaka, T},
booktitle = {Proc. 27th Int. Conf. Mach. Learn.},
pages = {799--806},
title = {{Nonparametric return distribution approximation for reinforcement learning}},
volume = {10}
}
@techreport{Watkins1992,
abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states. This paper presents and proves in detail a convergence theorem for Q,-learning based on that outlined in Watkins (1989). We show that Q-learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where many Q values can be changed each iteration, rather than just one.},
author = {Watkins, Christopher J C H and Dayan, Peter},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Watkins, Dayan - 1992 - Technical Note Q,-Learning.pdf:pdf},
title = {{No Title}}
}
@article{Royle2012,
abstract = {1.Understanding the factors affecting species occurrence is a pre-eminent focus of applied ecological research. However, direct information about species occurrence is lacking for many species. Instead, researchers sometimes have to rely on so-called presence-only data (i.e. when no direct information about absences is available), which often results from opportunistic, unstructured sampling. maxent is a widely used software program designed to model and map species distribution using presence-only data. 2.We provide a critical review of maxent as applied to species distribution modelling and discuss how it can lead to inferential errors. A chief concern is that maxent produces a number of poorly defined indices that are not directly related to the actual parameter of interest – the probability of occurrence ($\psi$). This focus on an index was motivated by the belief that it is not possible to estimate $\psi$ from presence-only data; however, we demonstrate that $\psi$ is identifiable using conventional likelihood methods under the assumptions of random sampling and constant probability of species detection. 3.The model is implemented in a convenient r package which we use to apply the model to simulated data and data from the North American Breeding Bird Survey. We demonstrate that maxent produces extreme under-predictions when compared to estimates produced by logistic regression which uses the full (presence/absence) data set. We note that maxent predictions are extremely sensitive to specification of the background prevalence, which is not objectively estimated using the maxent method. 4.As with maxent, formal model-based inference requires a random sample of presence locations. Many presence-only data sets, such as those based on museum records and herbarium collections, may not satisfy this assumption. However, when sampling is random, we believe that inference should be based on formal methods that facilitate inference about interpretable ecological quantities instead of vaguely defined indices.},
author = {Royle, J. Andrew and Chandler, Richard B. and Yackulic, Charles and Nichols, James D.},
doi = {10.1111/j.2041-210X.2011.00182.x},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Royle et al. - 2012 - Likelihood analysis of species occurrence probability from presence-only data for modelling species distributions.pdf:pdf},
isbn = {2041-210X},
issn = {2041210X},
journal = {Methods Ecol. Evol.},
keywords = {Bayes' rule,Detection probability,Logistic regression,Occupancy model,Occurrence probability,Presence-only data,Species distribution model,maxent},
number = {3},
pages = {545--554},
title = {{Likelihood analysis of species occurrence probability from presence-only data for modelling species distributions}},
volume = {3},
year = {2012}
}
@misc{gandhi2017a,
author = {Gandhi, D and Pinto, L and Gupta, A},
edition = {arXiv prep},
title = {{Learning to Fly by Crashing}}
}
@misc{jaderberg2018a,
author = {Jaderberg, M and Czarnecki, W M and Dunning, I and Marris, L and Lever, G and Castaneda, A G and Beattie, C and Rabinowitz, N C and Morcos, A S and Ruderman, A},
edition = {arXiv prep},
title = {{Human-level performance in firstperson multiplayer games with population-based deep reinforcement learning}}
}
@misc{rowland2018a,
author = {Rowland, M and Bellemare, M G and Dabney, W and Munos, R and Teh, Y W},
edition = {arXiv prep},
title = {{An Analysis of Categorical Distributional Reinforcement Learning}}
}
@article{Nogueira2017,
abstract = {Search engines play an important role in our everyday lives by assisting us in finding the information we need. When we input a complex query, however, results are often far from satisfactory. In this work, we introduce a query reformulation system based on a neural network that rewrites a query to maximize the number of relevant documents returned. We train this neural network with reinforcement learning. The actions correspond to selecting terms to build a reformulated query, and the reward is the document recall. We evaluate our approach on three datasets against strong baselines and show a relative improvement of 5-20{\%} in terms of recall. Furthermore, we present a simple method to estimate a conservative upper-bound performance of a model in a particular environment and verify that there is still large room for improvements.},
archivePrefix = {arXiv},
arxivId = {1704.04572},
author = {Nogueira, Rodrigo and Cho, Kyunghyun},
eprint = {1704.04572},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nogueira, Cho - 2017 - Task-Oriented Query Reformulation with Reinforcement Learning.pdf:pdf},
month = {apr},
title = {{Task-Oriented Query Reformulation with Reinforcement Learning}},
url = {http://arxiv.org/abs/1704.04572},
year = {2017}
}
@misc{krizhevsky2012a,
author = {Krizhevsky, A and Sutskever, I and Hinton, G E},
title = {{“Imagenet classification with deep convolutional neural networks”. In: Advances in neural information processing systems. 1097–1105}}
}
@article{And2018,
author = {Lydakis, Andreas},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lydakis - 2018 - Optimization of Control Measures Against Invasive Species BY Andreas Lydakis PROJECT Submitted to the University of New.pdf:pdf},
title = {{Optimization of Control Measures Against Invasive Species BY Andreas Lydakis PROJECT Submitted to the University of New Hampshire In Partial Fulfilment of Master of Science in Computer Science February , 2018 Dr Marek Petik ( supervisor )}},
year = {2018}
}
@book{goodfellow2016a,
author = {Goodfellow, I and Bengio, Y and Courville, A},
publisher = {MIT Press},
title = {{Deep learning}}
}
@misc{li2015a,
author = {Li, X and Li, L and Gao, J and He, X and Chen, J and Deng, L and He, J},
edition = {arXiv prep},
title = {{Recurrent reinforcement learning: a hybrid approach}}
}
@misc{lecun1995a,
address = {3361(10},
author = {LeCun, Y and Bengio, Y},
title = {{“Convolutional networks for images, speech, and time series”. The handbook of brain theory and neural networks}}
}
@incollection{kempka2016a,
author = {Kempka, M and Wydmuch, M and Runc, G and Toczek, J and Ja{\'{s}}kowski, W},
booktitle = {Comput. Intell. Games (CIG), 2016 IEEE Conf. on. IEEE},
pages = {1--8},
title = {{Vizdoom: A doom-based ai research platform for visual reinforcement learning}}
}
@misc{schulman2017b,
author = {Schulman, J and Wolski, F and Dhariwal, P and Radford, A and Klimov, O},
edition = {arXiv prep},
title = {{Proximal policy optimization algorithms}}
}
@inproceedings{Lange2010,
abstract = {This paper discusses the effectiveness of deep auto-encoder neural networks in visual reinforcement learning (RL) tasks. We propose a framework for combining the training of deep auto-encoders (for learning compact feature spaces) with recently-proposed batch-mode RL algorithms (for learning policies). An emphasis is put on the data-efficiency of this combination and on studying the properties of the feature spaces automatically constructed by the deep auto-encoders. These feature spaces are empirically shown to adequately resemble existing similarities and spatial relations between observations and allow to learn useful policies. We propose several methods for improving the topology of the feature spaces making use of task-dependent information. Finally, we present first results on successfully learning good control policies directly on synthesized and real images.},
archivePrefix = {arXiv},
arxivId = {1507.04296},
author = {Lange, Sascha and Riedmiller, Martin},
booktitle = {Proc. Int. Jt. Conf. Neural Networks},
doi = {10.1109/IJCNN.2010.5596468},
eprint = {1507.04296},
isbn = {9781424469178},
issn = {1098-7576},
pmid = {25719670},
title = {{Deep auto-encoder neural networks in reinforcement learning}},
year = {2010}
}
@article{kearns2002a,
author = {Kearns, M and Singh, S},
journal = {Mach. Learn.},
number = {2-3},
pages = {209--232},
title = {{Near-optimal reinforcement learning in polynomial time}},
volume = {49}
}
@misc{haber2018a,
author = {Haber, N and Mrowca, D and Fei-Fei, L and Yamins, D L},
edition = {arXiv prep},
title = {{Learning to Play with Intrinsically-Motivated Self-Aware Agents}}
}
@article{Strehl2008,
abstract = {Several algorithms for learning near-optimal policies in Markov Decision Processes have been analyzed and proven efficient. Empirical results have suggested that Model-based Interval Estimation (MBIE) learns efficiently in practice, effectively balancing exploration and exploitation. This paper presents a theoretical analysis of MBIE and a new variation called MBIE-EB, proving their efficiency even under worst-case conditions. The paper also introduces a new performance metric, average loss, and relates it to its less "online" cousins from the literature.},
annote = {Model-based Interval Estimation for MDPs},
author = {Strehl, Alexander L and Littman, Michael L},
doi = {10.1016/j.jcss.2007.08.009},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Strehl, Littman - 2008 - An analysis of model-based Interval Estimation for Markov Decision Processes.pdf:pdf},
journal = {J. Comput. Syst. Sci.},
keywords = {Learning theory,Markov Decision Processes,Reinforcement learning},
pages = {1309--1331},
title = {{An analysis of model-based Interval Estimation for Markov Decision Processes}},
url = {www.elsevier.com/locate/jcss},
volume = {74},
year = {2008}
}
@incollection{coumans2016a,
author = {Coumans, E and Bai, Y},
edition = {Barto.2012},
title = {{Bullet}},
url = {http://pybullet.org/.}
}
@misc{zamora2016a,
author = {Zamora, I and Lopez, N G and Vilches, V M and Cordero, A H},
edition = {arXiv prep},
title = {{Extending the OpenAI Gym for robotics: a toolkit for reinforcement learning using ROS and Gazebo}}
}
@misc{ostrovski2017a,
author = {Ostrovski, G and Bellemare, M G and v. d. Oord, A and Munos, R},
edition = {arXiv prep},
title = {{Count-based exploration with neural density models}}
}
@misc{salimans2017a,
author = {Salimans, T and Ho, J and Chen, X and Sutskever, I},
edition = {arXiv prep},
title = {{Evolution Strategies as a Scalable Alternative to Reinforcement Learning}}
}
@article{singh1996a,
author = {Singh, S P and Sutton, R S},
journal = {Mach. Learn.},
number = {1-3},
pages = {123--158},
title = {{Reinforcement learning with replacing eligibility traces}},
volume = {22}
}
@misc{haarnoja2017a,
author = {Haarnoja, T and Tang, H and Abbeel, P and Levine, S},
edition = {arXiv prep},
title = {{Reinforcement learning with deep energy-based policies}}
}
@article{Elith2006a,
abstract = {Prediction of species' distributions is central to diverse applications in ecology, evolution and conservation science. There is increasing electronic access to vast sets of occurrence records in museums and herbaria, yet little effective guidance on how best to use this information in the context of numerous approaches for modelling distributions. To meet this need, we compared 16 modelling methods over 226 species from 6 regions of the world, creating the most comprehensive set of model comparisons to date. We used presence-only data to fit models, and independent presence-absence data to evaluate the predictions. Along with well-established modelling methods such as generalised additive models and GARP and BIOCLIM, we explored methods that either have been developed recently or have rarely been applied to modelling species' distributions. These include machine-learning methods and community models, both of which have features that may make them particularly well suited to noisy or sparse information, as is typical of species' occurrence data. Presence-only data were effective for modelling species' distributions for many species and regions. The novel methods consistently outperformed more established methods. The results of our analysis are promising for the use of data from museums and herbaria, especially as methods suited to the noise inherent in such data improve.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Elith, J and Graham, C H and Anderson, R P and Dudik, M and Ferrier, S and Guisan, A and Hijmans, R J and Huettmann, F and Leathwick, J R and Lehmann, A and Li, J and Lohmann, L G and Loiselle, B A and Manion, G and Moritz, C and Nakamura, M and Nakazawa, Y and Overton, J M and Peterson, A T and Phillips, S J and Richardson, K and Scachetti-Pereira, R and Schapire, R E and Soberon, J and Williams, S and Wisz, M S and Zimmermann, N E},
doi = {10.1111/j.2006.0906-7590.04596.x},
eprint = {arXiv:1011.1669v3},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Elith et al. - 2006 - Novel methods improve prediction of species' distributions from occurrence data.pdf:pdf},
isbn = {1600-0587},
issn = {09067590},
journal = {Ecography (Cop.).},
keywords = {biodiversity,climate-change,conservation,distribution models,envelope models,habitat-suitability,logistic-regression,plant,potential distribution,spatial prediction},
number = {2},
pages = {129--151},
pmid = {1891},
title = {{Novel methods improve prediction of species' distributions from occurrence data}},
volume = {29},
year = {2006}
}
@article{dayan2008a,
author = {Dayan, P and Daw, N D},
journal = {Cogn. Affect. Behav. Neurosci.},
number = {4},
pages = {429--453},
title = {{Decision theory, reinforcement learning, and the brain}},
volume = {8}
}
@misc{kalashnikov2018a,
author = {Kalashnikov, D and Irpan, A and Pastor, P and Ibarz, J and Herzog, A and Jang, E and Quillen, D and Holly, E and Kalakrishnan, M and Vanhoucke, V and Levine, S},
edition = {arXiv prep},
title = {{Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation}}
}
@misc{savinov2018a,
author = {Savinov, N and Raichuk, A and Marinier, R and Vincent, D and Pollefeys, M and Lillicrap, T and Gelly, S},
edition = {arXiv prep},
title = {{Episodic Curiosity through Reachability}}
}
@book{sutton1998a,
author = {Sutton, R S and Barto, A G},
number = {1},
publisher = {MIT press Cambridge},
title = {{Reinforcement learning: An introduction}},
volume = {1}
}
@inproceedings{gal2016a,
address = {2016, New York City, NY, USA},
author = {Gal, Y and Ghahramani, Z},
booktitle = {Proc. 33nd Int. Conf. Mach. Learn. ICML},
pages = {1050},
title = {{Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning}}
}
@inproceedings{bouckaert2003a,
author = {Bouckaert, R R},
booktitle = {Proc. 20th Int. Conf. Mach. Learn.},
pages = {51--58},
title = {{Choosing between two learning algorithms based on calibrated tests}},
volume = {3}
}
@inproceedings{min-aaai,
author = {Minton, S and Johnston, M and Philips, A B and Laird, P},
booktitle = {Proc. AAAI-90},
title = {{Solving Large Scale Constraint Satisfaction and Scheduling Problems Using a Heuristic Repair Method}},
year = {1990}
}
@incollection{kakade2001a,
author = {Kakade, S},
booktitle = {Adv. Neural},
title = {{A Natural Policy Gradient}}
}
@article{Francois-Lavet2018,
annote = {version 2 published on arXiv},
archivePrefix = {arXiv},
arxivId = {1811.12560v2},
author = {Fran{\c{c}}ois-Lavet, Vincent and Henderson, Peter and Islam, Riashat and Bellemare, Marc G and Pineau, Joelle},
doi = {10.1561/2200000071},
eprint = {1811.12560v2},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fran{\c{c}}ois-Lavet et al. - 2018 - An Introduction to Deep Reinforcement Learning.pdf:pdf},
title = {{An Introduction to Deep Reinforcement Learning}},
url = {https://arxiv.org/pdf/1811.12560.pdf},
year = {2018}
}
@misc{lample2017a,
author = {Lample, G and Chaplot, D S},
title = {{“Playing FPS Games with Deep Reinforcement Learning.” In: AAAI. 2140–2146}}
}
@incollection{mankowitz2016a,
author = {Mankowitz, D J and Mann, T A and Mannor, S},
booktitle = {Adv. Neural Inf. Process. Syst.},
pages = {1588--1596},
title = {{Adaptive Skills Adaptive Partitions (ASAP)}}
}
@book{precup2000a,
author = {Precup, D},
publisher = {Computer Science Department Faculty Publication Series: 80},
title = {{Eligibility traces for off-policy policy evaluation}}
}
@article{Lydakis2018,
author = {Lydakis, Andreas and Allen, Jenica M. and Petrik, Marek and Szewczyk, Tim M.},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lydakis et al. - 2018 - Robust strategies for managing invasive plants.pdf:pdf},
journal = {IJCAI Artif. Intell. Wildl. Conserv.},
title = {{Robust strategies for managing invasive plants}},
year = {2018}
}
@techreport{braziunas2003a,
author = {Braziunas, D},
institution = {University of Toronto, Tech. Rep},
title = {{POMDP solution methods}}
}
@inproceedings{foerster2018a,
author = {Foerster, J and Chen, R Y and Al-Shedivat, M and Whiteson, S and Abbeel, P and Mordatch, I},
booktitle = {Proc. 17th Int. Conf. Auton. Agents MultiAgent Syst. Int. Found. Auton. Agents Multiagent Syst.},
pages = {122--130},
title = {{Learning with opponent-learning awareness}}
}
@article{geurts2006a,
author = {Geurts, P and Ernst, D and Wehenkel, L},
journal = {Mach. Learn.},
number = {1},
pages = {3--42},
title = {{Extremely randomized trees}},
volume = {63}
}
@misc{mcgovern1997a,
annote = {Vol. 1317},
author = {McGovern, A and Sutton, R S and Fagg, A H},
title = {{“Roles of macroactions in accelerating reinforcement learning”. In: Grace Hopper celebration of women in computing}}
}
@incollection{jakobi1995a,
author = {Jakobi, N and Husbands, P and Harvey, I},
booktitle = {Eur. Conf. Artif. Life. Springer},
pages = {704--720},
title = {{Noise and the reality gap: The use of simulation in evolutionary robotics}}
}
@techreport{Henderson,
abstract = {In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.},
archivePrefix = {arXiv},
arxivId = {1709.06560v2},
author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
eprint = {1709.06560v2},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Henderson et al. - Unknown - Deep Reinforcement Learning that Matters.pdf:pdf;:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Henderson et al. - Unknown - Deep Reinforcement Learning that Matters(2).pdf:pdf},
keywords = {Machine Learning Methods Track},
title = {{Deep Reinforcement Learning that Matters}},
url = {www.aaai.org}
}
@misc{bahdanau2016a,
author = {Bahdanau, D and Brakel, P and Xu, K and Goyal, A and Lowe, R and Pineau, J and Courville, A and Bengio, Y},
edition = {arXiv prep},
title = {{An actor-critic algorithm for sequence prediction}}
}
@misc{tessler2017a,
author = {Tessler, C and Givony, S and Zahavy, T and Mankowitz, D J and Mannor, S},
title = {{“A Deep Hierarchical Approach to Lifelong Learning in Minecraft.” In: AAAI. 1553–1561}}
}
@article{tanner2009a,
author = {Tanner, B and White, A},
journal = {J. Mach. Learn. Res.},
pages = {2133--2136},
title = {{RL-Glue: Language-independent software for reinforcement-learning experiments}},
volume = {10}
}
@techreport{Dietz,
abstract = {This presentation will provide an overview of the challenges of high power proton accelerators such as SNS, J-PARC, etc., and what we have learned from recent experiences. Beam loss mechanisms and methods to mitigate beam loss will also be discussed.},
author = {Dietz, Laura and Verma, Manisha and Radlinski, Filip and Craswell, Nick},
booktitle = {Trec},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dietz et al. - Unknown - TREC Complex Answer Retrieval Overview.pdf:pdf},
pages = {1--13},
title = {{TREC Complex Answer Retrieval Overview}},
url = {http://trec-car.cs.unh.edu},
year = {2017}
}
@article{papad,
author = {Johnson, D S and Papadimitrou, C H and Yannakakis, M},
journal = {J. Comput. Syst. Sci.},
pages = {79--100},
title = {{How Easy is Local Search?}},
volume = {37},
year = {1988}
}
@book{christopher2006a,
author = {Christopher, M B},
publisher = {Springer},
title = {{Pattern recognition and machine learning}}
}
@misc{wahlstroem2015a,
author = {Wahlstr{\"{o}}m, N and Sch{\"{o}}n, T B and Deisenroth, M P},
edition = {arXiv prep},
title = {{From pixels to torques: Policy learning with deep dynamical models}}
}
@techreport{Popov,
abstract = {Deep learning and reinforcement learning methods have recently been used to solve a variety of problems in continuous control domains. An obvious application of these techniques is dexterous manipulation tasks in robotics which are difficult to solve using traditional control theory or hand-engineered approaches. One example of such a task is to grasp an object and precisely stack it on another. Solving this difficult and practically relevant problem in the real world is an important long-term goal for the field of robotics. Here we take a step towards this goal by examining the problem in simulation and providing models and techniques aimed at solving it. We introduce two extensions to the Deep Deterministic Policy Gradient algorithm (DDPG), a model-free Q-learning based method, which make it significantly more data-efficient and scalable. Our results show that by making extensive use of off-policy data and replay, it is possible to find control policies that robustly grasp objects and stack them. Further, our results hint that it may soon be feasible to train successful stacking policies by collecting interactions on real robots.},
author = {Popov, Ivaylo and Heess, Nicolas and Lillicrap, Timothy and Hafner, Roland and Barth-Maron, Gabriel and Vecerik, Matej and Lampe, Thomas and Tassa, Yuval and Erez, Tom and Deepmind, Martin Riedmiller},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Popov et al. - Unknown - Data-efficient Deep Reinforcement Learning for Dexterous Manipulation.pdf:pdf},
title = {{Data-efficient Deep Reinforcement Learning for Dexterous Manipulation}},
url = {https://pdfs.semanticscholar.org/e701/9d9786af58f56c5de7e7daa2ce9050ba60f0.pdf}
}
@article{Peters2018,
abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
archivePrefix = {arXiv},
arxivId = {1802.05365},
author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
eprint = {1802.05365},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Peters et al. - 2018 - Deep contextualized word representations.pdf:pdf},
month = {feb},
title = {{Deep contextualized word representations}},
url = {http://arxiv.org/abs/1802.05365},
year = {2018}
}
@article{Warton2010,
abstract = {Presence-only data, point locations where a species has been recorded as being present, are often used in modeling the distribution of a species as a function of a set of explanatory variables-whether to map species occurrence , to understand its association with the environment, or to predict its response to environmental change. Currently, ecologists most commonly analyze presence-only data by adding randomly chosen "pseudo-absences" to the data such that it can be analyzed using logistic regression, an approach which has weaknesses in model specification, in interpretation, and in implementation. To address these issues, we propose Poisson point process model-ing of the intensity of presences. We also derive a link between the proposed approach and logistic regression-specifically, we show that as the number of pseudo-absences increases (in a regular or uniform random arrangement), logistic regression slope parameters and their standard errors converge to those of the corresponding Poisson point process model. We discuss the practical implications of these results. In particular, point process modeling offers a framework for choice of the number and location of pseudo-absences, both of which are currently chosen by ad hoc and sometimes ineffective methods in ecology, a point which we illustrate by example.},
author = {Warton, David I and Shepherd, Leah C},
doi = {10.1214/10-AOAS331},
journal = {Ann. Appl. Stat.},
keywords = {Habitat modeling,occurrence data,pseudo-absences,quadrature points,species distribution modeling},
number = {3},
pages = {1383--1402},
title = {{POISSON POINT PROCESS MODELS SOLVE THE "PSEUDO-ABSENCE PROBLEM" FOR PRESENCE-ONLY DATA IN ECOLOGY 1}},
url = {https://projecteuclid.org/download/pdfview{\_}1/euclid.aoas/1287409378},
volume = {4},
year = {2010}
}
@book{rasmussen2004a,
author = {Rasmussen, C E},
pages = {63--71},
publisher = {Springer},
title = {{“Gaussian processes in machine learning”. In: Advanced lectures on machine learning}}
}
@misc{hausknecht2015a,
author = {Hausknecht, M and Stone, P},
edition = {arXiv prep},
title = {{Deep recurrent Q-learning for partially observable MDPs}}
}
@inproceedings{kolter2009a,
author = {Kolter, J Z and Ng, A Y},
booktitle = {Proc. 26th Annu. Int. Conf. Mach. Learn. ACM},
pages = {513--520},
title = {{Near-Bayesian exploration in polynomial time}}
}
@book{unknown-a,
pages = {305--320},
publisher = {Springer},
title = {{Q ($\lambda$) with Off-Policy Corrections”. In: International Conference on Algorithmic Learning Theory}}
}
@article{bubeck2011a,
author = {Bubeck, S and Munos, R and Stoltz, G},
journal = {Theor. Comput. Sci.},
number = {19},
pages = {1832--1852},
title = {{Pure exploration in finitely-armed and continuous-armed bandits}},
volume = {412}
}
@misc{bacon2016a,
author = {Bacon, P.-L. and Harb, J and Precup, D},
edition = {arXiv prep},
title = {{The option-critic architecture}}
}
@book{ortner2014a,
author = {Ortner, R and Maillard, O.-A. and Ryabko, D},
pages = {140--154},
publisher = {Springer},
title = {{“Selecting nearoptimal approximate state representations in reinforcement learning”. In: International Conference on Algorithmic Learning Theory}}
}
@book{anderson1958a,
address = {New York},
author = {Anderson, T W and Anderson, T W and Anderson, T W and Anderson, T W and Math{\'{e}}maticien, E.-U.},
publisher = {Wiley},
title = {{An introduction to multivariate statistical analysis}},
volume = {2}
}
@thesis{fran2016a,
author = {Fran{\c{c}}ois-Lavet, V},
institution = {thesis. University of Liege, Belgium},
title = {{DeeR”. https://deer.readthedocs.io/. Fran{\c{c}}ois-Lavet, V.2017.“Contributions to deep reinforcement learning and its applications in smartgrids}},
type = {PhD}
}
@article{Silver2017,
abstract = {Starting from zero knowledge and without human data, AlphaGo Zero was able to teach itself to play Go and to develop novel strategies that provide new insights into the oldest of games.},
annote = {Solving Go},
author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature24270},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
keywords = {Computational science,Computer science,Reward},
month = {oct},
number = {7676},
pages = {354--359},
publisher = {Nature Publishing Group},
title = {{Mastering the game of Go without human knowledge}},
url = {http://www.nature.com/doifinder/10.1038/nature24270},
volume = {550},
year = {2017}
}
@book{sutton2017a,
author = {Sutton, R S and Barto, A G},
edition = {2},
publisher = {MIT Press},
title = {{Reinforcement Learning: An Introductionin progress)}}
}
@misc{machado-a,
author = {{Machado M. C.}, M G Bellemare and 2017a, M Bowling.},
edition = {arXiv prep},
title = {{A Laplacian Framework for Option Discovery in Reinforcement Learning}}
}
@incollection{dayan2008b,
author = {Dayan, P and Niv, Y},
booktitle = {Curr. Opin. Neurobiol.},
number = {2},
pages = {185--196},
title = {{Reinforcement learning: the good, the bad and the ugly}},
volume = {18}
}
@misc{miikkulainen2017a,
author = {Miikkulainen, R and Liang, J and Meyerson, E and Rawal, A and Fink, D and Francon, O and Raju, B and Navruzyan, A and Duffy, N and Hodjat, B},
edition = {arXiv prep},
title = {{Evolving Deep Neural Networks}}
}
@misc{peng2017a,
author = {Peng, P and Yuan, Q and Wen, Y and Yang, Y and Tang, Z and Long, H and Wang, J},
edition = {arXiv prep},
title = {{Multiagent Bidirectionally-Coordinated Nets for Learning to Play StarCraft Combat Games}}
}
@article{Kos2017,
abstract = {Adversarial examples have been shown to exist for a variety of deep learning architectures. Deep reinforcement learning has shown promising results on training agent policies directly on raw inputs such as image pixels. In this paper we present a novel study into adversarial attacks on deep reinforcement learning polices. We compare the effectiveness of the attacks using adversarial examples vs. random noise. We present a novel method for reducing the number of times adversarial examples need to be injected for a successful attack, based on the value function. We further explore how re-training on random noise and FGSM perturbations affects the resilience against adversarial examples.},
archivePrefix = {arXiv},
arxivId = {1705.06452},
author = {Kos, Jernej and Song, Dawn Xiaodong},
eprint = {1705.06452},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kos, Song - 2017 - Delving into adversarial attacks on deep policies.pdf:pdf},
journal = {undefined},
month = {may},
title = {{Delving into adversarial attacks on deep policies}},
url = {http://arxiv.org/abs/1705.06452 https://www.semanticscholar.org/paper/Delving-into-adversarial-attacks-on-deep-policies-Kos-Song/cf8ed2793bc6aec88da5306fe2de560dc0be9b15},
year = {2017}
}
@misc{hessel2018a,
author = {Hessel, M and Soyer, H and Espeholt, L and Czarnecki, W and Schmitt, S and van Hasselt, H},
edition = {arXiv prep},
title = {{Multi-task Deep Reinforcement Learning with PopArt}}
}
@techreport{Lagoudakis2003,
abstract = {We propose a new approach to reinforcement learning for control problems which combines value-function approximation with linear architectures and approximate policy iteration. This new approach is motivated by the least-squares temporal-difference learning algorithm (LSTD) for prediction problems, which is known for its efficient use of sample experiences compared to pure temporal-difference algorithms. Heretofore, LSTD has not had a straightforward application to control problems mainly because LSTD learns the state value function of a fixed policy which cannot be used for action selection and control without a model of the underlying process. Our new algorithm, least-squares policy iteration (LSPI), learns the state-action value function which allows for action selection without a model and for incremental policy improvement within a policy-iteration framework. LSPI is a model-free, off-policy method which can use efficiently (and reuse in each iteration) sample experiences collected in any manner. By separating the sample collection method, the choice of the linear approximation architecture, and the solution method, LSPI allows for focused attention on the distinct elements that contribute to practical reinforcement learning. LSPI is tested on the simple task of balancing an inverted pendulum and the harder task of balancing and riding a bicycle to a target location. In both cases, LSPI learns to control the pendulum or the bicycle by merely observing a relatively small number of trials where actions are selected randomly. LSPI is also compared against Q-learning (both with and without experience replay) using the same value function architecture. While LSPI achieves good performance fairly consistently on the difficult bicycle task, Q-learning variants were rarely able to balance for more than a small fraction of the time needed to reach the target location.},
annote = {LSPI paper},
author = {Lagoudakis, Michail G and Parr, Ronald},
booktitle = {J. Mach. Learn. Res.},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lagoudakis, Parr - 2003 - Least-Squares Policy Iteration.pdf:pdf},
keywords = {Approximate Policy Iteration,Least-Squares Methods,Markov Decision Processes,Reinforcement Learning,Value-Function Approximation},
pages = {1107--1149},
title = {{Least-Squares Policy Iteration}},
url = {http://www.jmlr.org/papers/volume4/lagoudakis03a/lagoudakis03a.pdf},
volume = {4},
year = {2003}
}
@article{szegedy2016a,
author = {Szegedy, C and Ioffe, S and Vanhoucke, V and Alemi, A A},
edition = {arXiv prep},
journal = {AAAI},
number = {12},
title = {{Inception-v4, inception-resnet and the impact of residual connections on learning}},
volume = {4}
}
@article{bartlett2002a,
author = {Bartlett, P L and Mendelson, S},
journal = {J. Mach. Learn. Res.},
pages = {463--482},
title = {{Rademacher and Gaussian complexities: Risk bounds and structural results}},
volume = {3}
}
@misc{ng2000a,
author = {Ng, A Y and Russell, S J},
pages = {663--670},
title = {{“Algorithms for inverse reinforcement learning.” In: Icml}}
}
@misc{wang2015a,
author = {Wang, Z and de Freitas, N and Lanctot, M},
edition = {arXiv prep},
title = {{Dueling network architectures for deep reinforcement learning}}
}
@incollection{gu2017b,
author = {Gu, S and Lillicrap, T and Ghahramani, Z and Turner, R E and Levine, S},
booktitle = {5th Int. Conf. Learn. Represent.},
edition = {arXiv prep},
publisher = {ICLR},
title = {{2016a. “Q-prop: Sample-efficient policy gradient with an off-policy critic”}}
}
@misc{gu-b,
author = {Gu, S and Lillicrap, T and Ghahramani, Z and Turner, R E and Sch{\"{o}}lkopf, B and Levine, S},
edition = {arXiv prep},
title = {{2017c. “Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning”}}
}
@article{leffler2007a,
author = {Leffler, B R and Littman, M L and Edmunds, T},
journal = {AAAI},
pages = {572--577},
title = {{Efficient reinforcement learning with relocatable action models}},
volume = {7}
}
@misc{petrik2009a,
author = {Petrik, M and Scherrer, B},
title = {{“Biasing approximate dynamic programming with a lower discount factor”. In: Advances in neural information processing systems. 1265–1272}}
}
@techreport{erhan2009a,
author = {Erhan, D and Bengio, Y and Courville, A and Vincent, P},
institution = {University of Montreal},
number = {3},
pages = {1},
title = {{Visualizing higher-layer features of a deep network}},
volume = {1341}
}
@misc{ruder2017a,
author = {Ruder, S},
edition = {arXiv prep},
title = {{An overview of multi-task learning in deep neural networks}}
}
@misc{chen2017a,
author = {Chen, X and Liu, C and Song, D},
edition = {arXiv prep},
title = {{Learning Neural Programs To Parse Programs}}
}
@misc{bengio2015a,
author = {Bengio, Y and Lee, D.-H. and Bornschein, J and Mesnard, T and Lin, Z},
edition = {arXiv prep},
title = {{Towards biologically plausible deep learning}}
}
@article{schaul2010a,
author = {Schaul, T and Bayer, J and Wierstra, D and Sun, Y and Felder, M and Sehnke, F and R{\"{u}}ckstie{\ss}, T and Schmidhuber, J},
journal = {J. Mach. Learn. Res.},
pages = {743--746},
title = {{PyBrain}},
volume = {11}
}
@misc{zhang-b,
author = {Zhang, C and Vinyals, O and Munos, R and S.},
edition = {arXiv prep},
title = {{Bengio. 2018c. “A Study on Overfitting in Deep Reinforcement Learning”}}
}
@misc{bostrom2017a,
annote = {Superintelligence. Dunod},
author = {Bostrom, N},
title = {{No Title}}
}
@article{Goodfellow2014,
abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
archivePrefix = {arXiv},
arxivId = {1412.6572},
author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
eprint = {1412.6572},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Goodfellow, Shlens, Szegedy - 2014 - Explaining and Harnessing Adversarial Examples.pdf:pdf},
month = {dec},
title = {{Explaining and Harnessing Adversarial Examples}},
url = {http://arxiv.org/abs/1412.6572},
year = {2014}
}
@misc{barto1983a,
author = {Barto, A G and Sutton, R S and Anderson, C W},
pages = {834--846},
title = {{“Neuronlike adaptive elements that can solve difficult learning control problems”. IEEE transactions on systems, man, and cybernetics}}
}
@article{sandve2013a,
author = {Sandve, G K and Nekrutenko, A and Taylor, J and Hovig, E},
journal = {PLoS Comput. Biol.},
number = {10},
pages = {1003285},
title = {{Ten simple rules for reproducible computational research}},
volume = {9}
}
@techreport{Boyan1998,
author = {Boyan, Justin A},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Boyan - 1998 - Learning Evaluation Functions for Global Optimization.pdf:pdf},
title = {{Learning Evaluation Functions for Global Optimization}},
url = {https://pdfs.semanticscholar.org/61d4/897dbf7ced83a0eb830a8de0dd64abb58ebd.pdf},
year = {1998}
}
@article{geman1992a,
author = {Geman, S and Bienenstock, E and Doursat, R},
journal = {Neural Comput.},
number = {1},
pages = {1--58},
title = {{Neural networks and the bias/variance dilemma}},
volume = {4}
}
@article{Narasimhan2016,
abstract = {Most successful information extraction systems operate with access to a large collection of documents. In this work, we explore the task of acquiring and incorporating external evidence to improve extraction accuracy in domains where the amount of training data is scarce. This process entails issuing search queries, extraction from new sources and reconciliation of extracted values, which are repeated until sufficient evidence is collected. We approach the problem using a reinforcement learning framework where our model learns to select optimal actions based on contextual information. We employ a deep Q-network, trained to optimize a reward function that reflects extraction accuracy while penalizing extra effort. Our experiments on two databases -- of shooting incidents, and food adulteration cases -- demonstrate that our system significantly outperforms traditional extractors and a competitive meta-classifier baseline.},
archivePrefix = {arXiv},
arxivId = {1603.07954},
author = {Narasimhan, Karthik and Yala, Adam and Barzilay, Regina},
eprint = {1603.07954},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Narasimhan, Yala, Barzilay - 2016 - Improving Information Extraction by Acquiring External Evidence with Reinforcement Learning.pdf:pdf},
month = {mar},
title = {{Improving Information Extraction by Acquiring External Evidence with Reinforcement Learning}},
url = {http://arxiv.org/abs/1603.07954},
year = {2016}
}
@inproceedings{dearden1999a,
author = {Dearden, R and Friedman, N and Andre, D},
booktitle = {Proc. Fifteenth Conf. Uncertain. Artif. Intell.},
pages = {150--159},
publisher = {Morgan Kaufmann Publishers Inc},
title = {{Model based Bayesian exploration}}
}
@article{sutton1999a,
author = {Sutton, R S and Precup, D and Singh, S},
journal = {Artificial},
number = {1-2},
pages = {181--211},
title = {{Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning}},
volume = {112}
}
@incollection{houthooft2016a,
author = {Houthooft, R and Chen, X and Duan, Y and Schulman, J and Turck, F.De and Abbeel, P},
booktitle = {Adv. Neural Inf. Process. Syst.},
pages = {1109--1117},
title = {{Vime: Variational information maximizing exploration}}
}
@incollection{finn2016a,
author = {Finn, C and Goodfellow, I and Levine, S},
booktitle = {Adv. Neural Inf. Process. Syst.},
pages = {64--72},
title = {{Unsupervised learning for physical interaction through video prediction}}
}
@misc{fox2015a,
author = {Fox, R and Pakman, A and Tishby, N},
edition = {arXiv prep},
title = {{Taming the noise in reinforcement learning via soft updates}}
}
@article{schmidhuber2015a,
author = {Schmidhuber, J},
journal = {Neural Networks},
pages = {85--117},
title = {{Deep learning in neural networks: An overview}},
volume = {61}
}
@book{ziebart2010a,
author = {Ziebart, B D},
publisher = {Carnegie Mellon University},
title = {{Modeling purposeful adaptive behavior with the principle of maximum causal entropy}}
}
@article{fortunato2017a,
author = {Fortunato, M and Azar, M G and Piot, B and Menick, J and Osband, I and Graves, A and Mnih, V and Munos, R and Hassabis, D and O.},
edition = {arXiv prep},
journal = {Pietquin, al},
title = {{Noisy networks for exploration}}
}
@incollection{xu2015a,
author = {Xu, K and Ba, J and Kiros, R and Cho, K and Courville, A and Salakhudinov, R and Zemel, R and Bengio, Y},
booktitle = {Int. Conf. Mach. Learn.},
pages = {2048--2057},
title = {{Show, attend and tell: Neural image caption generation with visual attention}}
}
@incollection{kulkarni2016a,
author = {Kulkarni, T D and Narasimhan, K and Saeedi, A and Tenenbaum, J},
booktitle = {Adv. Neural Inf. Process. Syst.},
pages = {3675--3683},
title = {{Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation}}
}
@misc{neu2012a,
author = {Neu, G and Szepesv{\'{a}}ri, C},
edition = {arXiv prep},
title = {{Apprenticeship learning using inverse reinforcement learning and gradient methods}}
}
@incollection{unknown2001a,
address = {Columbia, Canada},
booktitle = {Inf. Process. Syst. 14 [Neural Inf. Process. Syst. Nat. Synth. NIPS},
pages = {1531--1538},
publisher = {Vancouver, British},
title = {{No Title}}
}
@article{story2014a,
author = {Story, G and Vlaev, I and Seymour, B and Darzi, A and Dolan, R},
journal = {Front. Behav. Neurosci.},
pages = {76},
title = {{Does temporal discounting explain unhealthy behavior? A systematic review and reinforcement learning perspective}},
volume = {8}
}
@incollection{bennett2013a,
author = {Bennett, C C and Hauser, K},
booktitle = {Artif. Intell. Med.},
number = {1},
pages = {9--19},
title = {{Artificial intelligence framework for simulating clinical decision-making: A Markov decision process approach}},
volume = {57}
}
@incollection{sun2011a,
author = {Sun, Y and Gomez, F and Schmidhuber, J},
booktitle = {Artif. Gen. Intell. Springer},
pages = {41--51},
title = {{Planning to be surprised: Optimal bayesian exploration in dynamic environments}}
}
@incollection{cou2017a,
author = {Cou{\"{e}}toux},
booktitle = {9th Int. Conf. Agents Artif. Intell. (ICAART},
title = {{Approximate Bayes Optimal Policy Search using Neural Networks}}
}
@article{geramifard2015a,
author = {Geramifard, A and Dann, C and Klein, R H and Dabney, W and How, J P},
journal = {J. Mach. Learn. Res.},
pages = {1573--1578},
title = {{RLPy: A Value-Function-Based Reinforcement Learning Framework for Education and Research}},
volume = {16}
}
@misc{zhang2018a,
author = {Zhang, A and Satija, H and Pineau, J},
edition = {arXiv prep},
title = {{Decoupling Dynamics and Reward for Transfer Learning}}
}
@article{Renner2013,
abstract = {Summary Modeling the spatial distribution of a species is a fundamental problem in ecology. A number of modeling methods have been developed, an extremely popular one being MAXENT, a maximum entropy modeling approach. In this article, we show that MAXENT is equivalent to a Poisson regression model and hence is related to a Poisson point process model, differing only in the intercept term, which is scale-dependent in MAXENT. We illustrate a number of improvements to MAXENT that follow from these relations. In particular, a point process model approach facilitates methods for choosing the appropriate spatial resolution, assessing model adequacy, and choosing the LASSO penalty parameter, all currently unavailable to MAXENT. The equivalence result represents a significant step in the unification of the species distribution modeling literature.},
author = {Renner, Ian W. and Warton, David I.},
doi = {10.1111/j.1541-0420.2012.01824.x},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Renner, Warton - 2013 - Equivalence of MAXENT and Poisson Point Process Models for Species Distribution Modeling in Ecology.pdf:pdf},
isbn = {1541-0420},
issn = {0006341X},
journal = {Biometrics},
keywords = {Habitat modeling,Location-only,Maximum entropy,Poisson likelihood,Presence-only data,Use-availability},
number = {1},
pages = {274--281},
pmid = {23379623},
title = {{Equivalence of MAXENT and Poisson Point Process Models for Species Distribution Modeling in Ecology}},
volume = {69},
year = {2013}
}
@misc{ng1999a,
author = {Ng, A Y and Harada, D and Russell, S},
editor = {I.C.M.L.},
pages = {278--287},
title = {{Policy invariance under reward transformations: Theory and application to reward shaping}},
volume = {99}
}
@inproceedings{ginsberg,
author = {Ginsberg, M L and Harvey, W D},
booktitle = {Proc. AAAI-91},
title = {{Iterative Broadening}},
year = {1990}
}
@misc{juliani2018a,
author = {Juliani, A and Berges, V.-P. and Vckay, E and Gao, Y and Henry, H and Mattar, M and Lange, D},
edition = {arXiv prep},
title = {{Unity: A General Platform for Intelligent Agents}}
}
@misc{kakade2003a,
author = {Kakade, S and Kearns, M and Langford, J},
editor = {I.C.M.L.},
pages = {306--312},
title = {{Exploration in metric state spaces}},
volume = {3}
}
@article{sondik1978a,
author = {Sondik, E J},
journal = {Oper. Res.},
number = {2},
pages = {282--304},
title = {{The optimal control of partially observable Markov processes over the infinite horizon: Discounted costs}},
volume = {26}
}
@book{stone2000a,
author = {Stone, P and Veloso, M},
pages = {369--381},
publisher = {Machine Learning: ECML},
title = {{Layered learning}}
}
@misc{watter2015a,
author = {Watter, M and Springenberg, J and Boedecker, J and Riedmiller, M},
title = {{“Embed to control: A locally linear latent dynamics model for control from raw images”. In: Advances in neural information processing systems. 2746–2754}}
}
@incollection{bertsekas1995a,
address = {Belmont, MA},
author = {Bertsekas, D P and Bertsekas, D P and Bertsekas, D P and Bertsekas, D P},
number = {2},
title = {{Dynamic programming and optimal control}},
volume = {1}
}
@misc{ranzato2015a,
author = {Ranzato, M and Chopra, S and Auli, M and Zaremba, W},
edition = {arXiv prep},
title = {{Sequence level training with recurrent neural networks}}
}
@incollection{vezhnevets2016a,
author = {Vezhnevets, A and Mnih, V and Osindero, S and Graves, A and Vinyals, O and Agapiou, J},
booktitle = {Adv. Neural Inf. Process. Syst.},
pages = {3486--3494},
title = {{Strategic attentive writer for learning macro-actions}}
}
@misc{kalchbrenner2016a,
author = {Kalchbrenner, N and v. d. Oord, A and Simonyan, K and Danihelka, I and Vinyals, O and Graves, A and Kavukcuoglu, K},
edition = {arXiv prep},
title = {{Video pixel networks}}
}
@misc{vaswani2017a,
author = {Vaswani, A and Shazeer, N and Parmar, N and Uszkoreit, J and Jones, L and Gomez, A N and Kaiser, L and Polosukhin, I},
edition = {arXiv prep},
title = {{Attention Is All You Need}}
}
@incollection{thomas2016a,
author = {Thomas, P S and Brunskill, E},
booktitle = {Int. Conf. Mach. Learn.},
title = {{Data-efficient off-policy policy evaluation for reinforcement learning}}
}
@incollection{schraudolph1994a,
author = {Schraudolph, N N and Dayan, P and Sejnowski, T J},
booktitle = {Adv. Neural Inf. Process. Syst.},
pages = {817--824},
title = {{Temporal difference learning of position evaluation in the game of Go}}
}
@article{giusti2016a,
author = {Giusti, A and Guzzi, J and Cireşan, D C and He, F.-L. and Rodriguez, J P and Fontana, F and Faessler, M and Forster, C and Schmidhuber, J and Caro, G.Di},
journal = {IEEE Robot. Autom. Lett.},
number = {2},
pages = {661--667},
title = {{A machine learning approach to visual perception of forest trails for mobile robots}},
volume = {1}
}
@misc{zhang-a,
author = {{Zhang A.}, N Ballas and 2018a, J Pineau.},
edition = {arXiv prep},
title = {{A Dissection of Overfitting and Generalization in Continuous Reinforcement Learning}}
}
@misc{riedmiller2018a,
author = {Riedmiller, M and Hafner, R and Lampe, T and Neunert, M and Degrave, J and de Wiele, T.Van and Mnih, V and Heess, N and Springenberg, J T},
edition = {arXiv prep},
title = {{Learning by Playing - Solving Sparse Reward Tasks from Scratch}}
}
@inproceedings{jiang2016a,
author = {Jiang, N and Li, L},
booktitle = {Proc. 33rd Int. Conf. Mach. Learn.},
pages = {652--661},
title = {{Doubly robust off-policy value evaluation for reinforcement learning}}
}
@article{rumelhart1988a,
author = {Rumelhart, D E and Hinton, G E and Williams, R J},
journal = {Cogn. Model.},
number = {3},
pages = {1},
title = {{Learning representations by back-propagating errors}},
volume = {5}
}
@incollection{gu2017a,
author = {Gu, S and Holly, E and Lillicrap, T and Levine, S},
booktitle = {Robot. Autom. (ICRA), 2017 IEEE Int. Conf. on. IEEE},
pages = {3389--3396},
title = {{Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates}}
}
@misc{amodei2016a,
author = {Amodei, D and Olah, C and Steinhardt, J and Christiano, P and Schulman, J and Man{\'{e}}, D},
edition = {arXiv prep},
title = {{Concrete problems in AI safety}}
}
@article{vinyals2017a,
author = {Vinyals, O and Ewalds, T and Bartunov, S and Georgiev, P and Vezhnevets, A S and Yeo, M and Makhzani, A and K{\"{u}}ttler, H and Agapiou, J and J.},
edition = {arXiv prep},
journal = {Schrittwieser, al},
title = {{StarCraft II: A New Challenge for Reinforcement Learning}}
}
@article{Miller2012,
annote = {Tax Collection Optimization},
author = {Miller, Gerard and Weatherwax, Melissa and Gardinier, Timothy and Abe, Naoki and Melville, Prem and Pendus, Cezar and Jensen, David and Reddy, Chandan K and Thomas, Vince and Bennett, James and Anderson, Gary and Cooley, Brent},
doi = {10.1287/inte.1110.0618},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Miller et al. - 2012 - Tax Collections Optimization for New York State.pdf:pdf},
title = {{Tax Collections Optimization for New York State}},
url = {http://pubsonline.informs.org74-84.https//doi.org/10.1287/inte.1110.0618http://www.informs.org},
year = {2012}
}
@inproceedings{dinculescu2010a,
author = {Dinculescu, M and Precup, D},
booktitle = {Proc. 27th Int. Conf. Mach. Learn.},
pages = {895--902},
title = {{Approximate predictive representations of partially observable systems}},
volume = {10}
}
@article{schultz1997a,
author = {Schultz, W and Dayan, P and Montague, P R},
journal = {Science (80-. ).},
number = {5306},
pages = {1593--1599},
title = {{A neural substrate of prediction and reward}},
volume = {275}
}
@misc{fonteneau2008a,
author = {Fonteneau, R and Wehenkel, L and Ernst, D},
title = {{Variable selection for dynamic treatment regimes: a reinforcement learning approach}}
}
@book{niv2009b,
address = {In},
author = {Niv, Y and Montague, P R},
pages = {331--351},
publisher = {Neuroeconomics. Elsevier},
title = {{Theoretical and empirical studies of learning}}
}
@article{srivastava2014a,
author = {Srivastava, N and Hinton, G E and Krizhevsky, A and Sutskever, I and Salakhutdinov, R},
journal = {J. Mach. Learn. Res.},
number = {1},
pages = {1929--1958},
title = {{Dropout: a simple way to prevent neural networks from overfitting.}},
volume = {15}
}
@incollection{munos2016a,
author = {Munos, R and Stepleton, T and Harutyunyan, A and Bellemare, M},
booktitle = {Adv. Neural Inf. Process. Syst.},
pages = {1046--1054},
title = {{Safe and efficient off-policy reinforcement learning}}
}
@misc{henderson2017b,
author = {Henderson, P and Islam, R and Bachman, P and Pineau, J and Precup, D and Meger, D},
edition = {arXiv prep},
title = {{Deep Reinforcement Learning that Matters}}
}
@article{munos2002a,
author = {Munos, R and Moore, A},
journal = {Mach. Learn.},
number = {2},
pages = {291--323},
title = {{Variable resolution discretization in optimal control}},
volume = {49}
}
@misc{warnell2017a,
author = {Warnell, G and Waytowich, N and Lawhern, V and Stone, P},
edition = {arXiv prep},
title = {{Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces}}
}
@article{moore1990a,
author = {Moore, A W},
journal = {Comput. Chem. Eng.},
number = {4-5},
pages = {667--682},
title = {{Efficient memory-based learning for robot control}},
volume = {23}
}
@misc{you2017a,
author = {You, Y and Pan, X and Wang, Z and Lu, C},
edition = {arXiv prep},
title = {{Virtual to Real Reinforcement Learning for Autonomous Driving}}
}
@article{james2003a,
author = {James, G M},
journal = {Mach. Learn.},
number = {2},
pages = {115--135},
title = {{Variance and bias for general loss functions}},
volume = {51}
}
@misc{zoph2016a,
author = {Zoph, B and Le, Q V},
edition = {arXiv prep},
title = {{Neural architecture search with reinforcement learning}}
}
@techreport{Bradtke1996,
abstract = {We introduce two new temporal difference (TD) algorithms based on the theory of linear least-squares function approximation. We define an algorithm we call Least-Squares TD (LS TD) for which we prove probability-one convergence when it is used with a function approximator linear in the adjustable parameters. We then define a recursive version of this algorithm, Recursive Least-Squares TD (RLS TD). Although these new TD algorithms require more computation per time-step than do Sutton's TD(A) algorithms, they are more efficient in a statistical sense because they extract more information from training experiences. We describe a simulation experiment showing the substantial improvement in learning rate achieved by RLS TD in an example Markov prediction problem. To quantify this improvement, we introduce the TD error variance of a Markov chain, arc,, and experimentally conclude that the convergence rate of a TD algorithm depends linearly on {\~{}}ro. In addition to converging more rapidly, LS TD and RLS TD do not have control parameters, such as a learning rate parameter, thus eliminating the possibility of achieving poor performance by an unlucky choice of parameters.},
annote = {LSTD paper},
author = {Bradtke, Steven J and Barto, Andrew G},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bradtke, Barto - 1996 - Linear Least-Squares Algorithms for Temporal Difference Learning.pdf:pdf},
keywords = {Least-Squares,Markov Decision Problems,Reinforcement learning,Temporal Difference Methods},
pages = {33--57},
publisher = {Kluwer Academic Publishers},
title = {{Linear Least-Squares Algorithms for Temporal Difference Learning}},
url = {https://link.springer.com/content/pdf/10.1007{\%}2FBF00114723.pdf},
volume = {22},
year = {1996}
}
@book{silver2014a,
address = {In},
author = {Silver, D and Lever, G and Heess, N and Degris, T and Wierstra, D and Riedmiller, M},
publisher = {ICML},
title = {{Deterministic Policy Gradient Algorithms}}
}
@misc{teh2017a,
author = {Teh, Y W and Bapst, V and Czarnecki, W M and Quan, J and Kirkpatrick, J and Hadsell, R and Heess, N and Pascanu, R},
edition = {arXiv prep},
title = {{Distral: Robust Multitask Reinforcement Learning}}
}
@article{Phillips2008,
abstract = {1 The overall functional capacity of the liver was evaluated using [35S]-bromosulphophthalein (BSP, 100 mg/kg, i.v.) in biliary fistulated adult rats pretreated orally with different doses of paracetamol (APAP) for varying time intervals. 2 The maximal hepatic damage occurred between 12-18 h after single doses of APAP (0.5 or 1 g/kg); hepatic excretory function returned to control levels by 48-72 hours. 3 Administration of either 0.5 or 1 g/kg APAP 18 h before BSP caused a dose-dependent inhibition of the choleretic effect of BSP and of the 60 min cumulative excretion of the dye, but conversely, produced a significant increase in the liver and plasma concentrations of 35S. 4 Following acute (0.25 g/kg), or subacute (0.5 g/kg, twice daily for 7 days) treatment with APAP, the total excretion of 35S in bile and the retention of 35S in the liver or plasma remained essentially the same as that for the controls. 5 In rats given single doses of 1 g/kg APAP, the hepatic uptake of the dye was significantly increased during the early stages of intoxication, while the opposite effect was observed at late periods. 6 The bile flow appeared to be inversely related to the excretion of unchanged BSP, and directly related to the excretion of the major BSP conjugate in bile. 7 The hepatic clearance of BSP was more rapid in rats treated subacutely with 0.5 or 1 g/kg APAP, than in those treated acutely with equal doses, suggesting that the intensity of APAP-induced hepatotoxicity became less severe after the repeated administration of this drug. 8 It is concluded that the hepatic uptake, metabolism and excretion of BSP are reversibly impaired following APAP-induced liver injury.},
title = {{No Title}}
}
@article{deng2017a,
author = {Deng, Y and Bao, F and Kong, Y and Ren, Z and Dai, Q},
journal = {IEEE Trans. neural networks Learn. Syst.},
number = {3},
pages = {653--664},
title = {{Deep direct reinforcement learning for financial signal representation and trading}},
volume = {28}
}
@misc{lillicrap2015a,
author = {Lillicrap, T P and Hunt, J J and Pritzel, A and Heess, N and Erez, T and Tassa, Y and Silver, D and Wierstra, D},
edition = {arXiv prep},
title = {{Continuous control with deep reinforcement learning}}
}
@misc{duan-a,
author = {Duan, Y and Schulman, J and Chen, X and Bartlett, P L and Sutskever, I and P.},
edition = {arXiv prep},
title = {{Abbeel. 2016b. “RL2: Fast Reinforcement Learning via Slow Reinforcement Learning”}}
}
@article{dem2006a,
author = {Dem{\v{s}}ar, J},
journal = {J. Mach. Learn. Res.},
pages = {1--30},
title = {{Statistical comparisons of classifiers over multiple data sets}},
volume = {7}
}
@misc{unknown-a,
edition = {arXiv prep},
title = {{Asymmetric Actor Critic for Image-Based Robot Learning”}}
}
@article{Fithian2013,
abstract = {Statistical modeling of presence-only data has attracted much recent attention in the ecological literature, leading to a proliferation of methods, including the inhomogeneous Poisson process (IPP) model, maximum entropy (Maxent) modeling of species distributions and logistic regression models. Several recent articles have shown the close relationships between these methods. We explain why the IPP intensity function is a more natural object of inference in presence-only studies than occurrence probability (which is only defined with reference to quadrat size), and why presence-only data only allows estimation of relative, and not absolute intensity of species occurrence. All three of the above techniques amount to parametric density estimation under the same exponential family model (in the case of the IPP, the fitted density is multiplied by the number of presence records to obtain a fitted intensity). We show that IPP and Maxent give the exact same estimate for this density, but logistic regression in general yields a different estimate in finite samples. When the model is misspecified - as it practically always is - logistic regression and the IPP may have substantially different asymptotic limits with large data sets. We propose ``infinitely weighted logistic regression,'' which is exactly equivalent to the IPP in finite samples. Consequently, many already-implemented methods extending logistic regression can also extend the Maxent and IPP models in directly analogous ways using this technique.},
archivePrefix = {arXiv},
arxivId = {1207.6950},
author = {Fithian, William and Hastie, Trevor},
doi = {10.1214/13-AOAS667},
eprint = {1207.6950},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fithian, Hastie - 2013 - Finite-sample equivalence in statistical models for presence-only data.pdf:pdf},
isbn = {1932-6157},
issn = {19326157},
journal = {Ann. Appl. Stat.},
keywords = {Case-control sampling,Logistic regression,Maximum entropy,Poisson process models,Presence-only data,Species modeling},
number = {4},
pages = {1917--1939},
pmid = {25493106},
title = {{Finite-sample equivalence in statistical models for presence-only data}},
volume = {7},
year = {2013}
}
@incollection{ho2016a,
author = {Ho, J and Ermon, S},
booktitle = {Adv. Neural Inf. Process. Syst.},
pages = {4565--4573},
title = {{Generative adversarial imitation learning}}
}
@article{lin1992a,
author = {Lin, L.-J.},
journal = {Mach. Learn.},
number = {3-4},
pages = {293--321},
title = {{Self-improving reactive agents based on reinforcement learning, planning and teaching}},
volume = {8}
}
@misc{brys2014a,
author = {Brys, T and Harutyunyan, A and Vrancx, P and Taylor, M E and Kudenko, D and Now{\'{e}}, A},
title = {{“Multi-objectivization of reinforcement learning problems by reward shaping”. In: Neural Networks (IJCNN), 2014 International Joint Conference on. IEEE. 2315–2322}}
}
@thesis{watkins1989a,
address = {King's College, Cambridge},
author = {Watkins, C.J.C.H.},
title = {{“Learning from delayed rewards”. PhD thesis}}
}
@inproceedings{simmons-aaai88,
author = {Simmons, R G},
booktitle = {Proc. AAAI-88},
title = {{A Theory of Debugging Plans and Interpretations}},
year = {1988}
}
@article{Roy2016,
abstract = {In this paper a framework for Automatic Query Expansion (AQE) is proposed using distributed neural language model word2vec. Using semantic and contextual relation in a distributed and unsupervised framework, word2vec learns a low dimensional embedding for each vocabulary entry. Using such a framework, we devise a query expansion technique, where related terms to a query are obtained by K-nearest neighbor approach. We explore the performance of the AQE methods, with and without feedback query expansion, and a variant of simple K-nearest neighbor in the proposed framework. Experiments on standard TREC ad-hoc data (Disk 4, 5 with query sets 301-450, 601-700) and web data (WT10G data with query set 451-550) shows significant improvement over standard term-overlapping based retrieval methods. However the proposed method fails to achieve comparable performance with statistical co-occurrence based feedback method such as RM3. We have also found that the word2vec based query expansion methods perform similarly with and without any feedback information.},
archivePrefix = {arXiv},
arxivId = {1606.07608},
author = {Roy, Dwaipayan and Paul, Debjyoti and Mitra, Mandar and Garain, Utpal},
eprint = {1606.07608},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Roy et al. - 2016 - Using Word Embeddings for Automatic Query Expansion.pdf:pdf},
month = {jun},
title = {{Using Word Embeddings for Automatic Query Expansion}},
url = {http://arxiv.org/abs/1606.07608},
year = {2016}
}
@misc{rusu2015a,
author = {Rusu, A A and Colmenarejo, S G and Gulcehre, C and Desjardins, G and Kirkpatrick, J and Pascanu, R and Mnih, V and Kavukcuoglu, K and Hadsell, R},
edition = {arXiv prep},
title = {{Policy distillation}}
}
@article{liaw2002a,
author = {Liaw, A and Wiener, M},
journal = {R news},
number = {3},
pages = {18--22},
title = {{Classification and regression by randomForest}},
volume = {2}
}
@inproceedings{abbeel2004a,
author = {Abbeel, P and Ng, A Y},
booktitle = {Proc. twenty-first Int. Conf. Mach. Learn. ACM},
title = {{Apprenticeship learning via inverse reinforcement learning}}
}
@article{brafman2003a,
author = {Brafman, R I and Tennenholtz, M},
journal = {J. Mach. Learn. Res.},
pages = {213--231},
title = {{R-max-a general polynomial time algorithm for near-optimal reinforcement learning}},
volume = {3}
}
@misc{bruegmann1993a,
author = {Br{\"{u}}gmann, B},
title = {{Monte carlo go}},
type = {Tech. rep. Citeseer}
}
@article{salge2014a,
author = {Salge, C and Glackin, C and Polani, D},
journal = {Entropy},
number = {5},
pages = {2789--2819},
title = {{Changing the environment based on empowerment as intrinsic motivation}},
volume = {16}
}
@inproceedings{dearden2011a,
author = {{Dearden R.}, N Friedman and 1998, S Russell.},
booktitle = {Proc. 28th Int. Conf. Mach. Learn.},
pages = {465--472},
title = {{Bayesian Q-learning}},
volume = {11}
}
@incollection{oh2015a,
author = {Oh, J and Guo, X and Lee, H and Lewis, R L and Singh, S},
booktitle = {Adv. Neural Inf. Process. Syst.},
pages = {2863--2871},
title = {{Actionconditional video prediction using deep networks in atari games}}
}
@inproceedings{he2016a,
author = {He, K and Zhang, X and Ren, S and Sun, J},
booktitle = {Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
pages = {770--778},
title = {{Deep residual learning for image recognition}}
}
@article{beattie2016a,
edition = {arXiv prep},
journal = {Sadik, al},
title = {{DeepMind Lab}}
}
@incollection{dietterich2009a,
author = {Dietterich, T G},
booktitle = {Asian Conf. Mach. Learn. Springer},
pages = {1--5},
title = {{Machine learning and ecosystem informatics: challenges and opportunities}}
}
@inproceedings{adorf,
author = {Adorf, H M and Johnston, M D},
booktitle = {Proc. Int. Jt. Conf. Neural Networks},
title = {{A Discrete Stochastic Neural Network Algorithm for Constraint Satisfaction Problems}},
year = {1990}
}
@incollection{thrun1992a,
author = {Thrun, S B},
booktitle = {Adv. Neural Inf. Process. Syst.},
title = {{Efficient exploration in reinforcement learning}}
}
@article{Ward2009,
author = {Ward, Gill and Hastie, Trevor and Barry, Simon and Elith, Jane and Leathwick, John R.},
doi = {10.1111/j.1541-0420.2008.01116.x},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ward et al. - 2009 - Presence-Only Data and the EM Algorithm.pdf:pdf;:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ward et al. - 2009 - Presence-Only Data and the EM Algorithm(2).pdf:pdf},
issn = {0006341X},
journal = {Biometrics},
keywords = {Boosted trees,EM algorithm,Logistic model,Presence‐only data,Use‐availability data},
month = {jun},
number = {2},
pages = {554--563},
publisher = {Wiley/Blackwell (10.1111)},
title = {{Presence-Only Data and the EM Algorithm}},
url = {http://doi.wiley.com/10.1111/j.1541-0420.2008.01116.x},
volume = {65},
year = {2009}
}
@misc{hessel2017a,
author = {Hessel, M and Modayil, J and van Hasselt, H and Schaul, T and Ostrovski, G and Dabney, W and Horgan, D and Piot, B and Azar, M and Silver, D},
edition = {arXiv prep},
title = {{Rainbow: Combining Improvements in Deep Reinforcement Learning}}
}
@misc{machado2017a,
author = {Machado, M C and Bellemare, M G and Talvitie, E and Veness, J and Hausknecht, M and Bowling, M},
edition = {arXiv prep},
title = {{Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents}}
}
@article{sutton1988a,
author = {Sutton, R S},
journal = {Mach. Learn.},
number = {1},
pages = {9--44},
title = {{Learning to predict by the methods of temporal differences}},
volume = {3}
}
@article{Mnih2015,
abstract = {An artificial agent is developed that learns to play a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mnih et al. - 2015 - Human-level control through deep reinforcement learning.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
keywords = {Computer science},
month = {feb},
number = {7540},
pages = {529--533},
publisher = {Nature Publishing Group},
title = {{Human-level control through deep reinforcement learning}},
url = {http://www.nature.com/articles/nature14236},
volume = {5aaIA18},
year = {2015}
}
@article{papernot2016technical,
abstract = {CleverHans is a software library that provides standardized reference implementations of adversarial example construction techniques and adversarial training. The library may be used to develop more robust machine learning models and to provide standardized benchmarks of models' performance in the adversarial setting. Benchmarks constructed without a standardized implementation of adversarial example construction are not comparable to each other, because a good result may indicate a robust model or it may merely indicate a weak implementation of the adversarial example construction procedure. This technical report is structured as follows. Section 1 provides an overview of adversarial examples in machine learning and of the CleverHans software. Section 2 presents the core functionalities of the library: namely the attacks based on adversarial examples and defenses to improve the robustness of machine learning models to these attacks. Section 3 describes how to report benchmark results using the library. Section 4 describes the versioning system.},
archivePrefix = {arXiv},
arxivId = {1610.00768},
author = {Papernot, Nicolas and Faghri, Fartash and Carlini, Nicholas and Goodfellow, Ian and Feinman, Reuben and Kurakin, Alexey and Xie, Cihang and Sharma, Yash and Brown, Tom and Roy, Aurko and Matyasko, Alexander and Behzadan, Vahid and Hambardzumyan, Karen and Zhang, Zhishuai and Juang, Yi-Lin and Li, Zhi and Sheatsley, Ryan and Garg, Abhibhav and Uesato, Jonathan and Gierke, Willi and Dong, Yinpeng and Berthelot, David and Hendricks, Paul and Rauber, Jonas and Long, Rujun and McDaniel, Patrick},
eprint = {1610.00768},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Papernot et al. - 2016 - Technical Report on the CleverHans v2.1.0 Adversarial Examples Library.pdf:pdf},
month = {oct},
title = {{Technical Report on the CleverHans v2.1.0 Adversarial Examples Library}},
url = {http://arxiv.org/abs/1610.00768},
year = {2016}
}
@article{bellemare2013a,
author = {Bellemare, M G and Naddaf, Y and Veness, J and Bowling, M},
journal = {J. Artif. Intell. Res.},
pages = {253--279},
title = {{The Arcade Learning Environment: An evaluation platform for general agents.}},
volume = {47}
}
@book{fukushima1982a,
author = {Fukushima, K and Miyake, S},
pages = {267--285},
publisher = {Springer},
title = {{“Neocognitron: A self-organizing neural network model for a mechanism of visual pattern recognition”. In: Competition and cooperation in neural nets}}
}
@incollection{islam2017a,
author = {Islam, R and Henderson, P and Gomrokchi, M and Precup, D},
booktitle = {ICML Reprod. Mach. Learn. Work.},
title = {{Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control}}
}
@inproceedings{li2011a,
author = {Li, L and Chu, W and Langford, J and Wang, X},
booktitle = {Proc. fourth ACM Int. Conf.},
pages = {297--306},
title = {{Unbiased offline evaluation of contextual-bandit-based news article recommendation algorithms}}
}
@inproceedings{jiang-b,
booktitle = {Proc. 2015 Int. Conf. Auton. Agents Multiagent Syst. Int. Found.},
title = {{The Dependence of Effective Planning Horizon on Model Accuracy}}
}
@misc{burda2018a,
author = {Burda, Y and Edwards, H and Storkey, A and Klimov, O},
edition = {arXiv prep},
title = {{Exploration by Random Network Distillation}}
}
@misc{kansky2017a,
edition = {arXiv prep},
title = {{Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics}}
}
@incollection{montague2013a,
author = {Montague, P R},
pages = {271--277},
publisher = {Springer},
title = {{Reinforcement Learning Models Then-andNow: From Single Cells to Modern Neuroimaging}}
}
@incollection{kroon2009a,
author = {Kroon, M and Whiteson, S},
booktitle = {Mach. Learn. Appl. 2009. ICMLA'09. Int. Conf. on. IEEE},
pages = {324--330},
title = {{Automatic feature selection for model-based reinforcement learning in factored MDPs}}
}
@techreport{Lin2017,
abstract = {We introduce two tactics, namely the strategically-timed attack and the enchanting attack, to attack reinforcement learning agents trained by deep reinforcement learning algorithms using adversarial examples. In the strategically-timed attack, the adversary aims at minimizing the agent's reward by only attacking the agent at a small subset of time steps in an episode. Limiting the attack activity to this subset helps prevent detection of the attack by the agent. We propose a novel method to determine when an adversarial example should be crafted and applied. In the enchanting attack, the adversary aims at luring the agent to a designated target state. This is achieved by combining a generative model and a planning algorithm: while the generative model predicts the future states, the planning algorithm generates a preferred sequence of actions for luring the agent. A sequence of ad-versarial examples is then crafted to lure the agent to take the preferred sequence of actions. We apply the proposed tactics to the agents trained by the state-of-the-art deep reinforcement learning algorithm including DQN and A3C. In 5 Atari games, our strategically-timed attack reduces as much reward as the uniform attack (i.e., attacking at every time step) does by attacking the agent 4 times less often. Our enchanting attack lures the agent toward designated target states with a more than 70{\%} success rate. Example videos are available at http: //yclin.me/adversarial{\_}attack{\_}RL/.},
author = {Lin, Yen-Chen and Hong, Zhang-Wei and Liao, Yuan-Hong and Shih, Meng-Li and Liu, Ming-Yu and Sun, Min},
keywords = {Machine Learning: Deep Learning,Machine Learning: New Problems,Machine Learning: Reinforcement Learning,Multidisciplinary Topics and Applications: AI{\&}Secu},
title = {{Tactics of Adversarial Attack on Deep Reinforcement Learning Agents}},
url = {https://www.ijcai.org/proceedings/2017/0525.pdf},
year = {2017}
}
@misc{bellemare2017a,
author = {Bellemare, M G and Dabney, W and Munos, R},
edition = {arXiv prep},
title = {{A distributional perspective on reinforcement learning}}
}
@misc{lowe2017a,
author = {Lowe, R and Wu, Y and Tamar, A and Harb, J and Abbeel, P and Mordatch, I},
edition = {arXiv prep},
title = {{Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments}}
}
@article{williams1992a,
author = {Williams, R J},
journal = {Mach. Learn.},
number = {3-4},
pages = {229--256},
title = {{Simple statistical gradient-following algorithms for connectionist reinforcement learning}},
volume = {8}
}
@misc{ioffe2015a,
author = {Ioffe, S and Szegedy, C},
edition = {arXiv prep},
title = {{Batch normalization: Accelerating deep network training by reducing internal covariate shift}}
}
@inproceedings{mandel2014a,
author = {Mandel, T and Liu, Y.-E. and Levine, S and Brunskill, E and Popovic, Z},
title = {{“Offline policy evaluation across representations with applications to educational games”. In: Proceedings of the 2014 international conference on Autonomous agents and multi-agent systems. International Foundation for Autonomous Agents and Multiagent Sys}}
}
@misc{aytar2018a,
author = {Aytar, Y and Pfaff, T and Budden, D and Paine, T L and Wang, Z and de Freitas, N},
edition = {arXiv prep},
title = {{Playing hard exploration games by watching YouTube}}
}
@incollection{ghavamzadeh2015a,
author = {Ghavamzadeh, M and Mannor, S and Pineau, J and Tamar, A},
booktitle = {Found. Trends{\textregistered} Mach. Learn.},
number = {5-6},
pages = {359--483},
title = {{Bayesian reinforcement learning: A survey}},
volume = {8}
}
@misc{real2017a,
author = {Real, E and Moore, S and Selle, A and Saxena, S and Suematsu, Y L and Le, Q and Kurakin, A},
edition = {arXiv prep},
title = {{Large-Scale Evolution of Image Classifiers}}
}
@misc{casadevall2010a,
author = {Casadevall, A and Fang, F C},
title = {{Reproducible science}}
}
@article{turing1953a,
author = {Turing, A M},
journal = {Faster than thought},
title = {{Digital computers applied to games}}
}
@misc{schulman2017a,
author = {Schulman, J and Abbeel, P and Chen, X},
edition = {arXiv prep},
title = {{Equivalence Between Policy Gradients and Soft Q-Learning}}
}
@article{campbell2002a,
author = {Campbell, M and Hoane, A J and Hsu, F.-h},
journal = {Artificial},
number = {1-2},
pages = {57--83},
title = {{Deep blue}},
volume = {134}
}
@book{ueno2017a,
author = {Ueno, S and Osawa, M and Imai, M and Kato, T and Yamakawa, H},
publisher = {Springer},
title = {{““Re: ROS”: Prototyping of Reinforcement Learning Environment for Asynchronous Cognitive Architecture”. In: First International Early Research Career Enhancement School on Biologically Inspired Cognitive Architectures}}
}
@incollection{ravindran2004a,
author = {Ravindran, B and Barto, A G},
title = {{“An algebraic approach to abstraction in reinforcement learning”. PhD thesis}},
url = {at Amherst.}
}
@misc{wu2016a,
author = {Wu, Y and Tian, Y},
title = {{Training agent for first-person shooter game with actor-critic curriculum learning}}
}
@misc{wang2016a,
author = {Wang, Z and Bapst, V and Heess, N and Mnih, V and Munos, R and Kavukcuoglu, K and de Freitas, N},
edition = {arXiv prep},
title = {{Sample efficient actor-critic with experience replay}}
}
@misc{zhu2016a,
author = {Zhu, Y and Mottaghi, R and Kolve, E and Lim, J J and Gupta, A and FeiFei, L and Farhadi, A},
edition = {arXiv prep},
title = {{Target-driven visual navigation in indoor scenes using deep reinforcement learning}}
}
@misc{macglashan2017a,
author = {MacGlashan, J and Ho, M K and Loftin, R and Peng, B and Roberts, D and Taylor, M E and Littman, M L},
edition = {arXiv prep},
title = {{Interactive Learning from PolicyDependent Human Feedback}}
}
@incollection{sukhbaatar2016a,
author = {Sukhbaatar, S and Szlam, A and Fergus, R},
booktitle = {Adv. Neural Inf. Process. Syst.},
pages = {2244--2252},
title = {{Learning multiagent communication with backpropagation}}
}
@article{Iyengar2005,
abstract = {In this paper we propose a robust formulation for discrete time dynamic programming (DP). The objective of the robust formulation is to systematically mitigate the sensitivity of the DP optimal policy to ambiguity in the underlying transition probabilities. The ambiguity is modeled by associating a set of conditional measures with each state-action pair. Consequently, in the robust formulation each policy has a set of measures associated with it. We prove that when this set of measures has a certain "rectangularity" property, all of the main results for finite and infinite horizon DP extend to natural robust counterparts. We discuss techniques from Nilim and El Ghaoui [17] for constructing suitable sets of conditional measures that allow one to efficiently solve for the optimal robust policy. We also show that robust DP is equivalent to stochastic zero-sum games with perfect information. 1. Introduction. This paper is concerned with sequential decision making in uncertain environments. Decisions are made in stages and each decision, in addition to providing an immediate reward, changes the context of future decisions; thereby affecting the future rewards. Due to the uncertain nature of the environment, there is limited information about both the immediate reward from each decision and the resulting future state. In order to achieve a good performance over all the stages, the decision maker has to trade-off the immediate payoff with future payoffs. Dynamic programming (DP) is the mathematical framework that allows the decision maker to efficiently compute a good overall strategy by succinctly encoding the evolving information state. In the DP formalism the uncertainty in the environment is modeled by a Markov process whose transition probability depends both on the information state and the action taken by the decision maker. It is assumed that the transition probability corresponding to each state-action pair is known to the decision maker, and the goal is to choose a policy, i.e., a rule that maps states to actions, that maximizes some performance measure. Puterman [20] provides a excellent introduction to the DP formalism and its various applications. In this paper, we assume that the reader has some prior knowledge of DP. The DP formalism encodes information in the form of a "reward-to-go" function (see Puterman [20] for details) and chooses an action that maximizes the sum of the immediate reward and the expected "reward-to-go." Thus, to compute the optimal action in any given state the "reward-to-go" function for all the future states must be known. In many applications of DP, the number of states and actions available in each state are large; consequently , the computational effort required to compute the optimal policy for a DP can be overwhelming-Bellman's "curse of dimensionality." For this reason, considerable recent research effort has focused on developing algorithms that compute an approximately optimal policy efficiently (Bertsekas and Tsitsiklis [5], de Farias and Van Roy [8]). Fortunately, for many applications the DP optimal policy can be computed with a modest computational effort. In this paper we restrict attention to this class of DPs. Typically, the transition probability of the underlying Markov process is estimated from historical data and is, therefore, subject to statistical errors. In current practice, these errors are ignored and the optimal policy is computed assuming that the estimate is, indeed, the true transition probability. The DP optimal policy is quite sensitive to perturbations in the transition probability},
author = {Iyengar, Garud N},
doi = {10.1287/moor.1040.0129},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Iyengar - 2005 - Robust Dynamic Programming.pdf:pdf},
issn = {1526-5471},
journal = {Robust Dyn. Program. Math. Oper. Res.},
keywords = {90C25 OR/MS subject classificati,Markov decision processes,ambiguity MSC2000 subject classification: Primary:,dynamic programming,robust optimization,secondary: 90C40,secondary: probability-Markov processes},
number = {2},
pages = {257--280},
title = {{Robust Dynamic Programming}},
url = {http://pubsonline.informs.org.https//doi.org/10.1287/moor.1040.0129http://www.informs.orghttp://www.columbia.edu/∼gi10},
volume = {30},
year = {2005}
}
@inproceedings{branavan2012a,
address = {Long},
author = {Branavan, S and Kushman, N and Lei, T and Barzilay, R},
booktitle = {Proc. 50th Annu. Meet. Assoc. Comput. Linguist.},
pages = {135},
publisher = {Papers-Volume 1. Association for Computational Linguistics. 126},
title = {{Learning high-level planning from text}}
}
@article{holroyd2002a,
author = {Holroyd, C B and Coles, M G},
journal = {Psychol. Rev.},
number = {4},
pages = {679},
title = {{The neural basis of human error processing: reinforcement learning, dopamine, and the error-related negativity.}},
volume = {109}
}
@misc{lipton2016a,
author = {Lipton, Z C and Gao, J and Li, L and Li, X and Ahmed, F and Deng, L},
edition = {arXiv prep},
title = {{Efficient exploration for dialogue policy learning with BBQ networks {\&} replay buffer spiking}}
}
@misc{stadie2015a,
author = {Stadie, B C and Levine, S and Abbeel, P},
edition = {arXiv prep},
title = {{Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models}}
}
@inproceedings{langley,
author = {Langley, P},
booktitle = {Proc. AAAI-92},
title = {{Systematic and Nonsystematic Search Strategies}},
year = {1992}
}
@misc{sadeghi2016a,
author = {Sadeghi, F and Levine, S},
edition = {arXiv prep},
title = {{CAD2RL: Real single-image flight without a single real image}}
}
@misc{paine2018a,
author = {Paine, T L and Colmenarejo, S G and Wang, Z and Reed, S and Aytar, Y and Pfaff, T and Hoffman, M W and Barth-Maron, G and Cabi, S and Budden, D},
edition = {arXiv prep},
title = {{One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL}}
}
@misc{neelakantan2015a,
author = {Neelakantan, A and Le, Q V and Sutskever, I},
edition = {arXiv prep},
title = {{Neural programmer: Inducing latent programs with gradient descent}}
}
@article{baker2016a,
author = {Baker, M},
journal = {Nat. News},
number = {7604},
pages = {452},
title = {1,500 scientists lift the lid on reproducibility},
volume = {533}
}
@article{FitzpatrickMC;GotelliNJ;Ellison2013,
abstract = {MaxEnt is one of the most widely used tools in ecology, biogeography, and evolution for modeling and mapping species distributions using presence-only occurrence records and associated environmental covariates. Despite its popularity, the exponential model implemented by MaxEnt does not directly estimate occurrence probability, the natural quantity of interest when modeling species distributions. Instead, MaxEnt generates an index of relative habitat suitability. MaxLike, a newly introduced maximum-likelihood technique, has been shown to overcome the problem of directly estimating the probability of occurrence using presence-only data. However, the performance and relative merits of MaxEnt and MaxLike remain largely untested, especially when modeling species with relatively few occurrence data that encompass only a portion of the geographic range of the species. Using geo- referenced occurrence records for six species of ants in New England, we provide comparisons of MaxEnt and MaxLike. We show that by most quantitative metrics, the performance of MaxLike exceeds that of MaxEnt, regardless of whether MaxEnt models account for sampling bias and include greater model complexity than implemented in MaxLike. More importantly, for most species, the relative suitability index estimated by MaxEnt oftenwas poorly correlated with the probability of occurrence estimated by MaxLike, suggesting that the two methods are estimating different quantities. For species distribution modeling, MaxLike, and similar models that are based on an explicit sampling process and that directly estimate probability of occurrence, should be considered as important alternatives to the widely-used MaxEnt framework.},
author = {{Fitzpatrick, MC; Gotelli, NJ; Ellison}, Am},
doi = {10.1890/es13-00066.1},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fitzpatrick, MC Gotelli, NJ Ellison - 2013 - MaxEnt vs. MaxLike Empirical comparisons with ant species distributions.pdf:pdf},
isbn = {2150-8925},
issn = {2150-8925},
journal = {Ecosphere},
keywords = {ecological niche modeling,myrmecology,new england,occurrence probability,presence-only data,species distribution modeling},
number = {May},
pages = {1--15},
title = {{MaxEnt vs. MaxLike: Empirical comparisons with ant species distributions}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:MaxEnt+versus+MaxLike+:+empirical+comparisons+with+ant+species+distributions{\#}0},
volume = {4},
year = {2013}
}
@misc{oh2017a,
author = {Oh, J and Singh, S and Lee, H},
edition = {arXiv prep},
title = {{Value Prediction Network}}
}
@article{Papernot2016,
abstract = {Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the first practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and find that they are misclassified by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. We find that their DNN misclassifies 84.24{\%} of the adversarial examples crafted with our substitute. We demonstrate the general applicability of our strategy to many ML techniques by conducting the same attack against models hosted by Amazon and Google, using logistic regression substitutes. They yield adversarial examples misclassified by Amazon and Google at rates of 96.19{\%} and 88.94{\%}. We also find that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder.},
archivePrefix = {arXiv},
arxivId = {1602.02697},
author = {Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian and Jha, Somesh and Celik, Z. Berkay and Swami, Ananthram},
doi = {10.1145/3052973.3053009},
eprint = {1602.02697},
isbn = {978-1-4503-4944-4},
issn = {1998-4049},
journal = {ACM Asia Conf. Comput. Commun. Secur.},
pmid = {21546725},
title = {{Practical Black-Box Attacks against Machine Learning}},
url = {http://arxiv.org/abs/1602.02697},
year = {2016}
}
@incollection{hochreiter2001a,
author = {Hochreiter, S and Younger, A S and Conwell, P R},
booktitle = {Int. Conf. Artif. Neural Networks. Springer},
pages = {87--94},
title = {{Learning to learn using gradient descent}}
}
@article{kaelbling1998a,
author = {Kaelbling, L P and Littman, M L and Cassandra, A R},
journal = {Artificial},
number = {1},
pages = {99--134},
title = {{Planning and acting in partially observable stochastic domains}},
volume = {101}
}
@article{sunehag2017a,
author = {Sunehag, P and Lever, G and Gruslys, A and Czarnecki, W M and Zambaldi, V and Jaderberg, M and Lanctot, M and Sonnerat, N and Leibo, J Z and K.},
edition = {arXiv prep},
journal = {Tuyls, al},
title = {{Value-Decomposition Networks For Cooperative Multi-Agent Learning}}
}
@misc{baird1995a,
author = {Baird, L},
pages = {30--37},
title = {{“Residual algorithms: Reinforcement learning with function approximation”. In: ICML}}
}
@misc{fazel-zarandi2017a,
author = {Fazel-Zarandi, M and Li, S.-W. and Cao, J and Casale, J and Henderson, P and Whitney, D and Geramifard, A},
edition = {arXiv prep},
title = {{Learning Robust Dialog Policies in Noisy Environments}}
}
@incollection{murphy2012a,
author = {Murphy, K P},
edition = {arXiv prep},
title = {{Machine Learning: A Probabilistic Perspective.}}
}
@article{Kuzi,
abstract = {We present a suite of query expansion methods that are based on word embeddings. Using Word2Vec's CBOW embedding approach, applied over the entire corpus on which search is performed, we select terms that are semantically related to the query. Our methods either use the terms to expand the original query or integrate them with the effective pseudo-feedback-based relevance model. In the former case, retrieval performance is significantly better than that of using only the query, and in the latter case the performance is significantly better than that of the relevance model.},
author = {Kuzi, Saar and Shtok, Anna and Kurland, Oren},
doi = {10.1145/2983323.2983876},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kuzi, Shtok, Kurland - Unknown - Query Expansion Using Word Embeddings.pdf:pdf},
isbn = {9781450340731},
title = {{Query Expansion Using Word Embeddings}},
url = {http://dx.doi.org/10.1145/2983323.2983876}
}
@article{Szegedy2014,
author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian J. and Fergus, Rob},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Szegedy et al. - 2014 - Intriguing properties of neural networks.pdf:pdf},
journal = {undefined},
title = {{Intriguing properties of neural networks}},
url = {https://www.semanticscholar.org/paper/Intriguing-properties-of-neural-networks-Szegedy-Zaremba/83bfdd6a2b28106b9fb66e52832c45f08b828541},
year = {2014}
}
@misc{santoro2017a,
author = {Santoro, A and Raposo, D and Barrett, D G and Malinowski, M and Pascanu, R and Battaglia, P and Lillicrap, T},
edition = {arXiv prep},
title = {{A simple neural network module for relational reasoning}}
}
@misc{bellemare2018a,
author = {Bellemare, M G and Castro, P S and Gelada, C and Saurabh, K and Moitra, S},
title = {{Dopamine}},
url = {https://github.com/google/dopamine.}
}
@misc{gruslys2017a,
author = {Gruslys, A and Azar, M G and Bellemare, M G and Munos, R},
edition = {arXiv prep},
title = {{The Reactor: A Sample-Efficient Actor-Critic Architecture}}
}
@article{jaquette1973a,
author = {Jaquette, S C},
journal = {Ann. Stat.},
number = {3},
pages = {496--505},
title = {{Markov decision processes with a new optimality criterion: Discrete time}},
volume = {1}
}
@article{perez-liebana2016a,
author = {Perez-Liebana, D and Samothrakis, S and Togelius, J and Schaul, T and Lucas, S M and Cou{\"{e}}toux, A and Lee, J and Lim, C.-U. and Thompson, T},
journal = {IEEE Trans. Comput. Intell. AI Games},
number = {3},
pages = {229--243},
title = {{The 2014 general video game playing competition}},
volume = {8}
}
@misc{van2016a,
author = {{Van Hasselt}, H and Guez, A and Silver, D},
title = {{“Deep Reinforcement Learning with Double Q-Learning.” In: AAAI. 2094–2100}}
}
@misc{schulman2015a,
author = {Schulman, J and Levine, S and Abbeel, P and Jordan, M I and Moritz, P},
title = {{“Trust Region Policy Optimization”. In: ICML. 1889–1897}}
}
@article{dhariwal2017a,
author = {Dhariwal, P and Hesse, C and Plappert, M and Radford, A and Schulman, J and Sidor, S and Wu, Y},
journal = {Neural Comput.},
number = {7},
pages = {1895--1923},
title = {{OpenAI Baselines}},
volume = {10}
}
@misc{pascanu2017a,
author = {Pascanu, R and Li, Y and Vinyals, O and Heess, N and Buesing, L and Racani{\`{e}}re, S and Reichert, D and Weber, T and Wierstra, D and Battaglia, P},
edition = {arXiv prep},
title = {{Learning model-based planning from scratch}}
}
@book{gordon1999a,
author = {Gordon, G J},
publisher = {Robotics Institute: 228},
title = {{Approximate solutions to Markov decision processes}}
}
@inproceedings{jiang-a,
author = {{Jiang N.}, A Kulesza and 2015a, S Singh.},
booktitle = {Proc. 32nd Int. Conf. Mach. Learn.},
pages = {179--188},
title = {{Abstraction selection in model-based reinforcement learning}},
volume = {15}
}
@book{russek2017a,
author = {Russek, E M and Momennejad, I and Botvinick, M M and Gershman, S J and Daw, N D},
publisher = {bioRxiv: 083857},
title = {{Predictive representations can link model-based reinforcement learning to model-free mechanisms}}
}
@misc{florensa2017a,
author = {Florensa, C and Duan, Y and Abbeel, P},
edition = {arXiv prep},
title = {{Stochastic neural networks for hierarchical reinforcement learning}}
}
@misc{schaul2015a,
author = {Schaul, T and Quan, J and Antonoglou, I and Silver, D},
edition = {arXiv prep},
title = {{Prioritized Experience Replay}}
}
@book{riedmiller2005a,
address = {In},
author = {Riedmiller, M},
pages = {317--328},
publisher = {Machine Learning: ECML},
title = {{Neural fitted Q iteration–first experiences with a data efficient neural reinforcement learning method}}
}
@misc{gregor2016a,
author = {Gregor, K and Rezende, D J and Wierstra, D},
edition = {arXiv prep},
title = {{Variational Intrinsic Control}}
}
@article{Warton2010,
abstract = {Presence-only data, point locations where a species has been recorded as being present, are often used in modeling the distribution of a species as a function of a set of explanatory variables-whether to map species occurrence , to understand its association with the environment, or to predict its response to environmental change. Currently, ecologists most commonly analyze presence-only data by adding randomly chosen "pseudo-absences" to the data such that it can be analyzed using logistic regression, an approach which has weaknesses in model specification, in interpretation, and in implementation. To address these issues, we propose Poisson point process model-ing of the intensity of presences. We also derive a link between the proposed approach and logistic regression-specifically, we show that as the number of pseudo-absences increases (in a regular or uniform random arrangement), logistic regression slope parameters and their standard errors converge to those of the corresponding Poisson point process model. We discuss the practical implications of these results. In particular, point process modeling offers a framework for choice of the number and location of pseudo-absences, both of which are currently chosen by ad hoc and sometimes ineffective methods in ecology, a point which we illustrate by example.},
author = {Warton, David I and Shepherd, Leah C},
doi = {10.1214/10-AOAS331},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Warton, Shepherd - 2010 - POISSON POINT PROCESS MODELS SOLVE THE {\&}quotPSEUDO-ABSENCE PROBLEM{\&}quot FOR PRESENCE-ONLY DATA IN ECOLOGY 1.pdf:pdf},
journal = {Ann. Appl. Stat.},
keywords = {Habitat modeling,occurrence data,pseudo-absences,quadrature points,species distribution modeling},
number = {3},
pages = {1383--1402},
title = {{POISSON POINT PROCESS MODELS SOLVE THE "PSEUDO-ABSENCE PROBLEM" FOR PRESENCE-ONLY DATA IN ECOLOGY 1}},
url = {https://projecteuclid.org/download/pdfview{\_}1/euclid.aoas/1287409378},
volume = {4},
year = {2010}
}
@misc{matiisen2017a,
author = {Matiisen, T and Oliver, A and Cohen, T and Schulman, J},
edition = {arXiv prep},
title = {{TeacherStudent Curriculum Learning}}
}
@book{hacker,
author = {Sussman, G J},
publisher = {New York: New American Elsevier},
title = {{A Computer Model of Skill Acquisition}},
year = {1975}
}
@incollection{florensa2018a,
author = {Florensa, C and Held, D and Geng, X and Abbeel, P},
booktitle = {Int. Conf. Mach. Learn.},
pages = {1514--1523},
title = {{Automatic goal generation for reinforcement learning agents}}
}
@inproceedings{bengio2009a,
author = {Bengio, Y and Louradour, J and Collobert, R and Weston, J},
booktitle = {Proc. 26th Annu. Int. Conf. Mach. Learn. ACM},
pages = {41--48},
title = {{Curriculum learning}}
}
@misc{jaques2018a,
author = {Jaques, N and Lazaridou, A and Hughes, E and Gulcehre, C and Ortega, P A and Strouse, D and Leibo, J Z and de Freitas, N},
edition = {arXiv prep},
title = {{Intrinsic Social Motivation via Causal Influence in Multi-Agent RL}}
}
@misc{jaderberg2016a,
author = {Jaderberg, M and Mnih, V and Czarnecki, W M and Schaul, T and Leibo, J Z and Silver, D and Kavukcuoglu, K},
edition = {arXiv prep},
title = {{Reinforcement learning with unsupervised auxiliary tasks}}
}
@misc{higgins2017a,
author = {Higgins, I and Pal, A and Rusu, A A and Matthey, L and Burgess, C P and Pritzel, A and Botvinick, M and Blundell, C and Lerchner, A},
edition = {arXiv prep},
title = {{Darla: Improving zero-shot transfer in reinforcement learning}}
}
@article{fonteneau2013a,
author = {Fonteneau, R and Murphy, S A and Wehenkel, L and Ernst, D},
journal = {Ann. Oper. Res.},
number = {1},
pages = {383--416},
title = {{Batch mode reinforcement learning based on the synthesis of artificial trajectories}},
volume = {208}
}
@misc{zhang2016a,
author = {Zhang, C and Bengio, S and Hardt, M and Recht, B and Vinyals, O},
edition = {arXiv prep},
title = {{Understanding deep learning requires rethinking generalization}}
}
@inproceedings{littman1994a,
author = {Littman, M L},
booktitle = {Proc. Elev. Int. Conf. Mach. Learn.},
pages = {157--163},
title = {{Markov games as a framework for multi-agent reinforcement learning}},
volume = {157}
}
@misc{mirowski2016a,
author = {Mirowski, P and Pascanu, R and Viola, F and Soyer, H and Ballard, A and Banino, A and Denil, M and Goroshin, R and Sifre, L and Kavukcuoglu, K},
edition = {arXiv prep},
title = {{Learning to navigate in complex environments}}
}
@article{lecun2015a,
author = {LeCun, Y and Bengio, Y and Hinton, G},
journal = {Nature},
number = {7553},
pages = {436--444},
title = {{Deep learning}},
volume = {521}
}
@misc{hadfield-menell2016a,
author = {Hadfield-Menell, D and Russell, S J and Abbeel, P and Dragan, A},
title = {{“Cooperative inverse reinforcement learning”. In: Advances in neural information processing systems. 3909–3917}}
}
@article{Bengio2009,
abstract = {Theoretical results suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g., in vision, language, and other AI-level tasks), one may need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This monograph discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.},
archivePrefix = {arXiv},
arxivId = {submit/0500581},
author = {Bengio, Y.},
doi = {10.1561/2200000006},
eprint = {0500581},
isbn = {2200000006},
issn = {1935-8237},
journal = {Found. Trends{\textregistered} Mach. Learn.},
pmid = {17348934},
primaryClass = {submit},
title = {{Learning Deep Architectures for AI}},
year = {2009}
}
@misc{fran2017a,
author = {Fran{\c{c}}ois-Lavet, V and Ernst, D and Raphael, F},
edition = {arXiv prep},
title = {{On overfitting and asymptotic bias in batch reinforcement learning with partial observability}}
}
@misc{christiano2017a,
author = {Christiano, P and Leike, J and Brown, T B and Martic, M and Legg, S and Amodei, D},
edition = {arXiv prep},
title = {{Deep reinforcement learning from human preferences}}
}
@inproceedings{hauskrecht1998a,
author = {Hauskrecht, M and Meuleau, N and Kaelbling, L P and Dean, T and Boutilier, C},
booktitle = {Proc. Fourteenth Conf. Uncertain. Artif. Intell.},
pages = {220--229},
publisher = {Morgan Kaufmann Publishers Inc},
title = {{Hierarchical solution of Markov decision processes using macro-actions}}
}
@misc{sutton1984a,
author = {Sutton, R S},
title = {{Temporal credit assignment in reinforcement learning}}
}
@article{stones,
author = {Stone, H S and Stone, J M},
journal = {IBM J. Res. Dev.},
pages = {464--474},
publisher = {International Business Machines.},
title = {{Efficient Search Techniques - An Empirical Study of the N-Queens Problem}},
volume = {31},
year = {1987}
}
@incollection{levine2013a,
author = {Levine, S and Koltun, V},
booktitle = {Int. Conf. Mach. Learn.},
pages = {1--9},
title = {{Guided policy search}}
}
@incollection{thomas2014a,
author = {Thomas, P},
booktitle = {Int. Conf. Mach. Learn.},
pages = {441--448},
title = {{Bias in natural actor-critic algorithms}}
}
@misc{gelly2006a,
author = {Gelly, S and Wang, Y and Munos, R and Teytaud, O},
title = {{Modification of UCT with patterns in Monte-Carlo Go}}
}
@incollection{olah2017a,
author = {Olah, C and Mordvintsev, A and Schubert, L},
title = {{Feature Visualization}},
url = {https://distill.pub/2017/feature-visualization.}
}
@misc{osband2016a,
author = {Osband, I and Blundell, C and Pritzel, A and Roy, B.Van},
edition = {arXiv prep},
title = {{Deep Exploration via Bootstrapped DQN}}
}
@misc{singh1994a,
author = {Singh, S P and Jaakkola, T S and Jordan, M I},
pages = {284--292},
title = {{“Learning Without State-Estimation in Partially Observable Markovian Decision Processes.” In: ICML}}
}
@misc{kalakrishnan2013a,
author = {Kalakrishnan, M and Pastor, P and Righetti, L and Schaal, S},
title = {{“Learning objective functions for manipulation”. In: Robotics and Automation (ICRA), 2013 IEEE International Conference on. IEEE. 1331–1336}}
}
@techreport{Bellemare2017,
abstract = {In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman's equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.},
annote = {From Duplicate 1 (A Distributional Perspective on Reinforcement Learning - Bellemare, Marc G; Dabney, Will; Munos, R{\'{e}}mi)

Distributional Bellman Operator},
archivePrefix = {arXiv},
arxivId = {1707.06887v1},
author = {Bellemare, Marc G and Dabney, Will and Munos, R{\'{e}}mi},
eprint = {1707.06887v1},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bellemare, Dabney, Munos - 2017 - A Distributional Perspective on Reinforcement Learning.pdf:pdf;:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bellemare, Dabney, Munos - 2017 - A Distributional Perspective on Reinforcement Learning(2).pdf:pdf},
title = {{A Distributional Perspective on Reinforcement Learning}},
url = {https://arxiv.org/pdf/1707.06887.pdf http://proceedings.mlr.press/v70/bellemare17a/bellemare17a.pdf},
year = {2017}
}
@misc{todorov2012a,
author = {Todorov, E and Erez, T and Tassa, Y},
title = {{“MuJoCo: A physics engine for model-based control”. In: Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on. IEEE. 5026–5033}}
}
@article{abadi2016a,
author = {Abadi, M and Agarwal, A and Barham, P and Brevdo, E and Chen, Z and Citro, C and Corrado, G S and Davis, A and Dean, J and M.},
edition = {arXiv prep},
journal = {Devin, al},
title = {{TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}}
}
@incollection{heess2015a,
author = {Heess, N and Wayne, G and Silver, D and Lillicrap, T and Erez, T and Tassa, Y},
booktitle = {Adv. Neural Inf. Process. Syst.},
pages = {2944--2952},
title = {{Learning continuous control policies by stochastic value gradients}}
}
@misc{brundage2018a,
author = {Brundage, M and Avin, S and Clark, J and Toner, H and Eckersley, P and Garfinkel, B and Dafoe, A and Scharre, P and Zeitzoff, T and Filar, B},
edition = {arXiv prep},
title = {{The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation}}
}
@misc{sutton1996a,
author = {Sutton, R S},
title = {{“Generalization in reinforcement learning: Successful examples using sparse coarse coding”. Advances in neural information processing systems: 1038–1044}}
}
@techreport{Mnih2016b,
abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
author = {Mnih, Volodymyr and {Puigdom{\`{e}}nech Badia}, Adri{\`{a}} and Mirza, Mehdi and Graves, Alex and Harley, Tim and Lillicrap, Timothy P and Silver, David and Kavukcuoglu, Koray},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learning(2).pdf:pdf},
title = {{Asynchronous Methods for Deep Reinforcement Learning}},
url = {http://proceedings.mlr.press/v48/mniha16.pdf},
year = {2016}
}
@misc{schaarschmidt2017a,
author = {Schaarschmidt, M and Kuhnle, A and Fricke, K},
title = {{TensorForce: A TensorFlow library for applied reinforcement learning}}
}
@book{pavlov1927a,
author = {Pavlov, I P},
publisher = {Oxford University Press},
title = {{Conditioned reflexes}}
}
@misc{walsh2017a,
author = {Walsh, T},
title = {{It's Alive!: Artificial Intelligence from the Logic Piano to Killer Robots}}
}
@incollection{wender2012a,
author = {Wender, S and Watson, I},
booktitle = {Comput. Intell. Games (CIG), 2012 IEEE Conf. on. IEEE},
pages = {402--408},
title = {{Applying reinforcement learning to small scale combat in the real-time strategy game StarCraft: Broodwar}}
}
@incollection{brown2017a,
author = {Brown, N and Sandholm, T},
booktitle = {Int. Jt. Conf. Artif. Intell.},
title = {{Libratus: The Superhuman AI for No-Limit Poker}}
}
@article{schmidhuber2010a,
author = {Schmidhuber, J},
journal = {IEEE Trans. Auton. Ment. Dev.},
number = {3},
pages = {230--247},
title = {{Formal theory of creativity, fun, and intrinsic motivation (1990–2010)}},
volume = {2}
}
@misc{silver2016b,
author = {Silver, D and van Hasselt, H and Hessel, M and Schaul, T and Guez, A and Harley, T and Dulac-Arnold, G and Reichert, D and Rabinowitz, N and Barreto, A},
edition = {arXiv prep},
title = {{The predictron: End-to-end learning and planning}}
}
@inproceedings{peng1994a,
author = {Peng, J and Williams, R J},
booktitle = {Mach. Learn. Proc.},
pages = {226--232},
publisher = {Elsevier},
title = {{Incremental multi-step Q-learning}}
}
@article{silver2016a,
author = {Silver, D and Huang, A and Maddison, C J and Guez, A and Sifre, L and Driessche, G.Van Den and Schrittwieser, J and Antonoglou, I and Panneershelvam, V and Lanctot, M},
journal = {Nature},
number = {7587},
pages = {484--489},
title = {{Mastering the game of Go with deep neural networks and tree search}},
volume = {529}
}
@book{bouckaert2004a,
address = {In},
author = {Bouckaert, R R and Frank, E},
pages = {3--12},
publisher = {PAKDD. Springer},
title = {{Evaluating the replicability of significance tests for comparing learning algorithms}}
}
@misc{vapnik1998a,
author = {Vapnik, V N},
title = {{Statistical learning theory. Adaptive and learning systems for signal processing, communications, and control}}
}
@misc{foerster2017a,
author = {Foerster, J and Farquhar, G and Afouras, T and Nardelli, N and Whiteson, S},
edition = {arXiv prep},
title = {{Counterfactual Multi-Agent Policy Gradients}}
}
@misc{silver2013a,
author = {Silver, D L and Yang, Q and Li, L},
pages = {5},
title = {{“Lifelong Machine Learning Systems: Beyond Learning Algorithms.” In: AAAI Spring Symposium: Lifelong Machine Learning}},
volume = {13}
}
@article{singh2000a,
author = {Singh, S and Jaakkola, T and Littman, M L and Szepesv{\'{a}}ri, C},
journal = {Mach. Learn.},
number = {3},
pages = {287--308},
title = {{Convergence results for single-step on-policy reinforcement-learning algorithms}},
volume = {38}
}
@incollection{mordatch2015a,
author = {Mordatch, I and Lowrey, K and Andrew, G and Popovic, Z and Todorov, E V},
booktitle = {Adv. Neural Inf. Process. Syst.},
pages = {3132--3140},
title = {{Interactive control of diverse complex characters with neural networks}}
}
@misc{sutton2000a,
author = {Sutton, R S and McAllester, D A and Singh, S P and Mansour, Y},
title = {{“Policy gradient methods for reinforcement learning with function approximation”. In: Advances in neural information processing systems. 1057–1063}}
}
@misc{dosovitskiy2016a,
author = {Dosovitskiy, A and Koltun, V},
edition = {arXiv prep},
title = {{Learning to act by predicting the future}}
}
@article{o2016a,
author = {O'Donoghue, B and Munos, R and Kavukcuoglu, K and V.},
journal = {Mnih},
title = {{No Title}}
}
@misc{wang-a,
author = {Wang, J X and Kurth-Nelson, Z and Tirumala, D and Soyer, H and Leibo, J Z and Munos, R and Blundell, C and Kumaran, D and M.},
edition = {arXiv prep},
title = {{Botvinick. 2016a. “Learning to reinforcement learn”}}
}
@misc{reed2015a,
author = {Reed, S and Freitas, N.De},
edition = {arXiv prep},
title = {{Neural programmer-interpreters}}
}
@article{shannon-a,
author = {Shannon, C},
journal = {Philos. Mag.},
number = {314},
title = {{1950. “Programming a Computer for Playing Chess”}},
volume = {41}
}
@incollection{tamar2016a,
author = {Tamar, A and Levine, S and Abbeel, P and WU, Y and Thomas, G},
booktitle = {Adv. Neural Inf. Process. Syst.},
pages = {2146--2154},
title = {{Value iteration networks}}
}
@misc{duan2017a,
author = {Duan, Y and Andrychowicz, M and Stadie, B and Ho, J and Schneider, J and Sutskever, I and Abbeel, P and Zaremba, W},
edition = {arXiv prep},
title = {{One-Shot Imitation Learning}}
}
@misc{harari2014a,
author = {Harari, Y N},
title = {{Sapiens: A brief history of humankind}}
}
@misc{chebotar2018a,
author = {Chebotar, Y and Handa, A and Makoviychuk, V and Macklin, M and Issac, J and Ratliff, N and Fox, D},
edition = {arXiv prep},
title = {{Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience}}
}
@article{Phillips2008,
abstract = {1 The overall functional capacity of the liver was evaluated using [35S]-bromosulphophthalein (BSP, 100 mg/kg, i.v.) in biliary fistulated adult rats pretreated orally with different doses of paracetamol (APAP) for varying time intervals. 2 The maximal hepatic damage occurred between 12-18 h after single doses of APAP (0.5 or 1 g/kg); hepatic excretory function returned to control levels by 48-72 hours. 3 Administration of either 0.5 or 1 g/kg APAP 18 h before BSP caused a dose-dependent inhibition of the choleretic effect of BSP and of the 60 min cumulative excretion of the dye, but conversely, produced a significant increase in the liver and plasma concentrations of 35S. 4 Following acute (0.25 g/kg), or subacute (0.5 g/kg, twice daily for 7 days) treatment with APAP, the total excretion of 35S in bile and the retention of 35S in the liver or plasma remained essentially the same as that for the controls. 5 In rats given single doses of 1 g/kg APAP, the hepatic uptake of the dye was significantly increased during the early stages of intoxication, while the opposite effect was observed at late periods. 6 The bile flow appeared to be inversely related to the excretion of unchanged BSP, and directly related to the excretion of the major BSP conjugate in bile. 7 The hepatic clearance of BSP was more rapid in rats treated subacutely with 0.5 or 1 g/kg APAP, than in those treated acutely with equal doses, suggesting that the intensity of APAP-induced hepatotoxicity became less severe after the repeated administration of this drug. 8 It is concluded that the hepatic uptake, metabolism and excretion of BSP are reversibly impaired following APAP-induced liver injury.},
archivePrefix = {arXiv},
arxivId = {10.1111/j.2007.0906-7590.05203.x},
author = {Phillips, Steven J. and Dud{\'{i}}k, Miroslav},
doi = {10.1111/j.2007.0906-7590.05203.x},
eprint = {j.2007.0906-7590.05203.x},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Phillips, Dud{\'{i}}k - 2008 - Modeling of species distribution with Maxent new extensions and a comprehensive evalutation.pdf:pdf},
isbn = {0906-7590},
issn = {0007-1188},
journal = {Ecograpy},
keywords = {05203,0906-7590,10,1111,161 {\'{a}} 175,2007,2007 at,2007 ecography,2008,accepted 13 december 2007,doi,graphy 31,inc,j,jo,journal compilation,t,x},
number = {December 2007},
pages = {161--175},
pmid = {3245},
primaryClass = {10.1111},
title = {{Modeling of species distribution with Maxent: new extensions and a comprehensive evalutation}},
volume = {31},
year = {2008}
}
@inproceedings{Hasselt2010,
abstract = {In some stochastic environments the well-known reinforcement learning algo-rithm Q-learning performs very poorly. This poor performance is caused by large overestimations of action values. These overestimations result from a positive bias that is introduced because Q-learning uses the maximum action value as an approximation for the maximum expected action value. We introduce an alter-native way to approximate the maximum expected value for any set of random variables. The obtained double estimator method is shown to sometimes under-estimate rather than overestimate the maximum expected value. We apply the double estimator to Q-learning to construct Double Q-learning, a new off-policy reinforcement learning algorithm. We show the new algorithm converges to the optimal policy and that it performs well in some settings in which Q-learning per-forms poorly due to its overestimation.},
archivePrefix = {arXiv},
arxivId = {1606.04615},
author = {Hasselt, Hado Van and Group, Adaptive Computation and Wiskunde, Centrum and {Van Hasselt}, Hado},
booktitle = {Neural Inf. Proceeding Syst.},
doi = {10.1016/j.tws.2009.08.006},
eprint = {1606.04615},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Van Hasselt - Unknown - Double Q-learning.pdf:pdf},
isbn = {9781617823800},
issn = {02638231},
pmid = {26150344},
title = {{Double Q-learning}},
url = {https://papers.nips.cc/paper/3964-double-q-learning.pdf},
year = {2010}
}
@inproceedings{narvekar2016a,
author = {Narvekar, S and Sinapov, J and Leonetti, M and Stone, P},
booktitle = {Proc. 2016 Int. Conf. Auton. Agents Multiagent Syst. Int. Found. Auton. Agents Multiagent Syst.},
pages = {566--574},
title = {{Source task creation for curriculum learning}}
}
@incollection{bellman1957a,
address = {Bello},
author = {Bellman, R},
title = {{Dynamic Programming}}
}
@incollection{pathak2017a,
author = {Pathak, D and Agrawal, P and Efros, A A and Darrell, T},
booktitle = {Int. Conf. Mach. Learn.},
title = {{Curiositydriven exploration by self-supervised prediction}},
volume = {Vol.}
}
@misc{fran2015a,
author = {Fran{\c{c}}ois-Lavet, V and Fonteneau, R and Ernst, D},
edition = {arXiv prep},
title = {{How to Discount Deep Reinforcement Learning: Towards New Dynamic Strategies}}
}
@misc{mathieu2015a,
author = {Mathieu, M and Couprie, C and LeCun, Y},
edition = {arXiv prep},
title = {{Deep multi-scale video prediction beyond mean square error}}
}
@article{hafner2011a,
author = {Hafner, R and Riedmiller, M},
journal = {Mach. Learn.},
number = {1-2},
pages = {137--169},
title = {{Reinforcement learning in feedback control}},
volume = {84}
}
@misc{boularias2011a,
author = {Boularias, A and Kober, J and Peters, J},
pages = {182--189},
title = {{“Relative Entropy Inverse Reinforcement Learning.” In: AISTATS}}
}
@techreport{AlkaeeTaleghan2015a,
abstract = {In a simulator-defined MDP, the Markovian dynamics and rewards are provided in the form of a simulator from which samples can be drawn. This paper studies MDP planning algorithms that attempt to minimize the number of simulator calls before terminating and outputting a policy that is approximately optimal with high probability. The paper introduces two heuristics for efficient exploration and an improved confidence interval that enables earlier termination with probabilis-tic guarantees. We prove that the heuristics and the confidence interval are sound and produce with high probability an approximately optimal policy in polynomial time. Experiments on two benchmark problems and two instances of an invasive species management problem show that the improved confidence intervals and the new search heuristics yield reductions of between 8{\%} and 47{\%} in the number of simulator calls required to reach near-optimal policies.},
author = {{Alkaee Taleghan}, Majid and Dietterich, Thomas G and Crowley, Mark and Hall, Kim and {Jo Albers}, H and Auer, Peter and Hutter, Marcus and Orseau, Laurent},
booktitle = {J. Mach. Learn. Res.},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Alkaee Taleghan et al. - 2015 - PAC Optimal MDP Planning with Application to Invasive Species Management(2).pdf:pdf},
keywords = {Good-Turing estimate,MDP planning,Markov decision processes,invasive species management,reinforcement learning},
pages = {3877--3903},
title = {{PAC Optimal MDP Planning with Application to Invasive Species Management *}},
url = {http://jmlr.org/papers/volume16/taleghan15a/taleghan15a.pdf},
volume = {16},
year = {2015}
}
@misc{tobin2017a,
author = {Tobin, J and Fong, R and Ray, A and Schneider, J and Zaremba, W and Abbeel, P},
edition = {arXiv prep},
title = {{Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World}}
}
@article{cortes1995a,
author = {Cortes, C and Vapnik, V},
journal = {Mach. Learn.},
number = {3},
pages = {273--297},
title = {{Support-vector networks}},
volume = {20}
}
@article{amari1998a,
author = {Amari, S},
journal = {Neural Comput.},
number = {2},
pages = {251--276},
title = {{Natural Gradient Works Efficiently in Learning}},
volume = {10}
}
@article{Dudik2004a,
abstract = {We consider the problem of estimating an unknown probability distribution from samples using the principle of maximum entropy (maxent). To alleviate overfitting with a very large number of features, we propose applying the maxent principle with relaxed constraints on the expectations of the features. By convex duality, this turns out to be equivalent to finding the Gibbs distribution minimizing a regularized version of the empirical log loss. We prove non-asymptotic bounds showing that, with respect to the true underlying distribution, this relaxed version of maxent produces density estimates that are almost as good as the best possible. These bounds are in terms of the deviation of the feature empirical averages relative to their true expectations, a number that can be bounded using standard uniform-convergence techniques. In particular, this leads to bounds that drop quickly with the number of samples, and that depend very moderately on the number or complexity of the features. We also derive and prove convergence for both sequential-update and parallel-update algorithms. Finally, we briefly describe experiments on data relevant to the modeling of species geographical distributions.},
author = {Dud{\'{i}}k, Miroslav and Phillips, Steven J. and Schapire, Robert E.},
doi = {10.1007/978-3-540-27819-1_33},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dud{\'{i}}k, Phillips, Schapire - 2004 - Performance Guarantees for Regularized Maximum Entropy Density Estimation.pdf:pdf},
isbn = {0302-9743},
issn = {03029743},
pages = {472--486},
pmid = {6185},
title = {{Performance Guarantees for Regularized Maximum Entropy Density Estimation}},
url = {http://link.springer.com/10.1007/978-3-540-27819-1{\_}33},
year = {2004}
}
@incollection{duan2016a,
author = {Duan, Y and Chen, X and Houthooft, R and Schulman, J and Abbeel, P},
booktitle = {Int. Conf. Mach. Learn.},
pages = {1329--1338},
title = {{Benchmarking deep reinforcement learning for continuous control}}
}
@book{brassard,
author = {Brassard, G and Bratley, P},
publisher = {Englewood Cliffs, NJ: Prentice Hall},
title = {{Algorithmics - Theory and Practice}},
year = {1988}
}
@misc{liu2017a,
author = {Liu, Y and Gupta, A and Abbeel, P and Levine, S},
edition = {arXiv prep},
title = {{Imitation from Observation: Learning to Imitate Behaviors from Raw Video via Context Translation}}
}
@article{Huang2017,
abstract = {Machine learning classifiers are known to be vulnerable to inputs maliciously constructed by adversaries to force misclassification. Such adversarial examples have been extensively studied in the context of computer vision applications. In this work, we show adversarial attacks are also effective when targeting neural network policies in reinforcement learning. Specifically, we show existing adversarial example crafting techniques can be used to significantly degrade test-time performance of trained policies. Our threat model considers adversaries capable of introducing small perturbations to the raw input of the policy. We characterize the degree of vulnerability across tasks and training algorithms, for a subclass of adversarial-example attacks in white-box and black-box settings. Regardless of the learned task or training algorithm, we observe a significant drop in performance, even with small adversarial perturbations that do not interfere with human perception. Videos are available at http://rll.berkeley.edu/adversarial.},
archivePrefix = {arXiv},
arxivId = {1702.02284},
author = {Huang, Sandy and Papernot, Nicolas and Goodfellow, Ian and Duan, Yan and Abbeel, Pieter},
eprint = {1702.02284},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang et al. - 2017 - Adversarial Attacks on Neural Network Policies.pdf:pdf},
month = {feb},
title = {{Adversarial Attacks on Neural Network Policies}},
url = {http://arxiv.org/abs/1702.02284},
year = {2017}
}
@article{tsitsiklis1997a,
author = {Tsitsiklis, J N and Roy, B.Van},
journal = {Autom. Control. IEEE Trans.},
number = {5},
pages = {674--690},
title = {{An analysis of temporaldifference learning with function approximation}},
volume = {42}
}
@misc{gordon1996a,
author = {Gordon, G J},
title = {{“Stable fitted reinforcement learning”. In: Advances in neural information processing systems. 1052–1058}}
}
@misc{parisotto2015a,
author = {Parisotto, E and Ba, J L and Salakhutdinov, R},
edition = {arXiv prep},
title = {{Actor-mimic: Deep multitask and transfer reinforcement learning}}
}
@article{whiteson2011a,
author = {Whiteson, S and Tanner, B and Taylor, M E and Stone, P},
journal = {IEEE},
pages = {120--127},
title = {{“Protecting against evaluation overfitting in empirical reinforcement learning”. In: Adaptive Dynamic Programming And Reinforcement Learning (ADPRL), 2011 IEEE Symposium on}}
}
@misc{kaplan2017a,
author = {Kaplan, R and Sauer, C and Sosa, A},
edition = {arXiv prep},
title = {{Beating Atari with Natural Language Guided Reinforcement Learning}}
}
@article{Phillips2009,
archivePrefix = {arXiv},
arxivId = {1132},
author = {Phillips, S. J.},
doi = {10.1890/07-2153.1},
eprint = {1132},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Phillips - 2009 - Sample selection bias and presence-only distribution models implications for background and pseudo-absence data Refer.pdf:pdf},
isbn = {1051-0761},
issn = {1051-0761},
journal = {Ecol. Appl.},
keywords = {background data,niche modeling,presence-only distribution models,pseudo-absence,sample selection bias,species distribution modeling,target group},
number = {1},
pages = {181--197},
pmid = {19323182},
title = {{Sample selection bias and presence-only distribution models : implications for background and pseudo-absence data Reference Sample selection bias and presence-only distribution models : implications for background and pseudo-absence data}},
volume = {19},
year = {2009}
}
@inproceedings{schaul-a,
booktitle = {Proc. 32nd Int. Conf. Mach. Learn.},
pages = {1312--1320},
title = {{Universal value function approximators}},
volume = {15}
}
@article{tesauro1995a,
author = {Tesauro, G},
journal = {Commun. ACM},
number = {3},
pages = {58--68},
title = {{Temporal difference learning and TD-Gammon}},
volume = {38}
}
@misc{foerster2017b,
author = {Foerster, J and Nardelli, N and Farquhar, G and Torr, P and Kohli, P and Whiteson, S},
edition = {arXiv prep},
title = {{Stabilising experience replay for deep multi-agent reinforcement learning}}
}
@misc{konda2000a,
author = {Konda, V R and Tsitsiklis, J N},
title = {{“Actor-critic algorithms”. In: Advances in neural information processing systems. 1008–1014}}
}
@misc{kirkpatrick2016a,
author = {Kirkpatrick, J and Pascanu, R and Rabinowitz, N and Veness, J and Desjardins, G and Rusu, A A and Milan, K and Quan, J and Ramalho, T and Grabska-Barwinska, A},
edition = {arXiv prep},
title = {{Overcoming catastrophic forgetting in neural networks}}
}
@misc{guo2017a,
author = {Guo, Z D and Brunskill, E},
edition = {arXiv prep},
title = {{Sample efficient feature selection for factored mdps}}
}
@misc{graves2014a,
author = {Graves, A and Wayne, G and Danihelka, I},
edition = {arXiv prep},
title = {{Neural turing machines}}
}
@misc{goodfellow2014a,
author = {Goodfellow, I and Pouget-Abadie, J and Mirza, M and Xu, B and Warde-Farley, D and Ozair, S and Courville, A and Bengio, Y},
title = {{“Generative adversarial nets”. In: Advances in neural information processing systems. 2672– 2680}}
}
@inproceedings{duchesne2017a,
author = {Duchesne, L and Karangelos, E and Wehenkel, L},
title = {{“Machine learning of real-time power systems reliability management response”. PowerTech Manchester 2017 Proceedings}}
}
@misc{brockman2016a,
author = {Brockman, G and Cheung, V and Pettersson, L and Schneider, J and Schulman, J and Tang, J and Zaremba, W},
title = {{OpenAI Gym}}
}
@inproceedings{johnston,
author = {Johnston, M D and Adorf, H M},
booktitle = {Proc. NASA Conf. Sp. Telerobotics},
title = {{Learning in Stochastic Neural Networks for Constraint Satisfaction Problems}},
volume = {37},
year = {1989}
}
@thesis{mccallum1996a,
author = {McCallum, A K},
institution = {thesis. University of Rochester},
title = {{Reinforcement learning with selective perception and hidden state}},
type = {PhD}
}
@misc{gu2016a,
author = {Gu, S and Lillicrap, T and Sutskever, I and Levine, S},
edition = {arXiv prep},
title = {{Continuous Deep Q-Learning with Model-based Acceleration}}
}
@article{lecun1998a,
author = {LeCun, Y and Bottou, L and Bengio, Y and Haffner, P},
journal = {Proc. IEEE},
number = {11},
pages = {2278--2324},
title = {{Gradient-based learning applied to document recognition}},
volume = {86}
}
@misc{tzeng2015a,
author = {Tzeng, E and Devin, C and Hoffman, J and Finn, C and Abbeel, P and Levine, S and Saenko, K and Darrell, T},
edition = {arXiv prep},
title = {{Adapting deep visuomotor representations with weak pairwise constraints}}
}
@misc{rusu2016a,
edition = {arXiv prep},
title = {{Sim-to-real robot learning from pixels with progressive nets}}
}
@article{hochreiter1997a,
author = {Hochreiter, S and Schmidhuber, J},
journal = {Neural},
number = {8},
pages = {1735--1780},
title = {{Long short-term memory}},
volume = {9}
}
@book{schulman2016a,
address = {In},
author = {Schulman, J and Ho, J and Lee, C and Abbeel, P},
pages = {339--354},
publisher = {Robotics Research. Springer},
title = {{Learning from demonstrations through the use of non-rigid registration}}
}
@inproceedings{finn-a,
author = {{Finn C.}, S Levine and 2016b, P Abbeel.},
booktitle = {Proc. 33rd Int. Conf. Mach. Learn.},
title = {{Guided cost learning: Deep inverse optimal control via policy optimization}},
volume = {48}
}
@incollection{nagabandi2018a,
author = {Nagabandi, A and Kahn, G and Fearing, R S and Levine, S},
booktitle = {2018 IEEE Int. Conf. Robot. Autom. (ICRA). IEEE},
pages = {7559--7566},
title = {{Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning}}
}
@misc{tan2018a,
author = {Tan, J and Zhang, T and Coumans, E and Iscen, A and Bai, Y and Hafner, D and Bohez, S and Vanhoucke, V},
edition = {arXiv prep},
title = {{Sim-to-Real: Learning Agile Locomotion For Quadruped Robots}}
}
@article{Phillips2006,
abstract = {The availability of detailed environmental data, together with inexpensive and powerful computers, has fueled a rapid increase in predictive modeling of species environmental requirements and geographic distributions. For some species, detailed presence/ absence occurrence data are available, allowing the use of a variety of standard statistical techniques. However, absence data are not available for most species. In this paper, we introduce the use of the maximum entropy method (Maxent) for modeling species geographic distributions with presence-only data. Maxent is a general-purpose machine learning method with a simple and precise mathematical formulation, and it has a number of aspects that make it well-suited for species distribution modeling. In order to investigate the efficacy of themethod, here we performa continental-scale case study using two Neotropicalmammals: a lowland species of sloth, Bradypus variegatus, and a small montane murid rodent, Microryzomys minutus.We compared Maxent predictions with those of a commonly used presence-only modeling method, the Genetic Algorithm for Rule-Set Prediction (GARP). We made predictions on 10 random subsets of the occurrence records for both species, and then used the remaining localities for testing. Both algorithms provided reasonable estimates of the species' range, far superior to the shaded outline maps available in field guides. All models were significantly better than random in both binomial tests of omission and receiver operating characteristic (ROC) analyses. The area under the ROC curve (AUC) was almost always higher for Maxent, indicating better discrimination of suitable versus unsuitable areas for the species. The Maxent modeling approach can be used in its present form for many applications with presence-only datasets, and merits further research and development.},
archivePrefix = {arXiv},
arxivId = {11265},
author = {Phillips, Steven J and Anderson, Robert P and Schapire, Robert E},
doi = {10.1016/j.ecolmodel.2005.03.026},
eprint = {11265},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Phillips, Anderson, Schapire - 2006 - Maximum entropy modeling of species geographic distributions.pdf:pdf},
isbn = {033},
issn = {14666650},
journal = {Ecol. Model. 190 231–259},
keywords = {Ammonia,CMAQ,Dry deposition,Flux,Modelling,Nitrogen,Wet deposition},
pmid = {1474},
title = {{Maximum entropy modeling of species geographic distributions}},
year = {2006}
}
@misc{chen2015a,
author = {Chen, T and Goodfellow, I and Shlens, J},
edition = {arXiv prep},
title = {{Net2net: Accelerating learning via knowledge transfer}}
}
@misc{dabney2017a,
author = {Dabney, W and Rowland, M and Bellemare, M G and Munos, R},
edition = {arXiv prep},
title = {{Distributional Reinforcement Learning with Quantile Regression}}
}
@misc{johnson2016a,
author = {Johnson, M and Hofmann, K and Hutton, T and Bignell, D},
title = {{“The Malmo Platform for Artificial Intelligence Experimentation.” In: IJCAI. 4246–4247}}
}
@article{nguyen1990a,
author = {Nguyen, D H and Widrow, B},
journal = {IEEE Control Syst. Mag.},
number = {3},
pages = {18--23},
title = {{Neural networks for self-learning control systems}},
volume = {10}
}
@article{hassabis2017a,
author = {Hassabis, D and Kumaran, D and Summerfield, C and Botvinick, M},
journal = {Neuron},
number = {2},
pages = {245--258},
title = {{Neuroscience-inspired artificial intelligence}},
volume = {95}
}
@misc{chiappa2017a,
author = {Chiappa, S and Racaniere, S and Wierstra, D and Mohamed, S},
edition = {arXiv prep},
title = {{Recurrent Environment Simulators}}
}
@article{watkins1992a,
author = {Watkins, C J and Dayan, P},
journal = {Mach. Learn.},
number = {3-4},
pages = {279--292},
title = {{Q-learning}},
volume = {8}
}
@article{halsey2015a,
author = {Halsey, L G and Curran-Everett, D and Vowler, S L and Drummond, G B},
journal = {Nat. Methods},
number = {3},
pages = {179--185},
title = {{The fickle P value generates irreproducible results}},
volume = {12}
}
@inproceedings{Lavrenko2001,
address = {New York, New York, USA},
author = {Lavrenko, Victor and Croft, W. Bruce},
booktitle = {Proc. 24th Annu. Int. ACM SIGIR Conf. Res. Dev. Inf. Retr.  - SIGIR '01},
doi = {10.1145/383952.383972},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lavrenko, Croft - 2001 - Relevance based language models.pdf:pdf},
isbn = {1581133316},
pages = {120--127},
publisher = {ACM Press},
title = {{Relevance based language models}},
url = {http://portal.acm.org/citation.cfm?doid=383952.383972},
year = {2001}
}
@misc{garnelo2016a,
author = {Garnelo, M and Arulkumaran, K and Shanahan, M},
edition = {arXiv prep},
title = {{Towards Deep Symbolic Reinforcement Learning}}
}
@misc{bengio2017a,
author = {Bengio, Y},
edition = {arXiv prep},
title = {{The Consciousness Prior}}
}
@misc{johnson2017a,
author = {Johnson, J and Hariharan, B and van der Maaten, L and Hoffman, J and FeiFei, L and Zitnick, C L and Girshick, R},
edition = {arXiv prep},
title = {{Inferring and Executing Programs for Visual Reasoning}}
}
@article{russakovsky2015a,
author = {Russakovsky, O and Deng, J and Su, H and Krause, J and Satheesh, S and Ma, S and Huang, Z and Karpathy, A and Khosla, A and Bernstein, M},
journal = {Int. J. Comput. Vis.},
number = {3},
pages = {211--252},
title = {{Imagenet large scale visual recognition challenge}},
volume = {115}
}
@article{bellman-a,
author = {Bellman, R 1957a},
journal = {J. Math. Mech.},
pages = {679--684},
title = {{A Markovian decision process}}
}
@article{li2016a,
author = {Li, L and Lv, Y and Wang, F.-Y.},
journal = {IEEE/CAA J. Autom. Sin.},
number = {3},
pages = {247--254},
title = {{Traffic signal timing via deep reinforcement learning}},
volume = {3}
}
@misc{weber2017a,
author = {Weber, T and Racani{\`{e}}re, S and Reichert, D P and Buesing, L and Guez, A and Rezende, D J and Badia, A P and Vinyals, O and Heess, N and Li, Y},
edition = {arXiv prep},
title = {{Imagination-Augmented Agents for Deep Reinforcement Learning}}
}
@article{Pattanaik2018,
author = {Pattanaik, Anay and Tang, Zhenyi and Liu, Shuijing and Bommannan, Gautham and Chowdhary, Girish},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pattanaik et al. - 2018 - Robust Deep Reinforcement Learning with Adversarial Attacks.pdf:pdf},
journal = {undefined},
title = {{Robust Deep Reinforcement Learning with Adversarial Attacks}},
url = {https://www.semanticscholar.org/paper/Robust-Deep-Reinforcement-Learning-with-Adversarial-Pattanaik-Tang/3b6c891fbccaa564ea4fd8914a5e3952fcf42ee3},
year = {2018}
}
@article{levine2016a,
author = {Levine, S and Finn, C and Darrell, T and Abbeel, P},
journal = {J. Mach. Learn. Res.},
number = {39},
pages = {1--40},
title = {{End-to-end training of deep visuomotor policies}},
volume = {17}
}
@techreport{Watkins1992,
abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states. This paper presents and proves in detail a convergence theorem for Q,-learning based on that outlined in Watkins (1989). We show that Q-learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where many Q values can be changed each iteration, rather than just one.},
author = {Watkins, Christopher J C H and Dayan, Peter},
title = {{No Title}}
}
@article{niv2009a,
author = {Niv, Y},
journal = {J. Math. Psychol.},
number = {3},
pages = {139--154},
title = {{Reinforcement learning in the brain}},
volume = {53}
}
@techreport{Sutton1998,
abstract = {This introductory textbook on reinforcement learning is targeted toward engineers and scientists in artificial intelligence, operations research, neural networks, and control systems, and we hope it will also be of interest to psychologists and neuroscientists. If you would like to order a copy of the book, or if you are qualified instructor and would like to see an examination copy, please see the MIT Press home page for this book. Or you might be interested in the reviews at amazon.com. There is also a Japanese translation available. The table of contents of the book is given below, with associated HTML. The HTML version has a number of presentation problems, and its text is slightly different from the real book, but it may be useful for some purposes. q Preface Part I: The Problem q 1 Introduction r 1.1 Reinforcement Learning r 1.2 Examples r 1.3 Elements of Reinforcement Learning r 1.4 An Extended Example: Tic-Tac-Toe r 1.5 Summary r 1.6 History of Reinforcement Learning r 1.7 Bibliographical Remarks q 2 Evaluative Feedback r 2.1 An n-armed Bandit Problem r 2.2 Action-Value Methods r 2.3 Softmax Action Selection r 2.4 Evaluation versus Instruction r 2.5 Incremental Implementation r 2.6 Tracking a Nonstationary Problem r 2.7 Optimistic Initial Values r 2.8 Reinforcement Comparison r 2.9 Pursuit Methods r 2.10 Associative Search r 2.11 Conclusion r 2.12 Bibliographical and Historical Remarks q 3 The Reinforcement Learning Problem r 3.1 The Agent-Environment Interface r 3.2 Goals and Rewards r 3.3 Returns r 3.4 A Unified Notation for Episodic and Continual Tasks r 3.5 The Markov Property r 3.6 Markov Decision Processes r 3.7 Value Functions r 3.8 Optimal Value Functions r 3.9 Optimality and Approximation r 3.10 Summary r 3.11 Bibliographical and Historical Remarks Part II: Elementary Methods},
archivePrefix = {arXiv},
arxivId = {1603.02199},
author = {Sutton, Richard S. and Barto, Andrew G.},
booktitle = {MIT Press},
doi = {10.1109/TNN.1998.712192},
eprint = {1603.02199},
isbn = {0262193981},
issn = {10459227},
keywords = {reinforcement learning theory},
pmid = {18255791},
title = {{Sutton {\&} Barto Book: Reinforcement Learning: An Introduction}},
year = {1998}
}
@misc{synnaeve2016a,
author = {Synnaeve, G and Nardelli, N and Auvolat, A and Chintala, S and Lacroix, T and Lin, Z and Richoux, F and Usunier, N},
edition = {arXiv prep},
title = {{TorchCraft: a Library for Machine Learning Research on Real-Time Strategy Games}}
}
@article{cohen2007a,
author = {Cohen, J D and McClure, S M and Angela, J Y},
journal = {Philos. Trans. R. Soc. London B Biol. Sci.},
number = {1481},
pages = {933--942},
title = {{Should I stay or should I go? How the human brain manages the trade-off between exploitation and exploration}},
volume = {362}
}
@inproceedings{hopfield,
author = {Hopfield, J J},
booktitle = {Proc. Natl. Acad. Sci.},
publisher = {Washington, DC: National Academy Press},
title = {{Neural Networks and Physical Systems with Emergent Collective Computational Abilities}},
volume = {79},
year = {1982}
}
@article{Phillips2004,
abstract = {We study the problem of modeling species geographic distributions, a critical problem in conservation biology. We propose the use of maximum-entropy techniques for this problem, specifically, sequential-update algorithms that can handle a very large number of features. We describe experiments comparing maxent with a standard distribution-modeling tool, called GARP, on a dataset containing observation data for North American breeding birds. We also study how well maxent performs as a function of the number of training examples and training time, analyze the use of regularization to avoid overfitting when the number of examples is small, and explore the interpretability of models constructed using maxent.},
author = {Phillips, Steven J and Dud{\'{i}}k, M and Schapire, R E},
doi = {10.1145/1015330.1015412},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Phillips, Dud{\'{i}}k, Schapire - 2004 - A maximum entropy approach to species distribution modeling.pdf:pdf},
isbn = {1581138285},
issn = {00147672},
journal = {Twentyfirst Int. Conf. Mach. Learn. ICML 04},
pages = {83},
pmid = {6379},
title = {{A maximum entropy approach to species distribution modeling}},
url = {http://portal.acm.org/citation.cfm?doid=1015330.1015412},
volume = {69},
year = {2004}
}
@article{d2001a,
author = {D{\v{z}}eroski, S and Raedt, L.De and Driessens, K},
journal = {Mach. Learn.},
number = {1-2},
pages = {7--52},
title = {{Relational reinforcement learning}},
volume = {43}
}
@incollection{ernst2005a,
author = {Ernst, D and Geurts, P and Wehenkel, L},
booktitle = {J. Mach. Learn. Res.},
pages = {503--556},
title = {{Tree-based batch mode reinforcement learning}}
}
@misc{mohamed2015a,
author = {Mohamed, S and Rezende, D J},
title = {{“Variational information maximisation for intrinsically motivated reinforcement learning”. In: Advances in neural information processing systems. 2125–2133}}
}
@misc{henderson2017a,
author = {Henderson, P and Chang, W.-D. and Shkurti, F and Hansen, J and Meger, D and Dudek, G},
title = {{“Benchmark Environments for Multitask Learning in Continuous Domains”. ICML Lifelong Learning: A Reinforcement Learning Approach Workshop}}
}
@article{peng2017b,
author = {Peng, X B and Berseth, G and Yin, K and van de Panne, M},
journal = {ACM Trans. Graph.},
number = {4},
publisher = {Proc. SIGGRAPH},
title = {{DeepLoco: Dynamic Locomotion Skills Using Hierarchical Deep Reinforcement Learning}},
volume = {36}
}
@misc{gauci2018a,
author = {Gauci, J and Conti, E and Liang, Y and Virochsiri, K and He, Y and Kaden, Z and Narayanan, V and Ye, X},
edition = {arXiv prep},
title = {{Horizon: Facebook's Open Source Applied Reinforcement Learning Platform}}
}
@article{Lillicrap2015,
abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
archivePrefix = {arXiv},
arxivId = {1509.02971},
author = {Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
eprint = {1509.02971},
journal = {ICLR},
title = {{Continuous control with deep reinforcement learning: Deep Deterministic Policy Gradients (DDPG)}},
year = {2015}
}
@misc{fran2018a,
author = {Fran{\c{c}}ois-Lavet, V and Bengio, Y and Precup, D and Pineau, J},
edition = {arXiv prep},
title = {{Combined Reinforcement Learning via Abstract Representations}}
}
@article{Radford,
author = {Radford, A and Narasimhan, K and https://s3-us-west-2 {\ldots}, T Salimans - URL and undefined 2018},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Radford et al. - Unknown - Improving language understanding by generative pre-training.pdf:pdf},
journal = {cs.ubc.ca},
title = {{Improving language understanding by generative pre-training}},
url = {https://www.cs.ubc.ca/{~}amuham01/LING530/papers/radford2018improving.pdf}
}
@incollection{fran2016b,
author = {Fran{\c{c}}ois-Lavet, V and Taralla, D and Ernst, D and Fonteneau, R},
booktitle = {Eur. Work. Reinf. Learn.},
title = {{Deep Reinforcement Learning Solutions for Energy Microgrids Management}}
}
@techreport{Watkins1992,
abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states. This paper presents and proves in detail a convergence theorem for Q,-learning based on that outlined in Watkins (1989). We show that Q-learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where many Q values can be changed each iteration, rather than just one.},
author = {Watkins, Christopher J C H and Dayan, Peter},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Watkins, Dayan - 1992 - Technical Note Q,-Learning.pdf:pdf},
keywords = {Q-learning,asynchronous dynamic programming,reinforcement learning,temporal differences},
pages = {279--292},
title = {{Technical Note Q,-Learning}},
url = {http://www.gatsby.ucl.ac.uk/{~}dayan/papers/cjch.pdf},
volume = {8},
year = {1992}
}
@incollection{boyan1995a,
author = {Boyan, J A and Moore, A W},
booktitle = {Adv. Neural Inf. Process. Syst.},
pages = {369--376},
title = {{Generalization in reinforcement learning: Safely approximating the value function}}
}
@book{norris1998a,
author = {Norris, J R},
publisher = {Cambridge university press},
title = {{Markov chains. No. 2}}
}
@misc{oh2016a,
author = {Oh, J and Chockalingam, V and Singh, S and Lee, H},
edition = {arXiv prep},
title = {{Control of Memory, Active Perception, and Action in Minecraft}}
}
@misc{plappert2017a,
author = {Plappert, M and Houthooft, R and Dhariwal, P and Sidor, S and Chen, R Y and Chen, X and Asfour, T and Abbeel, P and Andrychowicz, M},
edition = {arXiv prep},
title = {{Parameter Space Noise for Exploration}}
}
