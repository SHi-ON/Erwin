Automatically generated by Mendeley Desktop 1.19.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Francois-Lavet2018,
annote = {version 2 published on arXiv},
archivePrefix = {arXiv},
arxivId = {1811.12560v2},
author = {Fran{\c{c}}ois-Lavet, Vincent and Henderson, Peter and Islam, Riashat and Bellemare, Marc G and Pineau, Joelle},
doi = {10.1561/2200000071},
eprint = {1811.12560v2},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fran{\c{c}}ois-Lavet et al. - 2018 - An Introduction to Deep Reinforcement Learning.pdf:pdf},
title = {{An Introduction to Deep Reinforcement Learning}},
url = {https://arxiv.org/pdf/1811.12560.pdf},
year = {2018}
}
@article{Iyengar2005,
abstract = {In this paper we propose a robust formulation for discrete time dynamic programming (DP). The objective of the robust formulation is to systematically mitigate the sensitivity of the DP optimal policy to ambiguity in the underlying transition probabilities. The ambiguity is modeled by associating a set of conditional measures with each state-action pair. Consequently, in the robust formulation each policy has a set of measures associated with it. We prove that when this set of measures has a certain "rectangularity" property, all of the main results for finite and infinite horizon DP extend to natural robust counterparts. We discuss techniques from Nilim and El Ghaoui [17] for constructing suitable sets of conditional measures that allow one to efficiently solve for the optimal robust policy. We also show that robust DP is equivalent to stochastic zero-sum games with perfect information. 1. Introduction. This paper is concerned with sequential decision making in uncertain environments. Decisions are made in stages and each decision, in addition to providing an immediate reward, changes the context of future decisions; thereby affecting the future rewards. Due to the uncertain nature of the environment, there is limited information about both the immediate reward from each decision and the resulting future state. In order to achieve a good performance over all the stages, the decision maker has to trade-off the immediate payoff with future payoffs. Dynamic programming (DP) is the mathematical framework that allows the decision maker to efficiently compute a good overall strategy by succinctly encoding the evolving information state. In the DP formalism the uncertainty in the environment is modeled by a Markov process whose transition probability depends both on the information state and the action taken by the decision maker. It is assumed that the transition probability corresponding to each state-action pair is known to the decision maker, and the goal is to choose a policy, i.e., a rule that maps states to actions, that maximizes some performance measure. Puterman [20] provides a excellent introduction to the DP formalism and its various applications. In this paper, we assume that the reader has some prior knowledge of DP. The DP formalism encodes information in the form of a "reward-to-go" function (see Puterman [20] for details) and chooses an action that maximizes the sum of the immediate reward and the expected "reward-to-go." Thus, to compute the optimal action in any given state the "reward-to-go" function for all the future states must be known. In many applications of DP, the number of states and actions available in each state are large; consequently , the computational effort required to compute the optimal policy for a DP can be overwhelming-Bellman's "curse of dimensionality." For this reason, considerable recent research effort has focused on developing algorithms that compute an approximately optimal policy efficiently (Bertsekas and Tsitsiklis [5], de Farias and Van Roy [8]). Fortunately, for many applications the DP optimal policy can be computed with a modest computational effort. In this paper we restrict attention to this class of DPs. Typically, the transition probability of the underlying Markov process is estimated from historical data and is, therefore, subject to statistical errors. In current practice, these errors are ignored and the optimal policy is computed assuming that the estimate is, indeed, the true transition probability. The DP optimal policy is quite sensitive to perturbations in the transition probability},
author = {Iyengar, Garud N},
doi = {10.1287/moor.1040.0129},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Iyengar - 2005 - Robust Dynamic Programming.pdf:pdf},
issn = {1526-5471},
journal = {Robust Dyn. Program. Math. Oper. Res.},
keywords = {Markov decision processes,ambiguity MSC2000 subject classification: Primary:,dynamic programming,robust optimization,secondary: 90C40, 90C25 OR/MS subject classificati,secondary: probability-Markov processes},
number = {2},
pages = {257--280},
title = {{Robust Dynamic Programming}},
url = {http://pubsonline.informs.org.https//doi.org/10.1287/moor.1040.0129http://www.informs.orghttp://www.columbia.edu/∼gi10},
volume = {30},
year = {2005}
}
@inproceedings{ginsberg,
author = {Ginsberg, M L and Harvey, W D},
booktitle = {Proc. AAAI-91},
title = {{Iterative Broadening}},
year = {1990}
}
@article{And2018,
author = {Lydakis, Andreas},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lydakis - 2018 - Optimization of Control Measures Against Invasive Species BY Andreas Lydakis PROJECT Submitted to the University of New.pdf:pdf},
title = {{Optimization of Control Measures Against Invasive Species BY Andreas Lydakis PROJECT Submitted to the University of New Hampshire In Partial Fulfilment of Master of Science in Computer Science February , 2018 Dr Marek Petik ( supervisor )}},
year = {2018}
}
@techreport{Sutton1998,
abstract = {This introductory textbook on reinforcement learning is targeted toward engineers and scientists in artificial intelligence, operations research, neural networks, and control systems, and we hope it will also be of interest to psychologists and neuroscientists. If you would like to order a copy of the book, or if you are qualified instructor and would like to see an examination copy, please see the MIT Press home page for this book. Or you might be interested in the reviews at amazon.com. There is also a Japanese translation available. The table of contents of the book is given below, with associated HTML. The HTML version has a number of presentation problems, and its text is slightly different from the real book, but it may be useful for some purposes. q Preface Part I: The Problem q 1 Introduction r 1.1 Reinforcement Learning r 1.2 Examples r 1.3 Elements of Reinforcement Learning r 1.4 An Extended Example: Tic-Tac-Toe r 1.5 Summary r 1.6 History of Reinforcement Learning r 1.7 Bibliographical Remarks q 2 Evaluative Feedback r 2.1 An n-armed Bandit Problem r 2.2 Action-Value Methods r 2.3 Softmax Action Selection r 2.4 Evaluation versus Instruction r 2.5 Incremental Implementation r 2.6 Tracking a Nonstationary Problem r 2.7 Optimistic Initial Values r 2.8 Reinforcement Comparison r 2.9 Pursuit Methods r 2.10 Associative Search r 2.11 Conclusion r 2.12 Bibliographical and Historical Remarks q 3 The Reinforcement Learning Problem r 3.1 The Agent-Environment Interface r 3.2 Goals and Rewards r 3.3 Returns r 3.4 A Unified Notation for Episodic and Continual Tasks r 3.5 The Markov Property r 3.6 Markov Decision Processes r 3.7 Value Functions r 3.8 Optimal Value Functions r 3.9 Optimality and Approximation r 3.10 Summary r 3.11 Bibliographical and Historical Remarks Part II: Elementary Methods},
archivePrefix = {arXiv},
arxivId = {1603.02199},
author = {Sutton, Richard S. and Barto, Andrew G.},
booktitle = {MIT Press},
doi = {10.1109/TNN.1998.712192},
eprint = {1603.02199},
isbn = {0262193981},
issn = {10459227},
keywords = {reinforcement learning theory},
pmid = {18255791},
title = {{Sutton {\&} Barto Book: Reinforcement Learning: An Introduction}},
year = {1998}
}
@book{hacker,
author = {Sussman, G J},
publisher = {New York: New American Elsevier},
title = {{A Computer Model of Skill Acquisition}},
year = {1975}
}
@article{Strehl2008,
abstract = {Several algorithms for learning near-optimal policies in Markov Decision Processes have been analyzed and proven efficient. Empirical results have suggested that Model-based Interval Estimation (MBIE) learns efficiently in practice, effectively balancing exploration and exploitation. This paper presents a theoretical analysis of MBIE and a new variation called MBIE-EB, proving their efficiency even under worst-case conditions. The paper also introduces a new performance metric, average loss, and relates it to its less "online" cousins from the literature.},
annote = {Model-based Interval Estimation for MDPs},
author = {Strehl, Alexander L and Littman, Michael L},
doi = {10.1016/j.jcss.2007.08.009},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Strehl, Littman - 2008 - An analysis of model-based Interval Estimation for Markov Decision Processes.pdf:pdf},
journal = {J. Comput. Syst. Sci.},
keywords = {Learning theory,Markov Decision Processes,Reinforcement learning},
pages = {1309--1331},
title = {{An analysis of model-based Interval Estimation for Markov Decision Processes}},
url = {www.elsevier.com/locate/jcss},
volume = {74},
year = {2008}
}
@article{Renner2013,
abstract = {Summary Modeling the spatial distribution of a species is a fundamental problem in ecology. A number of modeling methods have been developed, an extremely popular one being MAXENT, a maximum entropy modeling approach. In this article, we show that MAXENT is equivalent to a Poisson regression model and hence is related to a Poisson point process model, differing only in the intercept term, which is scale-dependent in MAXENT. We illustrate a number of improvements to MAXENT that follow from these relations. In particular, a point process model approach facilitates methods for choosing the appropriate spatial resolution, assessing model adequacy, and choosing the LASSO penalty parameter, all currently unavailable to MAXENT. The equivalence result represents a significant step in the unification of the species distribution modeling literature.},
author = {Renner, Ian W. and Warton, David I.},
doi = {10.1111/j.1541-0420.2012.01824.x},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Renner, Warton - 2013 - Equivalence of MAXENT and Poisson Point Process Models for Species Distribution Modeling in Ecology.pdf:pdf},
isbn = {1541-0420},
issn = {0006341X},
journal = {Biometrics},
keywords = {Habitat modeling,Location-only,Maximum entropy,Poisson likelihood,Presence-only data,Use-availability},
number = {1},
pages = {274--281},
pmid = {23379623},
title = {{Equivalence of MAXENT and Poisson Point Process Models for Species Distribution Modeling in Ecology}},
volume = {69},
year = {2013}
}
@article{Phillips2008,
abstract = {1 The overall functional capacity of the liver was evaluated using [35S]-bromosulphophthalein (BSP, 100 mg/kg, i.v.) in biliary fistulated adult rats pretreated orally with different doses of paracetamol (APAP) for varying time intervals. 2 The maximal hepatic damage occurred between 12-18 h after single doses of APAP (0.5 or 1 g/kg); hepatic excretory function returned to control levels by 48-72 hours. 3 Administration of either 0.5 or 1 g/kg APAP 18 h before BSP caused a dose-dependent inhibition of the choleretic effect of BSP and of the 60 min cumulative excretion of the dye, but conversely, produced a significant increase in the liver and plasma concentrations of 35S. 4 Following acute (0.25 g/kg), or subacute (0.5 g/kg, twice daily for 7 days) treatment with APAP, the total excretion of 35S in bile and the retention of 35S in the liver or plasma remained essentially the same as that for the controls. 5 In rats given single doses of 1 g/kg APAP, the hepatic uptake of the dye was significantly increased during the early stages of intoxication, while the opposite effect was observed at late periods. 6 The bile flow appeared to be inversely related to the excretion of unchanged BSP, and directly related to the excretion of the major BSP conjugate in bile. 7 The hepatic clearance of BSP was more rapid in rats treated subacutely with 0.5 or 1 g/kg APAP, than in those treated acutely with equal doses, suggesting that the intensity of APAP-induced hepatotoxicity became less severe after the repeated administration of this drug. 8 It is concluded that the hepatic uptake, metabolism and excretion of BSP are reversibly impaired following APAP-induced liver injury.},
archivePrefix = {arXiv},
arxivId = {10.1111/j.2007.0906-7590.05203.x},
author = {Phillips, Steven J. and Dud{\'{i}}k, Miroslav},
doi = {10.1111/j.2007.0906-7590.05203.x},
eprint = {j.2007.0906-7590.05203.x},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Phillips, Dud{\'{i}}k - 2008 - Modeling of species distribution with Maxent new extensions and a comprehensive evalutation.pdf:pdf},
isbn = {0906-7590},
issn = {0007-1188},
journal = {Ecograpy},
keywords = {05203,0906-7590,10,1111,161 {\'{a}} 175,2007,2007 at,2007 ecography,2008,accepted 13 december 2007,doi,graphy 31,inc,j,jo,journal compilation,t,x},
number = {December 2007},
pages = {161--175},
pmid = {3245},
primaryClass = {10.1111},
title = {{Modeling of species distribution with Maxent: new extensions and a comprehensive evalutation}},
volume = {31},
year = {2008}
}
@article{Elith2006a,
abstract = {Prediction of species' distributions is central to diverse applications in ecology, evolution and conservation science. There is increasing electronic access to vast sets of occurrence records in museums and herbaria, yet little effective guidance on how best to use this information in the context of numerous approaches for modelling distributions. To meet this need, we compared 16 modelling methods over 226 species from 6 regions of the world, creating the most comprehensive set of model comparisons to date. We used presence-only data to fit models, and independent presence-absence data to evaluate the predictions. Along with well-established modelling methods such as generalised additive models and GARP and BIOCLIM, we explored methods that either have been developed recently or have rarely been applied to modelling species' distributions. These include machine-learning methods and community models, both of which have features that may make them particularly well suited to noisy or sparse information, as is typical of species' occurrence data. Presence-only data were effective for modelling species' distributions for many species and regions. The novel methods consistently outperformed more established methods. The results of our analysis are promising for the use of data from museums and herbaria, especially as methods suited to the noise inherent in such data improve.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Elith, J and Graham, C H and Anderson, R P and Dudik, M and Ferrier, S and Guisan, A and Hijmans, R J and Huettmann, F and Leathwick, J R and Lehmann, A and Li, J and Lohmann, L G and Loiselle, B A and Manion, G and Moritz, C and Nakamura, M and Nakazawa, Y and Overton, J M and Peterson, A T and Phillips, S J and Richardson, K and Scachetti-Pereira, R and Schapire, R E and Soberon, J and Williams, S and Wisz, M S and Zimmermann, N E},
doi = {10.1111/j.2006.0906-7590.04596.x},
eprint = {arXiv:1011.1669v3},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Elith et al. - 2006 - Novel methods improve prediction of species' distributions from occurrence data.pdf:pdf},
isbn = {1600-0587},
issn = {09067590},
journal = {Ecography (Cop.).},
keywords = {biodiversity,climate-change,conservation,distribution models,envelope models,habitat-suitability,logistic-regression,plant,potential distribution,spatial prediction},
number = {2},
pages = {129--151},
pmid = {1891},
title = {{Novel methods improve prediction of species' distributions from occurrence data}},
volume = {29},
year = {2006}
}
@article{Miller2012,
annote = {Tax Collection Optimization},
author = {Miller, Gerard and Weatherwax, Melissa and Gardinier, Timothy and Abe, Naoki and Melville, Prem and Pendus, Cezar and Jensen, David and Reddy, Chandan K and Thomas, Vince and Bennett, James and Anderson, Gary and Cooley, Brent},
doi = {10.1287/inte.1110.0618},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Miller et al. - 2012 - Tax Collections Optimization for New York State.pdf:pdf},
title = {{Tax Collections Optimization for New York State}},
url = {http://pubsonline.informs.org74-84.https//doi.org/10.1287/inte.1110.0618http://www.informs.org},
year = {2012}
}
@article{Phillips2006,
abstract = {The availability of detailed environmental data, together with inexpensive and powerful computers, has fueled a rapid increase in predictive modeling of species environmental requirements and geographic distributions. For some species, detailed presence/ absence occurrence data are available, allowing the use of a variety of standard statistical techniques. However, absence data are not available for most species. In this paper, we introduce the use of the maximum entropy method (Maxent) for modeling species geographic distributions with presence-only data. Maxent is a general-purpose machine learning method with a simple and precise mathematical formulation, and it has a number of aspects that make it well-suited for species distribution modeling. In order to investigate the efficacy of themethod, here we performa continental-scale case study using two Neotropicalmammals: a lowland species of sloth, Bradypus variegatus, and a small montane murid rodent, Microryzomys minutus.We compared Maxent predictions with those of a commonly used presence-only modeling method, the Genetic Algorithm for Rule-Set Prediction (GARP). We made predictions on 10 random subsets of the occurrence records for both species, and then used the remaining localities for testing. Both algorithms provided reasonable estimates of the species' range, far superior to the shaded outline maps available in field guides. All models were significantly better than random in both binomial tests of omission and receiver operating characteristic (ROC) analyses. The area under the ROC curve (AUC) was almost always higher for Maxent, indicating better discrimination of suitable versus unsuitable areas for the species. The Maxent modeling approach can be used in its present form for many applications with presence-only datasets, and merits further research and development.},
archivePrefix = {arXiv},
arxivId = {11265},
author = {Phillips, Steven J and Anderson, Robert P and Schapire, Robert E},
doi = {10.1016/j.ecolmodel.2005.03.026},
eprint = {11265},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Phillips, Anderson, Schapire - 2006 - Maximum entropy modeling of species geographic distributions.pdf:pdf},
isbn = {033},
issn = {14666650},
journal = {Ecol. Model. 190 231–259},
keywords = {Ammonia,CMAQ,Dry deposition,Flux,Modelling,Nitrogen,Wet deposition},
pmid = {1474},
title = {{Maximum entropy modeling of species geographic distributions}},
year = {2006}
}
@techreport{AlkaeeTaleghan2015a,
abstract = {In a simulator-defined MDP, the Markovian dynamics and rewards are provided in the form of a simulator from which samples can be drawn. This paper studies MDP planning algorithms that attempt to minimize the number of simulator calls before terminating and outputting a policy that is approximately optimal with high probability. The paper introduces two heuristics for efficient exploration and an improved confidence interval that enables earlier termination with probabilis-tic guarantees. We prove that the heuristics and the confidence interval are sound and produce with high probability an approximately optimal policy in polynomial time. Experiments on two benchmark problems and two instances of an invasive species management problem show that the improved confidence intervals and the new search heuristics yield reductions of between 8{\%} and 47{\%} in the number of simulator calls required to reach near-optimal policies.},
author = {{Alkaee Taleghan}, Majid and Dietterich, Thomas G and Crowley, Mark and Hall, Kim and {Jo Albers}, H and Auer, Peter and Hutter, Marcus and Orseau, Laurent},
booktitle = {J. Mach. Learn. Res.},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Alkaee Taleghan et al. - 2015 - PAC Optimal MDP Planning with Application to Invasive Species Management(2).pdf:pdf},
keywords = {Good-Turing estimate,MDP planning,Markov decision processes,invasive species management,reinforcement learning},
pages = {3877--3903},
title = {{PAC Optimal MDP Planning with Application to Invasive Species Management *}},
url = {http://jmlr.org/papers/volume16/taleghan15a/taleghan15a.pdf},
volume = {16},
year = {2015}
}
@article{stones,
author = {Stone, H S and Stone, J M},
journal = {IBM J. Res. Dev.},
pages = {464--474},
publisher = {International Business Machines.},
title = {{Efficient Search Techniques - An Empirical Study of the N-Queens Problem}},
volume = {31},
year = {1987}
}
@inproceedings{langley,
author = {Langley, P},
booktitle = {Proc. AAAI-92},
title = {{Systematic and Nonsystematic Search Strategies}},
year = {1992}
}
@techreport{Bradtke1996,
abstract = {We introduce two new temporal difference (TD) algorithms based on the theory of linear least-squares function approximation. We define an algorithm we call Least-Squares TD (LS TD) for which we prove probability-one convergence when it is used with a function approximator linear in the adjustable parameters. We then define a recursive version of this algorithm, Recursive Least-Squares TD (RLS TD). Although these new TD algorithms require more computation per time-step than do Sutton's TD(A) algorithms, they are more efficient in a statistical sense because they extract more information from training experiences. We describe a simulation experiment showing the substantial improvement in learning rate achieved by RLS TD in an example Markov prediction problem. To quantify this improvement, we introduce the TD error variance of a Markov chain, arc,, and experimentally conclude that the convergence rate of a TD algorithm depends linearly on {\~{}}ro. In addition to converging more rapidly, LS TD and RLS TD do not have control parameters, such as a learning rate parameter, thus eliminating the possibility of achieving poor performance by an unlucky choice of parameters.},
annote = {LSTD paper},
author = {Bradtke, Steven J and Barto, Andrew G},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bradtke, Barto - 1996 - Linear Least-Squares Algorithms for Temporal Difference Learning.pdf:pdf},
keywords = {Least-Squares,Markov Decision Problems,Reinforcement learning,Temporal Difference Methods},
pages = {33--57},
publisher = {Kluwer Academic Publishers},
title = {{Linear Least-Squares Algorithms for Temporal Difference Learning}},
url = {https://link.springer.com/content/pdf/10.1007{\%}2FBF00114723.pdf},
volume = {22},
year = {1996}
}
@article{Lillicrap2015,
abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
archivePrefix = {arXiv},
arxivId = {1509.02971},
author = {Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
eprint = {1509.02971},
journal = {ICLR},
title = {{Continuous control with deep reinforcement learning: Deep Deterministic Policy Gradients (DDPG)}},
year = {2015}
}
@article{Lydakis2018,
author = {Lydakis, Andreas and Allen, Jenica M. and Petrik, Marek and Szewczyk, Tim M.},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lydakis et al. - 2018 - Robust strategies for managing invasive plants.pdf:pdf},
journal = {IJCAI Artif. Intell. Wildl. Conserv.},
title = {{Robust strategies for managing invasive plants}},
year = {2018}
}
@book{brassard,
author = {Brassard, G and Bratley, P},
publisher = {Englewood Cliffs, NJ: Prentice Hall},
title = {{Algorithmics - Theory and Practice}},
year = {1988}
}
@article{Bengio2009,
abstract = {Theoretical results suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g., in vision, language, and other AI-level tasks), one may need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This monograph discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.},
archivePrefix = {arXiv},
arxivId = {submit/0500581},
author = {Bengio, Y.},
doi = {10.1561/2200000006},
eprint = {0500581},
isbn = {2200000006},
issn = {1935-8237},
journal = {Found. Trends{\textregistered} Mach. Learn.},
pmid = {17348934},
primaryClass = {submit},
title = {{Learning Deep Architectures for AI}},
year = {2009}
}
@techreport{Popov,
abstract = {Deep learning and reinforcement learning methods have recently been used to solve a variety of problems in continuous control domains. An obvious application of these techniques is dexterous manipulation tasks in robotics which are difficult to solve using traditional control theory or hand-engineered approaches. One example of such a task is to grasp an object and precisely stack it on another. Solving this difficult and practically relevant problem in the real world is an important long-term goal for the field of robotics. Here we take a step towards this goal by examining the problem in simulation and providing models and techniques aimed at solving it. We introduce two extensions to the Deep Deterministic Policy Gradient algorithm (DDPG), a model-free Q-learning based method, which make it significantly more data-efficient and scalable. Our results show that by making extensive use of off-policy data and replay, it is possible to find control policies that robustly grasp objects and stack them. Further, our results hint that it may soon be feasible to train successful stacking policies by collecting interactions on real robots.},
author = {Popov, Ivaylo and Heess, Nicolas and Lillicrap, Timothy and Hafner, Roland and Barth-Maron, Gabriel and Vecerik, Matej and Lampe, Thomas and Tassa, Yuval and Erez, Tom and Deepmind, Martin Riedmiller},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Popov et al. - Unknown - Data-efficient Deep Reinforcement Learning for Dexterous Manipulation.pdf:pdf},
title = {{Data-efficient Deep Reinforcement Learning for Dexterous Manipulation}},
url = {https://pdfs.semanticscholar.org/e701/9d9786af58f56c5de7e7daa2ce9050ba60f0.pdf}
}
@techreport{Lagoudakis2003,
abstract = {We propose a new approach to reinforcement learning for control problems which combines value-function approximation with linear architectures and approximate policy iteration. This new approach is motivated by the least-squares temporal-difference learning algorithm (LSTD) for prediction problems, which is known for its efficient use of sample experiences compared to pure temporal-difference algorithms. Heretofore, LSTD has not had a straightforward application to control problems mainly because LSTD learns the state value function of a fixed policy which cannot be used for action selection and control without a model of the underlying process. Our new algorithm, least-squares policy iteration (LSPI), learns the state-action value function which allows for action selection without a model and for incremental policy improvement within a policy-iteration framework. LSPI is a model-free, off-policy method which can use efficiently (and reuse in each iteration) sample experiences collected in any manner. By separating the sample collection method, the choice of the linear approximation architecture, and the solution method, LSPI allows for focused attention on the distinct elements that contribute to practical reinforcement learning. LSPI is tested on the simple task of balancing an inverted pendulum and the harder task of balancing and riding a bicycle to a target location. In both cases, LSPI learns to control the pendulum or the bicycle by merely observing a relatively small number of trials where actions are selected randomly. LSPI is also compared against Q-learning (both with and without experience replay) using the same value function architecture. While LSPI achieves good performance fairly consistently on the difficult bicycle task, Q-learning variants were rarely able to balance for more than a small fraction of the time needed to reach the target location.},
annote = {LSPI paper},
author = {Lagoudakis, Michail G and Parr, Ronald},
booktitle = {J. Mach. Learn. Res.},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lagoudakis, Parr - 2003 - Least-Squares Policy Iteration.pdf:pdf},
keywords = {Approximate Policy Iteration,Least-Squares Methods,Markov Decision Processes,Reinforcement Learning,Value-Function Approximation},
pages = {1107--1149},
title = {{Least-Squares Policy Iteration}},
url = {http://www.jmlr.org/papers/volume4/lagoudakis03a/lagoudakis03a.pdf},
volume = {4},
year = {2003}
}
@article{Phillips2004,
abstract = {We study the problem of modeling species geographic distributions, a critical problem in conservation biology. We propose the use of maximum-entropy techniques for this problem, specifically, sequential-update algorithms that can handle a very large number of features. We describe experiments comparing maxent with a standard distribution-modeling tool, called GARP, on a dataset containing observation data for North American breeding birds. We also study how well maxent performs as a function of the number of training examples and training time, analyze the use of regularization to avoid overfitting when the number of examples is small, and explore the interpretability of models constructed using maxent.},
author = {Phillips, Steven J and Dud{\'{i}}k, M and Schapire, R E},
doi = {10.1145/1015330.1015412},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Phillips, Dud{\'{i}}k, Schapire - 2004 - A maximum entropy approach to species distribution modeling.pdf:pdf},
isbn = {1581138285},
issn = {00147672},
journal = {Twentyfirst Int. Conf. Mach. Learn. ICML 04},
pages = {83},
pmid = {6379},
title = {{A maximum entropy approach to species distribution modeling}},
url = {http://portal.acm.org/citation.cfm?doid=1015330.1015412},
volume = {69},
year = {2004}
}
@inproceedings{hopfield,
author = {Hopfield, J J},
booktitle = {Proc. Natl. Acad. Sci.},
publisher = {Washington, DC: National Academy Press},
title = {{Neural Networks and Physical Systems with Emergent Collective Computational Abilities}},
volume = {79},
year = {1982}
}
@inproceedings{simmons-aaai88,
author = {Simmons, R G},
booktitle = {Proc. AAAI-88},
title = {{A Theory of Debugging Plans and Interpretations}},
year = {1988}
}
@inproceedings{adorf,
author = {Adorf, H M and Johnston, M D},
booktitle = {Proc. Int. Jt. Conf. Neural Networks},
title = {{A Discrete Stochastic Neural Network Algorithm for Constraint Satisfaction Problems}},
year = {1990}
}
@article{Royle2012,
abstract = {1.Understanding the factors affecting species occurrence is a pre-eminent focus of applied ecological research. However, direct information about species occurrence is lacking for many species. Instead, researchers sometimes have to rely on so-called presence-only data (i.e. when no direct information about absences is available), which often results from opportunistic, unstructured sampling. maxent is a widely used software program designed to model and map species distribution using presence-only data. 2.We provide a critical review of maxent as applied to species distribution modelling and discuss how it can lead to inferential errors. A chief concern is that maxent produces a number of poorly defined indices that are not directly related to the actual parameter of interest – the probability of occurrence ($\psi$). This focus on an index was motivated by the belief that it is not possible to estimate $\psi$ from presence-only data; however, we demonstrate that $\psi$ is identifiable using conventional likelihood methods under the assumptions of random sampling and constant probability of species detection. 3.The model is implemented in a convenient r package which we use to apply the model to simulated data and data from the North American Breeding Bird Survey. We demonstrate that maxent produces extreme under-predictions when compared to estimates produced by logistic regression which uses the full (presence/absence) data set. We note that maxent predictions are extremely sensitive to specification of the background prevalence, which is not objectively estimated using the maxent method. 4.As with maxent, formal model-based inference requires a random sample of presence locations. Many presence-only data sets, such as those based on museum records and herbarium collections, may not satisfy this assumption. However, when sampling is random, we believe that inference should be based on formal methods that facilitate inference about interpretable ecological quantities instead of vaguely defined indices.},
author = {Royle, J. Andrew and Chandler, Richard B. and Yackulic, Charles and Nichols, James D.},
doi = {10.1111/j.2041-210X.2011.00182.x},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Royle et al. - 2012 - Likelihood analysis of species occurrence probability from presence-only data for modelling species distributions.pdf:pdf},
isbn = {2041-210X},
issn = {2041210X},
journal = {Methods Ecol. Evol.},
keywords = {Bayes' rule,Detection probability,Logistic regression,Occupancy model,Occurrence probability,Presence-only data,Species distribution model,maxent},
number = {3},
pages = {545--554},
title = {{Likelihood analysis of species occurrence probability from presence-only data for modelling species distributions}},
volume = {3},
year = {2012}
}
@inproceedings{Hasselt2010,
abstract = {In some stochastic environments the well-known reinforcement learning algo-rithm Q-learning performs very poorly. This poor performance is caused by large overestimations of action values. These overestimations result from a positive bias that is introduced because Q-learning uses the maximum action value as an approximation for the maximum expected action value. We introduce an alter-native way to approximate the maximum expected value for any set of random variables. The obtained double estimator method is shown to sometimes under-estimate rather than overestimate the maximum expected value. We apply the double estimator to Q-learning to construct Double Q-learning, a new off-policy reinforcement learning algorithm. We show the new algorithm converges to the optimal policy and that it performs well in some settings in which Q-learning per-forms poorly due to its overestimation.},
archivePrefix = {arXiv},
arxivId = {1606.04615},
author = {Hasselt, Hado Van and Group, Adaptive Computation and Wiskunde, Centrum},
booktitle = {Neural Inf. Proceeding Syst.},
doi = {10.1016/j.tws.2009.08.006},
eprint = {1606.04615},
isbn = {9781617823800},
issn = {02638231},
pmid = {26150344},
title = {{Double Q-learning}},
year = {2010}
}
@inproceedings{min-aaai,
author = {Minton, S and Johnston, M and Philips, A B and Laird, P},
booktitle = {Proc. AAAI-90},
title = {{Solving Large Scale Constraint Satisfaction and Scheduling Problems Using a Heuristic Repair Method}},
year = {1990}
}
@article{Warton2010,
abstract = {Presence-only data, point locations where a species has been recorded as being present, are often used in modeling the distribution of a species as a function of a set of explanatory variables-whether to map species occurrence , to understand its association with the environment, or to predict its response to environmental change. Currently, ecologists most commonly analyze presence-only data by adding randomly chosen "pseudo-absences" to the data such that it can be analyzed using logistic regression, an approach which has weaknesses in model specification, in interpretation, and in implementation. To address these issues, we propose Poisson point process model-ing of the intensity of presences. We also derive a link between the proposed approach and logistic regression-specifically, we show that as the number of pseudo-absences increases (in a regular or uniform random arrangement), logistic regression slope parameters and their standard errors converge to those of the corresponding Poisson point process model. We discuss the practical implications of these results. In particular, point process modeling offers a framework for choice of the number and location of pseudo-absences, both of which are currently chosen by ad hoc and sometimes ineffective methods in ecology, a point which we illustrate by example.},
author = {Warton, David I and Shepherd, Leah C},
doi = {10.1214/10-AOAS331},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Warton, Shepherd - 2010 - POISSON POINT PROCESS MODELS SOLVE THE {\&}quotPSEUDO-ABSENCE PROBLEM{\&}quot FOR PRESENCE-ONLY DATA IN ECOLOGY 1.pdf:pdf},
journal = {Ann. Appl. Stat.},
keywords = {Habitat modeling,occurrence data,pseudo-absences,quadrature points,species distribution modeling},
number = {3},
pages = {1383--1402},
title = {{POISSON POINT PROCESS MODELS SOLVE THE "PSEUDO-ABSENCE PROBLEM" FOR PRESENCE-ONLY DATA IN ECOLOGY 1}},
url = {https://projecteuclid.org/download/pdfview{\_}1/euclid.aoas/1287409378},
volume = {4},
year = {2010}
}
@article{Phillips2009,
archivePrefix = {arXiv},
arxivId = {1132},
author = {Phillips, S. J.},
doi = {10.1890/07-2153.1},
eprint = {1132},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Phillips - 2009 - Sample selection bias and presence-only distribution models implications for background and pseudo-absence data Refer.pdf:pdf},
isbn = {1051-0761},
issn = {1051-0761},
journal = {Ecol. Appl.},
keywords = {background data,niche modeling,presence-only distribution models,pseudo-absence,sample selection bias,species distribution modeling,target group},
number = {1},
pages = {181--197},
pmid = {19323182},
title = {{Sample selection bias and presence-only distribution models : implications for background and pseudo-absence data Reference Sample selection bias and presence-only distribution models : implications for background and pseudo-absence data}},
volume = {19},
year = {2009}
}
@article{papad,
author = {Johnson, D S and Papadimitrou, C H and Yannakakis, M},
journal = {J. Comput. Syst. Sci.},
pages = {79--100},
title = {{How Easy is Local Search?}},
volume = {37},
year = {1988}
}
@inproceedings{johnston,
author = {Johnston, M D and Adorf, H M},
booktitle = {Proc. NASA Conf. Sp. Telerobotics},
title = {{Learning in Stochastic Neural Networks for Constraint Satisfaction Problems}},
volume = {37},
year = {1989}
}
@article{Ward2009,
author = {Ward, Gill and Hastie, Trevor and Barry, Simon and Elith, Jane and Leathwick, John R.},
doi = {10.1111/j.1541-0420.2008.01116.x},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ward et al. - 2009 - Presence-Only Data and the EM Algorithm.pdf:pdf;:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ward et al. - 2009 - Presence-Only Data and the EM Algorithm(2).pdf:pdf},
issn = {0006341X},
journal = {Biometrics},
keywords = {Boosted trees,EM algorithm,Logistic model,Presence‐only data,Use‐availability data},
month = {jun},
number = {2},
pages = {554--563},
publisher = {Wiley/Blackwell (10.1111)},
title = {{Presence-Only Data and the EM Algorithm}},
url = {http://doi.wiley.com/10.1111/j.1541-0420.2008.01116.x},
volume = {65},
year = {2009}
}
@article{Mnih2015,
abstract = {An artificial agent is developed that learns to play a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mnih et al. - 2015 - Human-level control through deep reinforcement learning.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
keywords = {Computer science},
month = {feb},
number = {7540},
pages = {529--533},
publisher = {Nature Publishing Group},
title = {{Human-level control through deep reinforcement learning}},
url = {http://www.nature.com/articles/nature14236},
volume = {5aaIA18},
year = {2015}
}
@techreport{Boyan1998,
author = {Boyan, Justin A},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Boyan - 1998 - Learning Evaluation Functions for Global Optimization.pdf:pdf},
title = {{Learning Evaluation Functions for Global Optimization}},
url = {https://pdfs.semanticscholar.org/61d4/897dbf7ced83a0eb830a8de0dd64abb58ebd.pdf},
year = {1998}
}
@article{Phillips2006,
abstract = {The availability of detailed environmental data, together with inexpensive and powerful computers, has fueled a rapid increase in predictive modeling of species environmental requirements and geographic distributions. For some species, detailed presence/ absence occurrence data are available, allowing the use of a variety of standard statistical techniques. However, absence data are not available for most species. In this paper, we introduce the use of the maximum entropy method (Maxent) for modeling species geographic distributions with presence-only data. Maxent is a general-purpose machine learning method with a simple and precise mathematical formulation, and it has a number of aspects that make it well-suited for species distribution modeling. In order to investigate the efficacy of themethod, here we performa continental-scale case study using two Neotropicalmammals: a lowland species of sloth, Bradypus variegatus, and a small montane murid rodent, Microryzomys minutus.We compared Maxent predictions with those of a commonly used presence-only modeling method, the Genetic Algorithm for Rule-Set Prediction (GARP). We made predictions on 10 random subsets of the occurrence records for both species, and then used the remaining localities for testing. Both algorithms provided reasonable estimates of the species' range, far superior to the shaded outline maps available in field guides. All models were significantly better than random in both binomial tests of omission and receiver operating characteristic (ROC) analyses. The area under the ROC curve (AUC) was almost always higher for Maxent, indicating better discrimination of suitable versus unsuitable areas for the species. The Maxent modeling approach can be used in its present form for many applications with presence-only datasets, and merits further research and development.},
archivePrefix = {arXiv},
arxivId = {11265},
author = {Phillips, Steven J and Anderson, Robert P and Schapire, Robert E},
doi = {10.1016/j.ecolmodel.2005.03.026},
eprint = {11265},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Phillips, Anderson, Schapire - 2006 - Maximum entropy modeling of species geographic distributions.pdf:pdf},
isbn = {033},
issn = {14666650},
journal = {Ecol. Model. 190 231–259},
keywords = {Ammonia,CMAQ,Dry deposition,Flux,Modelling,Nitrogen,Wet deposition},
pmid = {1474},
title = {{Maximum entropy modeling of species geographic distributions}},
year = {2006}
}
@article{Phillips2008,
abstract = {1 The overall functional capacity of the liver was evaluated using [35S]-bromosulphophthalein (BSP, 100 mg/kg, i.v.) in biliary fistulated adult rats pretreated orally with different doses of paracetamol (APAP) for varying time intervals. 2 The maximal hepatic damage occurred between 12-18 h after single doses of APAP (0.5 or 1 g/kg); hepatic excretory function returned to control levels by 48-72 hours. 3 Administration of either 0.5 or 1 g/kg APAP 18 h before BSP caused a dose-dependent inhibition of the choleretic effect of BSP and of the 60 min cumulative excretion of the dye, but conversely, produced a significant increase in the liver and plasma concentrations of 35S. 4 Following acute (0.25 g/kg), or subacute (0.5 g/kg, twice daily for 7 days) treatment with APAP, the total excretion of 35S in bile and the retention of 35S in the liver or plasma remained essentially the same as that for the controls. 5 In rats given single doses of 1 g/kg APAP, the hepatic uptake of the dye was significantly increased during the early stages of intoxication, while the opposite effect was observed at late periods. 6 The bile flow appeared to be inversely related to the excretion of unchanged BSP, and directly related to the excretion of the major BSP conjugate in bile. 7 The hepatic clearance of BSP was more rapid in rats treated subacutely with 0.5 or 1 g/kg APAP, than in those treated acutely with equal doses, suggesting that the intensity of APAP-induced hepatotoxicity became less severe after the repeated administration of this drug. 8 It is concluded that the hepatic uptake, metabolism and excretion of BSP are reversibly impaired following APAP-induced liver injury.},
archivePrefix = {arXiv},
arxivId = {10.1111/j.2007.0906-7590.05203.x},
author = {Phillips, Steven J. and Dud{\'{i}}k, Miroslav},
doi = {10.1111/j.2007.0906-7590.05203.x},
eprint = {j.2007.0906-7590.05203.x},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Phillips, Dud{\'{i}}k - 2008 - Modeling of species distribution with Maxent new extensions and a comprehensive evalutation.pdf:pdf},
isbn = {0906-7590},
issn = {0007-1188},
journal = {Ecograpy},
keywords = {05203,0906-7590,10,1111,161 {\'{a}} 175,2007,2007 at,2007 ecography,2008,accepted 13 december 2007,doi,graphy 31,inc,j,jo,journal compilation,t,x},
number = {December 2007},
pages = {161--175},
pmid = {3245},
primaryClass = {10.1111},
title = {{Modeling of species distribution with Maxent: new extensions and a comprehensive evalutation}},
volume = {31},
year = {2008}
}
@inproceedings{Lange2010,
abstract = {This paper discusses the effectiveness of deep auto-encoder neural networks in visual reinforcement learning (RL) tasks. We propose a framework for combining the training of deep auto-encoders (for learning compact feature spaces) with recently-proposed batch-mode RL algorithms (for learning policies). An emphasis is put on the data-efficiency of this combination and on studying the properties of the feature spaces automatically constructed by the deep auto-encoders. These feature spaces are empirically shown to adequately resemble existing similarities and spatial relations between observations and allow to learn useful policies. We propose several methods for improving the topology of the feature spaces making use of task-dependent information. Finally, we present first results on successfully learning good control policies directly on synthesized and real images.},
archivePrefix = {arXiv},
arxivId = {1507.04296},
author = {Lange, Sascha and Riedmiller, Martin},
booktitle = {Proc. Int. Jt. Conf. Neural Networks},
doi = {10.1109/IJCNN.2010.5596468},
eprint = {1507.04296},
isbn = {9781424469178},
issn = {1098-7576},
pmid = {25719670},
title = {{Deep auto-encoder neural networks in reinforcement learning}},
year = {2010}
}
@article{Fithian2013,
abstract = {Statistical modeling of presence-only data has attracted much recent attention in the ecological literature, leading to a proliferation of methods, including the inhomogeneous Poisson process (IPP) model, maximum entropy (Maxent) modeling of species distributions and logistic regression models. Several recent articles have shown the close relationships between these methods. We explain why the IPP intensity function is a more natural object of inference in presence-only studies than occurrence probability (which is only defined with reference to quadrat size), and why presence-only data only allows estimation of relative, and not absolute intensity of species occurrence. All three of the above techniques amount to parametric density estimation under the same exponential family model (in the case of the IPP, the fitted density is multiplied by the number of presence records to obtain a fitted intensity). We show that IPP and Maxent give the exact same estimate for this density, but logistic regression in general yields a different estimate in finite samples. When the model is misspecified - as it practically always is - logistic regression and the IPP may have substantially different asymptotic limits with large data sets. We propose ``infinitely weighted logistic regression,'' which is exactly equivalent to the IPP in finite samples. Consequently, many already-implemented methods extending logistic regression can also extend the Maxent and IPP models in directly analogous ways using this technique.},
archivePrefix = {arXiv},
arxivId = {1207.6950},
author = {Fithian, William and Hastie, Trevor},
doi = {10.1214/13-AOAS667},
eprint = {1207.6950},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fithian, Hastie - 2013 - Finite-sample equivalence in statistical models for presence-only data.pdf:pdf},
isbn = {1932-6157},
issn = {19326157},
journal = {Ann. Appl. Stat.},
keywords = {Case-control sampling,Logistic regression,Maximum entropy,Poisson process models,Presence-only data,Species modeling},
number = {4},
pages = {1917--1939},
pmid = {25493106},
title = {{Finite-sample equivalence in statistical models for presence-only data}},
volume = {7},
year = {2013}
}
@article{Dudik2004a,
abstract = {We consider the problem of estimating an unknown probability distribution from samples using the principle of maximum entropy (maxent). To alleviate overfitting with a very large number of features, we propose applying the maxent principle with relaxed constraints on the expectations of the features. By convex duality, this turns out to be equivalent to finding the Gibbs distribution minimizing a regularized version of the empirical log loss. We prove non-asymptotic bounds showing that, with respect to the true underlying distribution, this relaxed version of maxent produces density estimates that are almost as good as the best possible. These bounds are in terms of the deviation of the feature empirical averages relative to their true expectations, a number that can be bounded using standard uniform-convergence techniques. In particular, this leads to bounds that drop quickly with the number of samples, and that depend very moderately on the number or complexity of the features. We also derive and prove convergence for both sequential-update and parallel-update algorithms. Finally, we briefly describe experiments on data relevant to the modeling of species geographical distributions.},
author = {Dud{\'{i}}k, Miroslav and Phillips, Steven J. and Schapire, Robert E.},
doi = {10.1007/978-3-540-27819-1_33},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dud{\'{i}}k, Phillips, Schapire - 2004 - Performance Guarantees for Regularized Maximum Entropy Density Estimation.pdf:pdf},
isbn = {0302-9743},
issn = {03029743},
pages = {472--486},
pmid = {6185},
title = {{Performance Guarantees for Regularized Maximum Entropy Density Estimation}},
url = {http://link.springer.com/10.1007/978-3-540-27819-1{\_}33},
year = {2004}
}
@techreport{Watkins1992,
abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states. This paper presents and proves in detail a convergence theorem for Q,-learning based on that outlined in Watkins (1989). We show that Q-learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where many Q values can be changed each iteration, rather than just one.},
author = {Watkins, Christopher J C H and Dayan, Peter},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Watkins, Dayan - 1992 - Technical Note Q,-Learning.pdf:pdf},
keywords = {Q-learning,asynchronous dynamic programming,reinforcement learning,temporal differences},
pages = {279--292},
title = {{Technical Note Q,-Learning}},
url = {http://www.gatsby.ucl.ac.uk/{~}dayan/papers/cjch.pdf},
volume = {8},
year = {1992}
}
@article{FitzpatrickMC;GotelliNJ;Ellison2013,
abstract = {MaxEnt is one of the most widely used tools in ecology, biogeography, and evolution for modeling and mapping species distributions using presence-only occurrence records and associated environmental covariates. Despite its popularity, the exponential model implemented by MaxEnt does not directly estimate occurrence probability, the natural quantity of interest when modeling species distributions. Instead, MaxEnt generates an index of relative habitat suitability. MaxLike, a newly introduced maximum-likelihood technique, has been shown to overcome the problem of directly estimating the probability of occurrence using presence-only data. However, the performance and relative merits of MaxEnt and MaxLike remain largely untested, especially when modeling species with relatively few occurrence data that encompass only a portion of the geographic range of the species. Using geo- referenced occurrence records for six species of ants in New England, we provide comparisons of MaxEnt and MaxLike. We show that by most quantitative metrics, the performance of MaxLike exceeds that of MaxEnt, regardless of whether MaxEnt models account for sampling bias and include greater model complexity than implemented in MaxLike. More importantly, for most species, the relative suitability index estimated by MaxEnt oftenwas poorly correlated with the probability of occurrence estimated by MaxLike, suggesting that the two methods are estimating different quantities. For species distribution modeling, MaxLike, and similar models that are based on an explicit sampling process and that directly estimate probability of occurrence, should be considered as important alternatives to the widely-used MaxEnt framework.},
author = {{Fitzpatrick, MC; Gotelli, NJ; Ellison}, Am},
doi = {10.1890/es13-00066.1},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fitzpatrick, MC Gotelli, NJ Ellison - 2013 - MaxEnt vs. MaxLike Empirical comparisons with ant species distributions.pdf:pdf},
isbn = {2150-8925},
issn = {2150-8925},
journal = {Ecosphere},
keywords = {ecological niche modeling,myrmecology,new england,occurrence probability,presence-only data,species distribution modeling},
number = {May},
pages = {1--15},
title = {{MaxEnt vs. MaxLike: Empirical comparisons with ant species distributions}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:MaxEnt+versus+MaxLike+:+empirical+comparisons+with+ant+species+distributions{\#}0},
volume = {4},
year = {2013}
}
@article{Phillips2009,
archivePrefix = {arXiv},
arxivId = {1132},
author = {Phillips, S J},
doi = {10.1890/07-2153.1},
eprint = {1132},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Phillips - 2009 - Sample selection bias and presence-only distribution models implications for background and pseudo-absence data Refer.pdf:pdf},
isbn = {1051-0761},
issn = {1051-0761},
journal = {Ecol. Appl.},
keywords = {background data,niche modeling,presence-only distribution models,pseudo-absence,sample selection bias,species distribution modeling,target group},
number = {1},
pages = {181--197},
pmid = {19323182},
title = {{Sample selection bias and presence-only distribution models : implications for background and pseudo-absence data Reference Sample selection bias and presence-only distribution models : implications for background and pseudo-absence data}},
volume = {19},
year = {2009}
}
@techreport{Mnih2016b,
abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
author = {Mnih, Volodymyr and {Puigdom{\`{e}}nech Badia}, Adri{\`{a}} and Mirza, Mehdi and Graves, Alex and Harley, Tim and Lillicrap, Timothy P and Silver, David and Kavukcuoglu, Koray},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learning(2).pdf:pdf},
title = {{Asynchronous Methods for Deep Reinforcement Learning}},
url = {http://proceedings.mlr.press/v48/mniha16.pdf},
year = {2016}
}
@article{Lydakis2018,
author = {Lydakis, Andreas and Allen, Jenica M. and Petrik, Marek and Szewczyk, Tim M.},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lydakis et al. - 2018 - Robust strategies for managing invasive plants.pdf:pdf},
journal = {IJCAI Artif. Intell. Wildl. Conserv.},
title = {{Robust strategies for managing invasive plants}},
year = {2018}
}
