\documentclass[a4paper,12pt]{article}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

%
% For alternative styles, see the biblatex manual:
% http://mirrors.ctan.org/macros/latex/contrib/biblatex/doc/biblatex.pdf
%
% The 'verbose' family of styles produces full citations in footnotes, 
% with and a variety of options for ibidem abbreviations.
%
\usepackage{graphicx}
\usepackage{csquotes}
\usepackage[style=verbose-ibid,backend=bibtex]{biblatex}
\bibliography{sample}

\title{Markovian Approach to Positive-only Problems}

\author{Shayan Amani}

\begin{document}
\maketitle

\section{Preliminaries}
Number of possible policies is equal to number of actions to the power of number of states.

\subsection{Prediction \& Control Tasks}
From the perspective of tasks, one can divide RL tasks into two distinct categories, prediction and control. It is worth to mention that policy iteration includes both prediction and control steps.

\subsubsection{Prediction}
We are given with the policy $\pi(a|s)$ and we need to \textit{predict} expected cumulative reward from any specified states. As an example of this task, we can mention \textbf{policy evaluation} step in algorithms such as Dynamic Programming.

\subsubsection{Control}
In this type of task we try to find a policy $\pi(a|s)$ which maximizes the expected cumulative reward. As an example of this task, we can mention \textbf{policy improvement} step in algorithms such as Dynamic Programming.

\subsection{On-policy vs. Off-policy}
Depending on how a method evaluates or improves the policy we can divide policies into two groups, on-policy and off-policy. In addition, there are two types of policy which need to be discussed here, behaviour and target policies. The policy that the algorithm uses to control the current action is behaviour and the policy that is being learned and/or evaluated is target

\subsubsection{On-policy methods}
This type of methods try to evaluate or improve the same policy from which the taken action comes. Examples of such methods are as follows:
\begin{itemize}
    \item Monte Carlo ES
    \item SARSA
\end{itemize}

\subsubsection{Off-policy methods}
These methods try to evaluate or improve a different policy than the one that used. Examples of such methods are as follows:
\begin{itemize}
    \item Q-learning
\end{itemize}

\subsection{Model-based vs. Model-free}
\textbf{Model-based} RL tries to enclose mechanics of the system. In that case, dependency on data for sampling would decrease. On the other hand, sampling is the major fact in \textbf{model-free} RL. Cost-wise talking, it is reasonable to probe a model instead of taking samples from a dataset or a simulator until sampling is not an expensive solution anymore. Unnecessary to mention that \textbf{transition probability P is the model} in model-based learning methods and has to be explicitly provided in such algorithms.

\subsubsection{General models}
The modeling frameworks can be used to represent the dynamics of the system are as follows:
\begin{itemize}
    \item Gaussian process: two similar inputs, yield two similar outputs too.
    \item Gaussian Mixture Model (GMM)
    \item Deep networks
\end{itemize}

\subsubsection{Model-based algorithms}
These group of algorithms completely dependent on transition probabilities which should be provided. List of a handful of such algorithms:
\begin{itemize}
    \item Dynamic programming
\end{itemize}

\subsubsection{Model-free algorithms}
Despite of previous category, these algorithms do not need transition probabilities and only rely on trial-and-error to probing their environment. List of a handful of such algorithms:
\begin{itemize}
    \item Q-learning
\end{itemize}


\section{Bounding}
We can use available inequalities to bound our solution. We are always interested in finding upper bound for the policies or solution we propose. Hence, by applying those inequalities -based on requirements of each of them- the upper-bound can be found. Confidence intervals come into the problem where the results are bounded with an upper-bound. Moreover, confidence intervals are authentic measures of reliability of a solution.

\subsection{Markov's Inequality}
The most simplistic inequality among the other described here in terms of being relaxed about the constraints and requirements. The only requirement that needs to be satisfied is $X \geq 0$.
\begin{equation}
\mathrm {P}(X \geq a ) \leq \frac { \mathrm { E}( X ) } { a }
\end{equation}
Note that it should be not confused with Markov brothers' inequality.

\subsection{Chebyshev's Inequality}
\begin{equation}
\mathrm {P}(|X - \mathrm{E}([X])| \geq a ) \leq \frac { \mathrm { Var}( X ) } { a^2 }
\end{equation}

\subsection{Chernoff bound}

\subsection{Hoeffding's Inequality}
Using Hoeffding's lemma we can prove this inequality which the definition is as follows:
\begin{equation}
\mathrm {P}(|\frac{1}{n} \sum_{i=1}^{n} \mathrm{X_i} - \mathrm{E}[X_i]| \geq a ) \leq  \mathrm { e^{\frac{-2n a^2}{(d-b)^2}}}
\end{equation}

As PAC Optimal MDP Planning paper \autocite{Telghan et al. 2015} proposed they replaced Hoeffding-bound with Weissman confidence interval.
    

\section{Methodologies}
We are studying a simulator-based MDP problem. transition probabilities and rewards can be generated by the simulator which is a generative model built upon different environmental parameters.

\subsection{$\epsilon$-greedy Algorithm}
Popular in Deep Reinforcement Learning (DRL) which makes a good trade-off in terms of exploration and exploitation.



\section{Case Study}
The current case is a study on an invasive plant (non-native to the ecosystem) in New England area, namely glossy buckthorn [capitalization?].


\section{Bayesian Settings}
Sampling using a logistic regression (or any other methods) we can generate some distributions for our features (bio10 and bio5 REVIEW and alpha) then we are able to feed these to a linear program to calculate weights (L1 norm or any other distance metrics) and then we can build our ambiguity set based on what we get as weights.
\subsection{Dirichlet Distribution}
    In Bayesian analysis, this type of distribution is widely used as prior distribution.
    
\subsection{Pareto Distribution}
    People usually use this distribution as a tool to model the tail region of another distribution.
% \includegraphics[width=1\columnwidth]{elements.png}

\section{Multi-armed Bandit Problem vs. A/B Test}
In a multi-armed bandit problem agent choose and pull one of the arms randomly at the beginning. The pulled arm bandit generates a distribution with specific values of mean $\mu_i$ and variance. The distribution is stationary but still unknown before pulling. \autocite{Strehl and Littman 2008}

\begin{tabular}{|c|c|}
    \hline
    \textbf{A/B test}    &   \textbf{Multi-armed bandit}  \\
    \hline
    \hline
    pure exploration    &   exploration along with exploitation \\
    \hline
    return after completion &   immediate return while running \\
    \hline
    expensive   &   cost-efficient \\
    \hline
\end{tabular}

\subsubsection{Upper Confidence Bound Algorithm}
Upper confidence interval includes true expected value and it gets narrow down (becomes smaller) after each iteration of the algorithm. In other words, the agent becomes more assured about the return value as it runs through each iterations. The agent picks the next upper confidence interval to exploit and continue with that until it finds a new higher value. Unneeded to mention that the two main actor in this algorithm are as follows:
\begin{itemize}
    \item Confidence interval becomes tighter along with iterations
    \item Return value along iterations converges to the expected true value.
\end{itemize}

\section {Future Review}
\begin{itemize}
    \item Interval Estimation approach relationship with multi-armed bandit.

\end{itemize}


% This is an example citation \autocite{ginsberg}.
% \lipsum[1] % dummy text

% This is another example citation \autocite{brassard}.
% \lipsum[2] % dummy text

% This is a repeated citation \autocite{brassard}.
% \lipsum[3] % dummy text

% This is another example citation \autocite{adorf}.
% \lipsum[4] % dummy text 

\end{document}