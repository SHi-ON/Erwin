Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@misc{sadeghi2016a,
author = {Sadeghi, F and Levine, S},
edition = {arXiv prep},
title = {{CAD2RL: Real single-image flight without a single real image}}
}
@misc{petrik2009a,
author = {Petrik, M and Scherrer, B},
title = {{“Biasing approximate dynamic programming with a lower discount factor”. In: Advances in neural information processing systems. 1265–1272}}
}
@book{goodfellow2016a,
author = {Goodfellow, I and Bengio, Y and Courville, A},
publisher = {MIT Press},
title = {{Deep learning}}
}
@article{li2016a,
author = {Li, L and Lv, Y and Wang, F.-Y.},
journal = {IEEE/CAA Journal of Automatica Sinica},
number = {3},
pages = {247--254},
title = {{Traffic signal timing via deep reinforcement learning}},
volume = {3}
}
@article{kaelbling1998a,
author = {Kaelbling, L P and Littman, M L and Cassandra, A R},
journal = {Artificial},
number = {1},
pages = {99--134},
title = {{Planning and acting in partially observable stochastic domains}},
volume = {101}
}
@inproceedings{dinculescu2010a,
author = {Dinculescu, M and Precup, D},
booktitle = {Proceedings of the 27th International Conference on Machine Learning},
pages = {895--902},
title = {{Approximate predictive representations of partially observable systems}},
volume = {10}
}
@inproceedings{bouckaert2003a,
author = {Bouckaert, R R},
booktitle = {Proceedings of the 20th International Conference on Machine Learning},
pages = {51--58},
title = {{Choosing between two learning algorithms based on calibrated tests}},
volume = {3}
}
@misc{baird1995a,
author = {Baird, L},
pages = {30--37},
title = {{“Residual algorithms: Reinforcement learning with function approximation”. In: ICML}}
}
@article{Scott1979,
author = {Scott, David W.},
doi = {10.2307/2335182},
issn = {00063444},
journal = {Biometrika},
month = {dec},
number = {3},
pages = {605},
publisher = {JSTOR},
title = {{On Optimal and Data-Based Histograms}},
volume = {66},
year = {1979}
}
@incollection{kulkarni2016a,
author = {Kulkarni, T D and Narasimhan, K and Saeedi, A and Tenenbaum, J},
booktitle = {Advances in Neural Information Processing Systems},
pages = {3675--3683},
title = {{Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation}}
}
@article{Singh1995,
author = {Singh, Satinder P and Jaakkola, Tommi and Jordan, Mchael J},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Singh, Jaakkola, Jordan - 1995 - Reinforcement Learning with Soft State Aggregation.pdf:pdf},
issn = {1049-5258},
journal = {Advances in Neural Information Processing Systems 7},
pages = {361--368},
title = {{Reinforcement Learning with Soft State Aggregation}},
year = {1995}
}
@misc{jaderberg2018a,
author = {Jaderberg, M and Czarnecki, W M and Dunning, I and Marris, L and Lever, G and Castaneda, A G and Beattie, C and Rabinowitz, N C and Morcos, A S and Ruderman, A},
edition = {arXiv prep},
title = {{Human-level performance in firstperson multiplayer games with population-based deep reinforcement learning}}
}
@incollection{murphy2012a,
author = {Murphy, K P},
edition = {arXiv prep},
title = {{Machine Learning: A Probabilistic Perspective.}}
}
@misc{christiano2017a,
author = {Christiano, P and Leike, J and Brown, T B and Martic, M and Legg, S and Amodei, D},
edition = {arXiv prep},
title = {{Deep reinforcement learning from human preferences}}
}
@article{dayan2008a,
author = {Dayan, P and Daw, N D},
journal = {Cognitive, Affective, {\&} Behavioral Neuroscience},
number = {4},
pages = {429--453},
title = {{Decision theory, reinforcement learning, and the brain}},
volume = {8}
}
@inproceedings{narvekar2016a,
author = {Narvekar, S and Sinapov, J and Leonetti, M and Stone, P},
booktitle = {Proceedings of the 2016 International Conference on Autonomous Agents {\&} Multiagent Systems. International Foundation for Autonomous Agents and Multiagent Systems},
pages = {566--574},
title = {{Source task creation for curriculum learning}}
}
@article{dem2006a,
author = {Dem{\v{s}}ar, J},
journal = {Journal of Machine learning research},
pages = {1--30},
title = {{Statistical comparisons of classifiers over multiple data sets}},
volume = {7}
}
@inproceedings{jiang2016a,
author = {Jiang, N and Li, L},
booktitle = {Proceedings of The 33rd International Conference on Machine Learning},
pages = {652--661},
title = {{Doubly robust off-policy value evaluation for reinforcement learning}}
}
@misc{bellemare2018a,
author = {Bellemare, M G and Castro, P S and Gelada, C and Saurabh, K and Moitra, S},
title = {{Dopamine}},
url = {https://github.com/google/dopamine.}
}
@misc{zhang2018a,
author = {Zhang, A and Satija, H and Pineau, J},
edition = {arXiv prep},
title = {{Decoupling Dynamics and Reward for Transfer Learning}}
}
@article{williams1992a,
author = {Williams, R J},
journal = {Machine learning},
number = {3-4},
pages = {229--256},
title = {{Simple statistical gradient-following algorithms for connectionist reinforcement learning}},
volume = {8}
}
@misc{klambauer2017a,
author = {Klambauer, G and Unterthiner, T and Mayr, A and Hochreiter, S},
edition = {arXiv prep},
title = {{SelfNormalizing Neural Networks}}
}
@article{Phillips2006,
abstract = {The availability of detailed environmental data, together with inexpensive and powerful computers, has fueled a rapid increase in predictive modeling of species environmental requirements and geographic distributions. For some species, detailed presence/ absence occurrence data are available, allowing the use of a variety of standard statistical techniques. However, absence data are not available for most species. In this paper, we introduce the use of the maximum entropy method (Maxent) for modeling species geographic distributions with presence-only data. Maxent is a general-purpose machine learning method with a simple and precise mathematical formulation, and it has a number of aspects that make it well-suited for species distribution modeling. In order to investigate the efficacy of themethod, here we performa continental-scale case study using two Neotropicalmammals: a lowland species of sloth, Bradypus variegatus, and a small montane murid rodent, Microryzomys minutus.We compared Maxent predictions with those of a commonly used presence-only modeling method, the Genetic Algorithm for Rule-Set Prediction (GARP). We made predictions on 10 random subsets of the occurrence records for both species, and then used the remaining localities for testing. Both algorithms provided reasonable estimates of the species' range, far superior to the shaded outline maps available in field guides. All models were significantly better than random in both binomial tests of omission and receiver operating characteristic (ROC) analyses. The area under the ROC curve (AUC) was almost always higher for Maxent, indicating better discrimination of suitable versus unsuitable areas for the species. The Maxent modeling approach can be used in its present form for many applications with presence-only datasets, and merits further research and development.},
archivePrefix = {arXiv},
arxivId = {11265},
author = {Phillips, Steven J and Anderson, Robert P and Schapire, Robert E},
doi = {10.1016/j.ecolmodel.2005.03.026},
eprint = {11265},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Phillips, Anderson, Schapire - 2006 - Maximum entropy modeling of species geographic distributions(3).pdf:pdf},
isbn = {033},
issn = {14666650},
journal = {Ecological Modelling 190 (2006) 231–259},
keywords = {Ammonia,CMAQ,Dry deposition,Flux,Modelling,Nitrogen,Wet deposition},
pmid = {1474},
title = {{Maximum entropy modeling of species geographic distributions}},
year = {2006}
}
@article{Renner2013,
abstract = {Summary Modeling the spatial distribution of a species is a fundamental problem in ecology. A number of modeling methods have been developed, an extremely popular one being MAXENT, a maximum entropy modeling approach. In this article, we show that MAXENT is equivalent to a Poisson regression model and hence is related to a Poisson point process model, differing only in the intercept term, which is scale-dependent in MAXENT. We illustrate a number of improvements to MAXENT that follow from these relations. In particular, a point process model approach facilitates methods for choosing the appropriate spatial resolution, assessing model adequacy, and choosing the LASSO penalty parameter, all currently unavailable to MAXENT. The equivalence result represents a significant step in the unification of the species distribution modeling literature.},
author = {Renner, Ian W. and Warton, David I.},
doi = {10.1111/j.1541-0420.2012.01824.x},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Renner, Warton - 2013 - Equivalence of MAXENT and Poisson Point Process Models for Species Distribution Modeling in Ecology(3).pdf:pdf},
isbn = {1541-0420},
issn = {0006341X},
journal = {Biometrics},
keywords = {Habitat modeling,Location-only,Maximum entropy,Poisson likelihood,Presence-only data,Use-availability},
number = {1},
pages = {274--281},
pmid = {23379623},
title = {{Equivalence of MAXENT and Poisson Point Process Models for Species Distribution Modeling in Ecology}},
volume = {69},
year = {2013}
}
@misc{machado2017a,
author = {Machado, M C and Bellemare, M G and Talvitie, E and Veness, J and Hausknecht, M and Bowling, M},
edition = {arXiv prep},
title = {{Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents}}
}
@misc{aytar2018a,
author = {Aytar, Y and Pfaff, T and Budden, D and Paine, T L and Wang, Z and de Freitas, N},
edition = {arXiv prep},
title = {{Playing hard exploration games by watching YouTube}}
}
@misc{jaderberg2016a,
author = {Jaderberg, M and Mnih, V and Czarnecki, W M and Schaul, T and Leibo, J Z and Silver, D and Kavukcuoglu, K},
edition = {arXiv prep},
title = {{Reinforcement learning with unsupervised auxiliary tasks}}
}
@article{d2001a,
author = {D{\v{z}}eroski, S and Raedt, L.De and Driessens, K},
journal = {Machine learning},
number = {1-2},
pages = {7--52},
title = {{Relational reinforcement learning}},
volume = {43}
}
@misc{matiisen2017a,
author = {Matiisen, T and Oliver, A and Cohen, T and Schulman, J},
edition = {arXiv prep},
title = {{TeacherStudent Curriculum Learning}}
}
@misc{rowland2018a,
author = {Rowland, M and Bellemare, M G and Dabney, W and Munos, R and Teh, Y W},
edition = {arXiv prep},
title = {{An Analysis of Categorical Distributional Reinforcement Learning}}
}
@misc{henderson2017a,
author = {Henderson, P and Chang, W.-D. and Shkurti, F and Hansen, J and Meger, D and Dudek, G},
title = {{“Benchmark Environments for Multitask Learning in Continuous Domains”. ICML Lifelong Learning: A Reinforcement Learning Approach Workshop}}
}
@misc{casadevall2010a,
author = {Casadevall, A and Fang, F C},
title = {{Reproducible science}}
}
@article{tanner2009a,
author = {Tanner, B and White, A},
journal = {The Journal of Machine Learning Research},
pages = {2133--2136},
title = {{RL-Glue: Language-independent software for reinforcement-learning experiments}},
volume = {10}
}
@article{baker2016a,
author = {Baker, M},
journal = {Nature News},
number = {7604},
pages = {452},
title = {1,500 scientists lift the lid on reproducibility},
volume = {533}
}
@misc{harari2014a,
author = {Harari, Y N},
title = {{Sapiens: A brief history of humankind}}
}
@misc{wu2016a,
author = {Wu, Y and Tian, Y},
title = {{Training agent for first-person shooter game with actor-critic curriculum learning}}
}
@article{Phillips2009,
archivePrefix = {arXiv},
arxivId = {1132},
author = {Phillips, S. J.},
doi = {10.1890/07-2153.1},
eprint = {1132},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Phillips - 2009 - Sample selection bias and presence-only distribution models implications for background and pseudo-absence data Ref(3).pdf:pdf},
isbn = {1051-0761},
issn = {1051-0761},
journal = {Ecological Applications},
keywords = {background data,niche modeling,presence-only distribution models,pseudo-absence,sample selection bias,species distribution modeling,target group},
number = {1},
pages = {181--197},
pmid = {19323182},
title = {{Sample selection bias and presence-only distribution models : implications for background and pseudo-absence data Reference Sample selection bias and presence-only distribution models : implications for background and pseudo-absence data}},
volume = {19},
year = {2009}
}
@misc{kahneman2011a,
author = {Kahneman, D},
title = {{No Title}}
}
@misc{foerster2017b,
author = {Foerster, J and Nardelli, N and Farquhar, G and Torr, P and Kohli, P and Whiteson, S},
edition = {arXiv prep},
title = {{Stabilising experience replay for deep multi-agent reinforcement learning}}
}
@misc{ioffe2015a,
author = {Ioffe, S and Szegedy, C},
edition = {arXiv prep},
title = {{Batch normalization: Accelerating deep network training by reducing internal covariate shift}}
}
@incollection{thrun1992a,
author = {Thrun, S B},
booktitle = {Advances in Neural Information Processing Systems},
title = {{Efficient exploration in reinforcement learning}}
}
@inproceedings{bengio2009a,
author = {Bengio, Y and Louradour, J and Collobert, R and Weston, J},
booktitle = {Proceedings of the 26th annual international conference on machine learning. ACM},
pages = {41--48},
title = {{Curriculum learning}}
}
@article{samuel1959a,
author = {Samuel, A L},
journal = {IBM Journal of research and development},
number = {3},
pages = {210--229},
title = {{Some studies in machine learning using the game of checkers}},
volume = {3}
}
@incollection{gu2017b,
author = {Gu, S and Lillicrap, T and Ghahramani, Z and Turner, R E and Levine, S},
booktitle = {5th International Conference on Learning Representations},
edition = {arXiv prep},
publisher = {ICLR},
title = {{2016a. “Q-prop: Sample-efficient policy gradient with an off-policy critic”}}
}
@article{cortes1995a,
author = {Cortes, C and Vapnik, V},
journal = {Machine learning},
number = {3},
pages = {273--297},
title = {{Support-vector networks}},
volume = {20}
}
@article{levine2016a,
author = {Levine, S and Finn, C and Darrell, T and Abbeel, P},
journal = {Journal of Machine Learning Research},
number = {39},
pages = {1--40},
title = {{End-to-end training of deep visuomotor policies}},
volume = {17}
}
@misc{ng1999a,
author = {Ng, A Y and Harada, D and Russell, S},
editor = {I.C.M.L.},
pages = {278--287},
title = {{Policy invariance under reward transformations: Theory and application to reward shaping}},
volume = {99}
}
@misc{reed2015a,
author = {Reed, S and Freitas, N.De},
edition = {arXiv prep},
title = {{Neural programmer-interpreters}}
}
@misc{krizhevsky2012a,
author = {Krizhevsky, A and Sutskever, I and Hinton, G E},
title = {{“Imagenet classification with deep convolutional neural networks”. In: Advances in neural information processing systems. 1097–1105}}
}
@techreport{Petrik2014,
author = {Petrik, Marek and Subramanian, Dharmashankar},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Petrik, Subramanian - 2014 - RAAM The Benefits of Robustness in Approximating Aggregated MDPs in Reinforcement Learning.pdf:pdf},
pages = {1979--1987},
title = {{RAAM: The Benefits of Robustness in Approximating Aggregated MDPs in Reinforcement Learning}},
year = {2014}
}
@misc{bahdanau2016a,
author = {Bahdanau, D and Brakel, P and Xu, K and Goyal, A and Lowe, R and Pineau, J and Courville, A and Bengio, Y},
edition = {arXiv prep},
title = {{An actor-critic algorithm for sequence prediction}}
}
@misc{schulman2015a,
author = {Schulman, J and Levine, S and Abbeel, P and Jordan, M I and Moritz, P},
title = {{“Trust Region Policy Optimization”. In: ICML. 1889–1897}}
}
@misc{osband2016a,
author = {Osband, I and Blundell, C and Pritzel, A and Roy, B.Van},
edition = {arXiv prep},
title = {{Deep Exploration via Bootstrapped DQN}}
}
@misc{amodei2016a,
author = {Amodei, D and Olah, C and Steinhardt, J and Christiano, P and Schulman, J and Man{\'{e}}, D},
edition = {arXiv prep},
title = {{Concrete problems in AI safety}}
}
@book{Scott2015,
author = {Scott, DW},
title = {{Multivariate density estimation: theory, practice, and visualization}},
year = {2015}
}
@misc{zhang-a,
author = {{Zhang A.}, N Ballas and 2018a, J Pineau.},
edition = {arXiv prep},
title = {{A Dissection of Overfitting and Generalization in Continuous Reinforcement Learning}}
}
@article{Lydakis2018,
author = {Lydakis, Andreas and Allen, Jenica M. and Petrik, Marek and Szewczyk, Tim M.},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Lydakis et al. - 2018 - Robust strategies for managing invasive plants(3).pdf:pdf},
journal = {IJCAI Artificial Intelligence for Wildlife Conservation},
title = {{Robust strategies for managing invasive plants}},
year = {2018}
}
@incollection{Gordon1995,
author = {Gordon, Geoffrey J.},
booktitle = {Machine Learning Proceedings 1995},
doi = {10.1016/b978-1-55860-377-6.50040-2},
month = {jan},
pages = {261--268},
publisher = {Elsevier},
title = {{Stable Function Approximation in Dynamic Programming}},
year = {1995}
}
@article{Elith2006a,
abstract = {Prediction of species' distributions is central to diverse applications in ecology, evolution and conservation science. There is increasing electronic access to vast sets of occurrence records in museums and herbaria, yet little effective guidance on how best to use this information in the context of numerous approaches for modelling distributions. To meet this need, we compared 16 modelling methods over 226 species from 6 regions of the world, creating the most comprehensive set of model comparisons to date. We used presence-only data to fit models, and independent presence-absence data to evaluate the predictions. Along with well-established modelling methods such as generalised additive models and GARP and BIOCLIM, we explored methods that either have been developed recently or have rarely been applied to modelling species' distributions. These include machine-learning methods and community models, both of which have features that may make them particularly well suited to noisy or sparse information, as is typical of species' occurrence data. Presence-only data were effective for modelling species' distributions for many species and regions. The novel methods consistently outperformed more established methods. The results of our analysis are promising for the use of data from museums and herbaria, especially as methods suited to the noise inherent in such data improve.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Elith, J and Graham, C H and Anderson, R P and Dudik, M and Ferrier, S and Guisan, A and Hijmans, R J and Huettmann, F and Leathwick, J R and Lehmann, A and Li, J and Lohmann, L G and Loiselle, B A and Manion, G and Moritz, C and Nakamura, M and Nakazawa, Y and Overton, J M and Peterson, A T and Phillips, S J and Richardson, K and Scachetti-Pereira, R and Schapire, R E and Soberon, J and Williams, S and Wisz, M S and Zimmermann, N E},
doi = {10.1111/j.2006.0906-7590.04596.x},
eprint = {arXiv:1011.1669v3},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Elith et al. - 2006 - Novel methods improve prediction of species' distributions from occurrence data(3).pdf:pdf},
isbn = {1600-0587},
issn = {09067590},
journal = {Ecography},
keywords = {biodiversity,climate-change,conservation,distribution models,envelope models,habitat-suitability,logistic-regression,plant,potential distribution,spatial prediction},
number = {2},
pages = {129--151},
pmid = {1891},
title = {{Novel methods improve prediction of species' distributions from occurrence data}},
volume = {29},
year = {2006}
}
@misc{miikkulainen2017a,
author = {Miikkulainen, R and Liang, J and Meyerson, E and Rawal, A and Fink, D and Francon, O and Raju, B and Navruzyan, A and Duffy, N and Hodjat, B},
edition = {arXiv prep},
title = {{Evolving Deep Neural Networks}}
}
@article{lee2012a,
author = {Lee, D and Seo, H and Jung, M W},
journal = {Annual review of neuroscience},
pages = {287--308},
title = {{Neural basis of reinforcement learning and decision making}},
volume = {35}
}
@inproceedings{foerster2018a,
author = {Foerster, J and Chen, R Y and Al-Shedivat, M and Whiteson, S and Abbeel, P and Mordatch, I},
booktitle = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems. International Foundation for Autonomous Agents and Multiagent Systems},
pages = {122--130},
title = {{Learning with opponent-learning awareness}}
}
@article{srivastava2014a,
author = {Srivastava, N and Hinton, G E and Krizhevsky, A and Sutskever, I and Salakhutdinov, R},
journal = {Journal of Machine Learning Research},
number = {1},
pages = {1929--1958},
title = {{Dropout: a simple way to prevent neural networks from overfitting.}},
volume = {15}
}
@incollection{jakobi1995a,
author = {Jakobi, N and Husbands, P and Harvey, I},
booktitle = {European Conference on Artificial Life. Springer},
pages = {704--720},
title = {{Noise and the reality gap: The use of simulation in evolutionary robotics}}
}
@misc{higgins2017a,
author = {Higgins, I and Pal, A and Rusu, A A and Matthey, L and Burgess, C P and Pritzel, A and Botvinick, M and Blundell, C and Lerchner, A},
edition = {arXiv prep},
title = {{Darla: Improving zero-shot transfer in reinforcement learning}}
}
@misc{mcgovern1997a,
annote = {Vol. 1317},
author = {McGovern, A and Sutton, R S and Fagg, A H},
title = {{“Roles of macroactions in accelerating reinforcement learning”. In: Grace Hopper celebration of women in computing}}
}
@misc{zhang-b,
author = {Zhang, C and Vinyals, O and Munos, R and S.},
edition = {arXiv prep},
title = {{Bengio. 2018c. “A Study on Overfitting in Deep Reinforcement Learning”}}
}
@techreport{Abel,
abstract = {Conducting reinforcement-learning experiments can be a complex and timely process. A full experimental pipeline will typically consist of a simulation of an environment , an implementation of one or many learning algorithms, a variety of additional components designed to facilitate the agent-environment interplay, and any requisite analysis, plotting, and logging thereof. In light of this complexity, this paper introduces simple rl 1 , a new open source library for carrying out reinforcement learning experiments in Python 2 and 3 with a focus on simplicity. The goal of simple rl is to support seamless, reproducible methods for running reinforcement learning experiments. This paper gives an overview of the core design philosophy of the package, how it differs from existing libraries, and showcases its central features.},
annote = {generation of plots for RL algorithms},
author = {Abel, David},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Abel - Unknown - simple rl Reproducible Reinforcement Learning in Python(2).pdf:pdf},
title = {{simple rl: Reproducible Reinforcement Learning in Python}},
url = {https://github.com/keras-rl/keras-rl}
}
@article{Phillips2008,
abstract = {1 The overall functional capacity of the liver was evaluated using [35S]-bromosulphophthalein (BSP, 100 mg/kg, i.v.) in biliary fistulated adult rats pretreated orally with different doses of paracetamol (APAP) for varying time intervals. 2 The maximal hepatic damage occurred between 12-18 h after single doses of APAP (0.5 or 1 g/kg); hepatic excretory function returned to control levels by 48-72 hours. 3 Administration of either 0.5 or 1 g/kg APAP 18 h before BSP caused a dose-dependent inhibition of the choleretic effect of BSP and of the 60 min cumulative excretion of the dye, but conversely, produced a significant increase in the liver and plasma concentrations of 35S. 4 Following acute (0.25 g/kg), or subacute (0.5 g/kg, twice daily for 7 days) treatment with APAP, the total excretion of 35S in bile and the retention of 35S in the liver or plasma remained essentially the same as that for the controls. 5 In rats given single doses of 1 g/kg APAP, the hepatic uptake of the dye was significantly increased during the early stages of intoxication, while the opposite effect was observed at late periods. 6 The bile flow appeared to be inversely related to the excretion of unchanged BSP, and directly related to the excretion of the major BSP conjugate in bile. 7 The hepatic clearance of BSP was more rapid in rats treated subacutely with 0.5 or 1 g/kg APAP, than in those treated acutely with equal doses, suggesting that the intensity of APAP-induced hepatotoxicity became less severe after the repeated administration of this drug. 8 It is concluded that the hepatic uptake, metabolism and excretion of BSP are reversibly impaired following APAP-induced liver injury.},
title = {{No Title}}
}
@incollection{brown2017a,
author = {Brown, N and Sandholm, T},
booktitle = {International Joint Conference on Artificial Intelligence (IJCAI-17)},
title = {{Libratus: The Superhuman AI for No-Limit Poker}}
}
@article{halsey2015a,
author = {Halsey, L G and Curran-Everett, D and Vowler, S L and Drummond, G B},
journal = {Nature methods},
number = {3},
pages = {179--185},
title = {{The fickle P value generates irreproducible results}},
volume = {12}
}
@article{Serban2018,
archivePrefix = {arXiv},
arxivId = {1807.04723},
author = {Serban, Iulian Vlad and Sankar, Chinnadhurai and Pieper, Michael and Pineau, Joelle and Bengio, Yoshua},
eprint = {1807.04723},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Serban et al. - 2018 - The Bottleneck Simulator A Model-based Deep Reinforcement Learning Approach.pdf:pdf},
month = {jul},
title = {{The Bottleneck Simulator: A Model-based Deep Reinforcement Learning Approach}},
year = {2018}
}
@article{perez-liebana2016a,
author = {Perez-Liebana, D and Samothrakis, S and Togelius, J and Schaul, T and Lucas, S M and Cou{\"{e}}toux, A and Lee, J and Lim, C.-U. and Thompson, T},
journal = {IEEE Transactions on Computational Intelligence and AI in Games},
number = {3},
pages = {229--243},
title = {{The 2014 general video game playing competition}},
volume = {8}
}
@incollection{kroon2009a,
author = {Kroon, M and Whiteson, S},
booktitle = {Machine Learning and Applications, 2009. ICMLA'09. International Conference on. IEEE},
pages = {324--330},
title = {{Automatic feature selection for model-based reinforcement learning in factored MDPs}}
}
@article{papad,
author = {Johnson, D S and Papadimitrou, C H and Yannakakis, M},
journal = {Journal of Computer and System Sciences},
pages = {79--100},
title = {{How Easy is Local Search?}},
volume = {37},
year = {1988}
}
@incollection{montague2013a,
author = {Montague, P R},
pages = {271--277},
publisher = {Springer},
title = {{Reinforcement Learning Models Then-andNow: From Single Cells to Modern Neuroimaging}}
}
@misc{you2017a,
author = {You, Y and Pan, X and Wang, Z and Lu, C},
edition = {arXiv prep},
title = {{Virtual to Real Reinforcement Learning for Autonomous Driving}}
}
@misc{hadfield-menell2016a,
author = {Hadfield-Menell, D and Russell, S J and Abbeel, P and Dragan, A},
title = {{“Cooperative inverse reinforcement learning”. In: Advances in neural information processing systems. 3909–3917}}
}
@article{o2016a,
author = {O'Donoghue, B and Munos, R and Kavukcuoglu, K and V.},
journal = {Mnih},
title = {{No Title}}
}
@misc{vapnik1998a,
author = {Vapnik, V N},
title = {{Statistical learning theory. Adaptive and learning systems for signal processing, communications, and control}}
}
@inproceedings{schaul-a,
booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
pages = {1312--1320},
title = {{Universal value function approximators}},
volume = {15}
}
@article{lin1992a,
author = {Lin, L.-J.},
journal = {Machine learning},
number = {3-4},
pages = {293--321},
title = {{Self-improving reactive agents based on reinforcement learning, planning and teaching}},
volume = {8}
}
@misc{li2015a,
author = {Li, X and Li, L and Gao, J and He, X and Chen, J and Deng, L and He, J},
edition = {arXiv prep},
title = {{Recurrent reinforcement learning: a hybrid approach}}
}
@book{Bertsekas2019,
author = {Bertsekas, Dimitri P.},
isbn = {9781886529397},
pages = {373},
title = {{Reinforcement Learning and Optimal Control}},
year = {2019}
}
@misc{kansky2017a,
edition = {arXiv prep},
title = {{Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics}}
}
@misc{rusu2015a,
author = {Rusu, A A and Colmenarejo, S G and Gulcehre, C and Desjardins, G and Kirkpatrick, J and Pascanu, R and Mnih, V and Kavukcuoglu, K and Hadsell, R},
edition = {arXiv prep},
title = {{Policy distillation}}
}
@incollection{wender2012a,
author = {Wender, S and Watson, I},
booktitle = {Computational Intelligence and Games (CIG), 2012 IEEE Conference on. IEEE},
pages = {402--408},
title = {{Applying reinforcement learning to small scale combat in the real-time strategy game StarCraft: Broodwar}}
}
@misc{tessler2017a,
author = {Tessler, C and Givony, S and Zahavy, T and Mankowitz, D J and Mannor, S},
title = {{“A Deep Hierarchical Approach to Lifelong Learning in Minecraft.” In: AAAI. 1553–1561}}
}
@misc{goodfellow2014a,
author = {Goodfellow, I and Pouget-Abadie, J and Mirza, M and Xu, B and Warde-Farley, D and Ozair, S and Courville, A and Bengio, Y},
title = {{“Generative adversarial nets”. In: Advances in neural information processing systems. 2672– 2680}}
}
@article{russakovsky2015a,
author = {Russakovsky, O and Deng, J and Su, H and Krause, J and Satheesh, S and Ma, S and Huang, Z and Karpathy, A and Khosla, A and Bernstein, M},
journal = {International Journal of Computer Vision},
number = {3},
pages = {211--252},
title = {{Imagenet large scale visual recognition challenge}},
volume = {115}
}
@misc{jaques2018a,
author = {Jaques, N and Lazaridou, A and Hughes, E and Gulcehre, C and Ortega, P A and Strouse, D and Leibo, J Z and de Freitas, N},
edition = {arXiv prep},
title = {{Intrinsic Social Motivation via Causal Influence in Multi-Agent RL}}
}
@misc{machado-a,
author = {{Machado M. C.}, M G Bellemare and 2017a, M Bowling.},
edition = {arXiv prep},
title = {{A Laplacian Framework for Option Discovery in Reinforcement Learning}}
}
@misc{salimans2017a,
author = {Salimans, T and Ho, J and Chen, X and Sutskever, I},
edition = {arXiv prep},
title = {{Evolution Strategies as a Scalable Alternative to Reinforcement Learning}}
}
@article{stones,
author = {Stone, H S and Stone, J M},
journal = {IBM Journal of Research and Development},
pages = {464--474},
publisher = {International Business Machines.},
title = {{Efficient Search Techniques - An Empirical Study of the N-Queens Problem}},
volume = {31},
year = {1987}
}
@misc{fonteneau2008a,
author = {Fonteneau, R and Wehenkel, L and Ernst, D},
title = {{Variable selection for dynamic treatment regimes: a reinforcement learning approach}}
}
@book{ueno2017a,
author = {Ueno, S and Osawa, M and Imai, M and Kato, T and Yamakawa, H},
publisher = {Springer},
title = {{““Re: ROS”: Prototyping of Reinforcement Learning Environment for Asynchronous Cognitive Architecture”. In: First International Early Research Career Enhancement School on Biologically Inspired Cognitive Architectures}}
}
@misc{riedmiller2018a,
author = {Riedmiller, M and Hafner, R and Lampe, T and Neunert, M and Degrave, J and de Wiele, T.Van and Mnih, V and Heess, N and Springenberg, J T},
edition = {arXiv prep},
title = {{Learning by Playing - Solving Sparse Reward Tasks from Scratch}}
}
@misc{mathieu2015a,
author = {Mathieu, M and Couprie, C and LeCun, Y},
edition = {arXiv prep},
title = {{Deep multi-scale video prediction beyond mean square error}}
}
@article{whiteson2011a,
author = {Whiteson, S and Tanner, B and Taylor, M E and Stone, P},
journal = {IEEE},
pages = {120--127},
title = {{“Protecting against evaluation overfitting in empirical reinforcement learning”. In: Adaptive Dynamic Programming And Reinforcement Learning (ADPRL), 2011 IEEE Symposium on}}
}
@misc{kalakrishnan2013a,
author = {Kalakrishnan, M and Pastor, P and Righetti, L and Schaal, S},
title = {{“Learning objective functions for manipulation”. In: Robotics and Automation (ICRA), 2013 IEEE International Conference on. IEEE. 1331–1336}}
}
@inproceedings{hopfield,
author = {Hopfield, J J},
booktitle = {Proceedings of the National Academy of Sciences},
publisher = {Washington, DC: National Academy Press},
title = {{Neural Networks and Physical Systems with Emergent Collective Computational Abilities}},
volume = {79},
year = {1982}
}
@article{lecun2015a,
author = {LeCun, Y and Bengio, Y and Hinton, G},
journal = {Nature},
number = {7553},
pages = {436--444},
title = {{Deep learning}},
volume = {521}
}
@article{Papernot2016,
abstract = {Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the first practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and find that they are misclassified by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. We find that their DNN misclassifies 84.24{\%} of the adversarial examples crafted with our substitute. We demonstrate the general applicability of our strategy to many ML techniques by conducting the same attack against models hosted by Amazon and Google, using logistic regression substitutes. They yield adversarial examples misclassified by Amazon and Google at rates of 96.19{\%} and 88.94{\%}. We also find that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder.},
archivePrefix = {arXiv},
arxivId = {1602.02697},
author = {Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian and Jha, Somesh and Celik, Z. Berkay and Swami, Ananthram},
doi = {10.1145/3052973.3053009},
eprint = {1602.02697},
isbn = {978-1-4503-4944-4},
issn = {1998-4049},
journal = {ACM Asia Conference on Computer and Communications Security},
pmid = {21546725},
title = {{Practical Black-Box Attacks against Machine Learning}},
url = {http://arxiv.org/abs/1602.02697},
year = {2016}
}
@misc{lillicrap2015a,
author = {Lillicrap, T P and Hunt, J J and Pritzel, A and Heess, N and Erez, T and Tassa, Y and Silver, D and Wierstra, D},
edition = {arXiv prep},
title = {{Continuous control with deep reinforcement learning}}
}
@inproceedings{johnston,
author = {Johnston, M D and Adorf, H M},
booktitle = {Proceedings of NASA Conference on Space Telerobotics},
title = {{Learning in Stochastic Neural Networks for Constraint Satisfaction Problems}},
volume = {37},
year = {1989}
}
@book{christopher2006a,
author = {Christopher, M B},
publisher = {Springer},
title = {{Pattern recognition and machine learning}}
}
@misc{Devlin2018,
author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina and Google, Kristina Toutanova and Language, A I and Toutanova, Kristina},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Devlin et al. - 2018 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding(3).pdf:pdf},
month = {oct},
title = {{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}},
url = {http://arxiv.org/abs/1810.04805 https://github.com/tensorflow/tensor2tensor},
year = {2018}
}
@inproceedings{branavan2012a,
address = {Long},
author = {Branavan, S and Kushman, N and Lei, T and Barzilay, R},
booktitle = {Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics},
pages = {135},
publisher = {Papers-Volume 1. Association for Computational Linguistics. 126},
title = {{Learning high-level planning from text}}
}
@book{unknown-a,
pages = {305--320},
publisher = {Springer},
title = {{Q ($\lambda$) with Off-Policy Corrections”. In: International Conference on Algorithmic Learning Theory}}
}
@misc{hessel2018a,
author = {Hessel, M and Soyer, H and Espeholt, L and Czarnecki, W and Schmitt, S and van Hasselt, H},
edition = {arXiv prep},
title = {{Multi-task Deep Reinforcement Learning with PopArt}}
}
@misc{kirkpatrick2016a,
author = {Kirkpatrick, J and Pascanu, R and Rabinowitz, N and Veness, J and Desjardins, G and Rusu, A A and Milan, K and Quan, J and Ramalho, T and Grabska-Barwinska, A},
edition = {arXiv prep},
title = {{Overcoming catastrophic forgetting in neural networks}}
}
@article{kearns2002a,
author = {Kearns, M and Singh, S},
journal = {Machine Learning},
number = {2-3},
pages = {209--232},
title = {{Near-optimal reinforcement learning in polynomial time}},
volume = {49}
}
@article{amari1998a,
author = {Amari, S},
journal = {Neural Computation},
number = {2},
pages = {251--276},
title = {{Natural Gradient Works Efficiently in Learning}},
volume = {10}
}
@misc{oh2016a,
author = {Oh, J and Chockalingam, V and Singh, S and Lee, H},
edition = {arXiv prep},
title = {{Control of Memory, Active Perception, and Action in Minecraft}}
}
@misc{konda2000a,
author = {Konda, V R and Tsitsiklis, J N},
title = {{“Actor-critic algorithms”. In: Advances in neural information processing systems. 1008–1014}}
}
@misc{brundage2018a,
author = {Brundage, M and Avin, S and Clark, J and Toner, H and Eckersley, P and Garfinkel, B and Dafoe, A and Scharre, P and Zeitzoff, T and Filar, B},
edition = {arXiv prep},
title = {{The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation}}
}
@inproceedings{Zhong2011,
author = {Zhong, Mingyuan and Todorov, Emanuel},
booktitle = {IFAC Proceedings Volumes (IFAC-PapersOnline)},
doi = {10.3182/20110828-6-IT-1002.03729},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Zhong, Todorov - 2011 - Aggregation methods for lineary-solvable Markov decision process.pdf:pdf},
isbn = {9783902661937},
issn = {14746670},
month = {jan},
number = {1 PART 1},
pages = {11220--11225},
publisher = {IFAC Secretariat},
title = {{Aggregation methods for lineary-solvable Markov decision process}},
volume = {44},
year = {2011}
}
@article{Francois-Lavet2018,
annote = {version 2 published on arXiv},
archivePrefix = {arXiv},
arxivId = {1811.12560v2},
author = {Fran{\c{c}}ois-Lavet, Vincent and Henderson, Peter and Islam, Riashat and Bellemare, Marc G and Pineau, Joelle},
doi = {10.1561/2200000071},
eprint = {1811.12560v2},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Fran{\c{c}}ois-Lavet et al. - 2018 - An Introduction to Deep Reinforcement Learning(2).pdf:pdf},
title = {{An Introduction to Deep Reinforcement Learning}},
year = {2018}
}
@article{Kuzi,
abstract = {We present a suite of query expansion methods that are based on word embeddings. Using Word2Vec's CBOW embedding approach, applied over the entire corpus on which search is performed, we select terms that are semantically related to the query. Our methods either use the terms to expand the original query or integrate them with the effective pseudo-feedback-based relevance model. In the former case, retrieval performance is significantly better than that of using only the query, and in the latter case the performance is significantly better than that of the relevance model.},
author = {Kuzi, Saar and Shtok, Anna and Kurland, Oren},
doi = {10.1145/2983323.2983876},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Kuzi, Shtok, Kurland - Unknown - Query Expansion Using Word Embeddings(2).pdf:pdf},
isbn = {9781450340731},
title = {{Query Expansion Using Word Embeddings}},
url = {http://dx.doi.org/10.1145/2983323.2983876}
}
@misc{foerster2017a,
author = {Foerster, J and Farquhar, G and Afouras, T and Nardelli, N and Whiteson, S},
edition = {arXiv prep},
title = {{Counterfactual Multi-Agent Policy Gradients}}
}
@misc{juliani2018a,
author = {Juliani, A and Berges, V.-P. and Vckay, E and Gao, Y and Henry, H and Mattar, M and Lange, D},
edition = {arXiv prep},
title = {{Unity: A General Platform for Intelligent Agents}}
}
@incollection{ravindran2004a,
author = {Ravindran, B and Barto, A G},
title = {{“An algebraic approach to abstraction in reinforcement learning”. PhD thesis}},
url = {at Amherst.}
}
@techreport{Lin2017,
abstract = {We introduce two tactics, namely the strategically-timed attack and the enchanting attack, to attack reinforcement learning agents trained by deep reinforcement learning algorithms using adversarial examples. In the strategically-timed attack, the adversary aims at minimizing the agent's reward by only attacking the agent at a small subset of time steps in an episode. Limiting the attack activity to this subset helps prevent detection of the attack by the agent. We propose a novel method to determine when an adversarial example should be crafted and applied. In the enchanting attack, the adversary aims at luring the agent to a designated target state. This is achieved by combining a generative model and a planning algorithm: while the generative model predicts the future states, the planning algorithm generates a preferred sequence of actions for luring the agent. A sequence of ad-versarial examples is then crafted to lure the agent to take the preferred sequence of actions. We apply the proposed tactics to the agents trained by the state-of-the-art deep reinforcement learning algorithm including DQN and A3C. In 5 Atari games, our strategically-timed attack reduces as much reward as the uniform attack (i.e., attacking at every time step) does by attacking the agent 4 times less often. Our enchanting attack lures the agent toward designated target states with a more than 70{\%} success rate. Example videos are available at http: //yclin.me/adversarial{\_}attack{\_}RL/.},
author = {Lin, Yen-Chen and Hong, Zhang-Wei and Liao, Yuan-Hong and Shih, Meng-Li and Liu, Ming-Yu and Sun, Min},
keywords = {Machine Learning: Deep Learning,Machine Learning: New Problems,Machine Learning: Reinforcement Learning,Multidisciplinary Topics and Applications: AI{\&}Secu},
title = {{Tactics of Adversarial Attack on Deep Reinforcement Learning Agents}},
url = {https://www.ijcai.org/proceedings/2017/0525.pdf},
year = {2017}
}
@incollection{schraudolph1994a,
author = {Schraudolph, N N and Dayan, P and Sejnowski, T J},
booktitle = {Advances in Neural Information Processing Systems},
pages = {817--824},
title = {{Temporal difference learning of position evaluation in the game of Go}}
}
@techreport{Watkins1992,
abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states. This paper presents and proves in detail a convergence theorem for Q,-learning based on that outlined in Watkins (1989). We show that Q-learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where many Q values can be changed each iteration, rather than just one.},
author = {Watkins, Christopher J C H and Dayan, Peter},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Watkins, Dayan - 1992 - Technical Note Q,-Learning(3).pdf:pdf},
title = {{No Title}}
}
@inproceedings{simmons-aaai88,
author = {Simmons, R G},
booktitle = {Proceedings of AAAI-88},
title = {{A Theory of Debugging Plans and Interpretations}},
year = {1988}
}
@incollection{ghavamzadeh2015a,
author = {Ghavamzadeh, M and Mannor, S and Pineau, J and Tamar, A},
booktitle = {Foundations and Trends{\textregistered} in Machine Learning},
number = {5-6},
pages = {359--483},
title = {{Bayesian reinforcement learning: A survey}},
volume = {8}
}
@misc{bacon2016a,
author = {Bacon, P.-L. and Harb, J and Precup, D},
edition = {arXiv prep},
title = {{The option-critic architecture}}
}
@techreport{Popov,
abstract = {Deep learning and reinforcement learning methods have recently been used to solve a variety of problems in continuous control domains. An obvious application of these techniques is dexterous manipulation tasks in robotics which are difficult to solve using traditional control theory or hand-engineered approaches. One example of such a task is to grasp an object and precisely stack it on another. Solving this difficult and practically relevant problem in the real world is an important long-term goal for the field of robotics. Here we take a step towards this goal by examining the problem in simulation and providing models and techniques aimed at solving it. We introduce two extensions to the Deep Deterministic Policy Gradient algorithm (DDPG), a model-free Q-learning based method, which make it significantly more data-efficient and scalable. Our results show that by making extensive use of off-policy data and replay, it is possible to find control policies that robustly grasp objects and stack them. Further, our results hint that it may soon be feasible to train successful stacking policies by collecting interactions on real robots.},
author = {Popov, Ivaylo and Heess, Nicolas and Lillicrap, Timothy and Hafner, Roland and Barth-Maron, Gabriel and Vecerik, Matej and Lampe, Thomas and Tassa, Yuval and Erez, Tom and Deepmind, Martin Riedmiller},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Popov et al. - Unknown - Data-efficient Deep Reinforcement Learning for Dexterous Manipulation(3).pdf:pdf},
title = {{Data-efficient Deep Reinforcement Learning for Dexterous Manipulation}},
url = {https://pdfs.semanticscholar.org/e701/9d9786af58f56c5de7e7daa2ce9050ba60f0.pdf}
}
@article{sutton1999a,
author = {Sutton, R S and Precup, D and Singh, S},
journal = {Artificial},
number = {1-2},
pages = {181--211},
title = {{Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning}},
volume = {112}
}
@article{Dudik2004a,
abstract = {We consider the problem of estimating an unknown probability distribution from samples using the principle of maximum entropy (maxent). To alleviate overfitting with a very large number of features, we propose applying the maxent principle with relaxed constraints on the expectations of the features. By convex duality, this turns out to be equivalent to finding the Gibbs distribution minimizing a regularized version of the empirical log loss. We prove non-asymptotic bounds showing that, with respect to the true underlying distribution, this relaxed version of maxent produces density estimates that are almost as good as the best possible. These bounds are in terms of the deviation of the feature empirical averages relative to their true expectations, a number that can be bounded using standard uniform-convergence techniques. In particular, this leads to bounds that drop quickly with the number of samples, and that depend very moderately on the number or complexity of the features. We also derive and prove convergence for both sequential-update and parallel-update algorithms. Finally, we briefly describe experiments on data relevant to the modeling of species geographical distributions.},
author = {Dud{\'{i}}k, Miroslav and Phillips, Steven J. and Schapire, Robert E.},
doi = {10.1007/978-3-540-27819-1_33},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Dud{\'{i}}k, Phillips, Schapire - 2004 - Performance Guarantees for Regularized Maximum Entropy Density Estimation(3).pdf:pdf},
isbn = {0302-9743},
issn = {03029743},
pages = {472--486},
pmid = {6185},
title = {{Performance Guarantees for Regularized Maximum Entropy Density Estimation}},
url = {http://link.springer.com/10.1007/978-3-540-27819-1{\_}33},
year = {2004}
}
@misc{lample2017a,
author = {Lample, G and Chaplot, D S},
title = {{“Playing FPS Games with Deep Reinforcement Learning.” In: AAAI. 2140–2146}}
}
@misc{gu2016a,
author = {Gu, S and Lillicrap, T and Sutskever, I and Levine, S},
edition = {arXiv prep},
title = {{Continuous Deep Q-Learning with Model-based Acceleration}}
}
@article{liaw2002a,
author = {Liaw, A and Wiener, M},
journal = {R news},
number = {3},
pages = {18--22},
title = {{Classification and regression by randomForest}},
volume = {2}
}
@misc{fran2017a,
author = {Fran{\c{c}}ois-Lavet, V and Ernst, D and Raphael, F},
edition = {arXiv prep},
title = {{On overfitting and asymptotic bias in batch reinforcement learning with partial observability}}
}
@misc{haber2018a,
author = {Haber, N and Mrowca, D and Fei-Fei, L and Yamins, D L},
edition = {arXiv prep},
title = {{Learning to Play with Intrinsically-Motivated Self-Aware Agents}}
}
@article{beattie2016a,
edition = {arXiv prep},
journal = {Sadik, et al},
title = {{DeepMind Lab}}
}
@incollection{houthooft2016a,
author = {Houthooft, R and Chen, X and Duan, Y and Schulman, J and Turck, F.De and Abbeel, P},
booktitle = {Advances in Neural Information Processing Systems},
pages = {1109--1117},
title = {{Vime: Variational information maximizing exploration}}
}
@incollection{NIPS2019_8799,
author = {Zanette, Andrea and Lazaric, Alessandro and Kochenderfer, Mykel J and Brunskill, Emma},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {Wallach, H and Larochelle, H and Beygelzimer, A and d$\backslash$textquotesingle Alch{\'{e}}-Buc, F and Fox, E and Garnett, R},
pages = {5616--5625},
publisher = {Curran Associates, Inc.},
title = {{Limiting Extrapolation in Linear Approximate Value Iteration}},
url = {http://papers.nips.cc/paper/8799-limiting-extrapolation-in-linear-approximate-value-iteration.pdf},
year = {2019}
}
@article{Freedman1981,
author = {Freedman, David and Diaconis, Persi},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Freedman, Freedman, Diaconis - 1981 - On the Histogram as a Density Estimator L 2 Theory.pdf:pdf},
title = {{On the Histogram as a Density Estimator: L 2 Theory}},
year = {1981}
}
@techreport{HongLim2019,
abstract = {The robust Markov Decision Process (MDP) framework aims to address the problem of parameter uncertainty due to model mismatch, approximation errors or even adversarial behaviors. It is especially relevant when deploying the learned policies in real-world applications. Scaling up the robust MDP framework to large or continuous state space remains a challenging problem. The use of function approximation in this case is usually inevitable and this can only amplify the problem of model mismatch and parameter uncertainties. It has been previously shown that, in the case of MDPs with state aggregation, the robust policies enjoy a tighter performance bound compared to standard solutions due to its reduced sensitivity to approximation errors. We extend these results to the much larger class of kernel-based approximators and show, both analytically and empirically that the robust policies can significantly outperform the non-robust counterpart.},
author = {{Hong Lim}, Shiau and Autef, Arnaud},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Hong Lim, Autef - 2019 - Kernel-Based Reinforcement Learning in Robust Markov Decision Processes.pdf:pdf},
issn = {1938-7228},
month = {may},
pages = {3973--3981},
title = {{Kernel-Based Reinforcement Learning in Robust Markov Decision Processes}},
url = {http://proceedings.mlr.press/v97/lim19a.html},
year = {2019}
}
@incollection{rescorla1972a,
author = {Rescorla, R A and Wagner, A R},
booktitle = {Classical conditioning II: Current research and theory. 2},
pages = {64--99},
title = {{A theory of Pavlovian conditioning: Variations in the effectiveness of reinforcement and nonreinforcement}}
}
@misc{weber2017a,
author = {Weber, T and Racani{\`{e}}re, S and Reichert, D P and Buesing, L and Guez, A and Rezende, D J and Badia, A P and Vinyals, O and Heess, N and Li, Y},
edition = {arXiv prep},
title = {{Imagination-Augmented Agents for Deep Reinforcement Learning}}
}
@techreport{AlkaeeTaleghan2015a,
abstract = {In a simulator-defined MDP, the Markovian dynamics and rewards are provided in the form of a simulator from which samples can be drawn. This paper studies MDP planning algorithms that attempt to minimize the number of simulator calls before terminating and outputting a policy that is approximately optimal with high probability. The paper introduces two heuristics for efficient exploration and an improved confidence interval that enables earlier termination with probabilis-tic guarantees. We prove that the heuristics and the confidence interval are sound and produce with high probability an approximately optimal policy in polynomial time. Experiments on two benchmark problems and two instances of an invasive species management problem show that the improved confidence intervals and the new search heuristics yield reductions of between 8{\%} and 47{\%} in the number of simulator calls required to reach near-optimal policies.},
author = {{Alkaee Taleghan}, Majid and Dietterich, Thomas G and Crowley, Mark and Hall, Kim and {Jo Albers}, H and Auer, Peter and Hutter, Marcus and Orseau, Laurent},
booktitle = {Journal of Machine Learning Research},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Alkaee Taleghan et al. - 2015 - PAC Optimal MDP Planning with Application to Invasive Species Management(3).pdf:pdf},
keywords = {Good-Turing estimate,MDP planning,Markov decision processes,invasive species management,reinforcement learning},
pages = {3877--3903},
title = {{PAC Optimal MDP Planning with Application to Invasive Species Management *}},
url = {http://jmlr.org/papers/volume16/taleghan15a/taleghan15a.pdf},
volume = {16},
year = {2015}
}
@techreport{Bradtke1996,
abstract = {We introduce two new temporal difference (TD) algorithms based on the theory of linear least-squares function approximation. We define an algorithm we call Least-Squares TD (LS TD) for which we prove probability-one convergence when it is used with a function approximator linear in the adjustable parameters. We then define a recursive version of this algorithm, Recursive Least-Squares TD (RLS TD). Although these new TD algorithms require more computation per time-step than do Sutton's TD(A) algorithms, they are more efficient in a statistical sense because they extract more information from training experiences. We describe a simulation experiment showing the substantial improvement in learning rate achieved by RLS TD in an example Markov prediction problem. To quantify this improvement, we introduce the TD error variance of a Markov chain, arc,, and experimentally conclude that the convergence rate of a TD algorithm depends linearly on {\~{}}ro. In addition to converging more rapidly, LS TD and RLS TD do not have control parameters, such as a learning rate parameter, thus eliminating the possibility of achieving poor performance by an unlucky choice of parameters.},
annote = {LSTD paper},
author = {Bradtke, Steven J and Barto, Andrew G},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Bradtke, Barto - 1996 - Linear Least-Squares Algorithms for Temporal Difference Learning(3).pdf:pdf},
keywords = {Least-Squares,Markov Decision Problems,Reinforcement learning,Temporal Difference Methods},
pages = {33--57},
publisher = {Kluwer Academic Publishers},
title = {{Linear Least-Squares Algorithms for Temporal Difference Learning}},
url = {https://link.springer.com/content/pdf/10.1007{\%}2FBF00114723.pdf},
volume = {22},
year = {1996}
}
@article{vinyals2017a,
author = {Vinyals, O and Ewalds, T and Bartunov, S and Georgiev, P and Vezhnevets, A S and Yeo, M and Makhzani, A and K{\"{u}}ttler, H and Agapiou, J and J.},
edition = {arXiv prep},
journal = {Schrittwieser, et al},
title = {{StarCraft II: A New Challenge for Reinforcement Learning}}
}
@inproceedings{Lange2010,
abstract = {This paper discusses the effectiveness of deep auto-encoder neural networks in visual reinforcement learning (RL) tasks. We propose a framework for combining the training of deep auto-encoders (for learning compact feature spaces) with recently-proposed batch-mode RL algorithms (for learning policies). An emphasis is put on the data-efficiency of this combination and on studying the properties of the feature spaces automatically constructed by the deep auto-encoders. These feature spaces are empirically shown to adequately resemble existing similarities and spatial relations between observations and allow to learn useful policies. We propose several methods for improving the topology of the feature spaces making use of task-dependent information. Finally, we present first results on successfully learning good control policies directly on synthesized and real images.},
archivePrefix = {arXiv},
arxivId = {1507.04296},
author = {Lange, Sascha and Riedmiller, Martin},
booktitle = {Proceedings of the International Joint Conference on Neural Networks},
doi = {10.1109/IJCNN.2010.5596468},
eprint = {1507.04296},
isbn = {9781424469178},
issn = {1098-7576},
pmid = {25719670},
title = {{Deep auto-encoder neural networks in reinforcement learning}},
year = {2010}
}
@incollection{finn2016a,
author = {Finn, C and Goodfellow, I and Levine, S},
booktitle = {Advances In Neural Information Processing Systems},
pages = {64--72},
title = {{Unsupervised learning for physical interaction through video prediction}}
}
@misc{bellemare2017a,
author = {Bellemare, M G and Dabney, W and Munos, R},
edition = {arXiv prep},
title = {{A distributional perspective on reinforcement learning}}
}
@incollection{munos2016a,
author = {Munos, R and Stepleton, T and Harutyunyan, A and Bellemare, M},
booktitle = {Advances in Neural Information Processing Systems},
pages = {1046--1054},
title = {{Safe and efficient off-policy reinforcement learning}}
}
@misc{chiappa2017a,
author = {Chiappa, S and Racaniere, S and Wierstra, D and Mohamed, S},
edition = {arXiv prep},
title = {{Recurrent Environment Simulators}}
}
@misc{bruegmann1993a,
author = {Br{\"{u}}gmann, B},
title = {{Monte carlo go}},
type = {Tech. rep. Citeseer}
}
@article{Warton2010,
abstract = {Presence-only data, point locations where a species has been recorded as being present, are often used in modeling the distribution of a species as a function of a set of explanatory variables-whether to map species occurrence , to understand its association with the environment, or to predict its response to environmental change. Currently, ecologists most commonly analyze presence-only data by adding randomly chosen "pseudo-absences" to the data such that it can be analyzed using logistic regression, an approach which has weaknesses in model specification, in interpretation, and in implementation. To address these issues, we propose Poisson point process model-ing of the intensity of presences. We also derive a link between the proposed approach and logistic regression-specifically, we show that as the number of pseudo-absences increases (in a regular or uniform random arrangement), logistic regression slope parameters and their standard errors converge to those of the corresponding Poisson point process model. We discuss the practical implications of these results. In particular, point process modeling offers a framework for choice of the number and location of pseudo-absences, both of which are currently chosen by ad hoc and sometimes ineffective methods in ecology, a point which we illustrate by example.},
author = {Warton, David I and Shepherd, Leah C},
doi = {10.1214/10-AOAS331},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Warton, Shepherd - 2010 - POISSON POINT PROCESS MODELS SOLVE THE {\&}quotPSEUDO-ABSENCE PROBLEM{\&}quot FOR PRESENCE-ONLY DATA IN ECOLOGY 1.pdf:pdf},
journal = {The Annals of Applied Statistics},
keywords = {Habitat modeling,occurrence data,pseudo-absences,quadrature points,species distribution modeling},
number = {3},
pages = {1383--1402},
title = {{POISSON POINT PROCESS MODELS SOLVE THE "PSEUDO-ABSENCE PROBLEM" FOR PRESENCE-ONLY DATA IN ECOLOGY 1}},
url = {https://projecteuclid.org/download/pdfview{\_}1/euclid.aoas/1287409378},
volume = {4},
year = {2010}
}
@misc{walsh2017a,
author = {Walsh, T},
title = {{It's Alive!: Artificial Intelligence from the Logic Piano to Killer Robots}}
}
@misc{ng2000a,
author = {Ng, A Y and Russell, S J},
pages = {663--670},
title = {{“Algorithms for inverse reinforcement learning.” In: Icml}}
}
@article{geramifard2015a,
author = {Geramifard, A and Dann, C and Klein, R H and Dabney, W and How, J P},
journal = {Journal of Machine Learning Research},
pages = {1573--1578},
title = {{RLPy: A Value-Function-Based Reinforcement Learning Framework for Education and Research}},
volume = {16}
}
@book{hacker,
author = {Sussman, G J},
publisher = {New York: New American Elsevier},
title = {{A Computer Model of Skill Acquisition}},
year = {1975}
}
@inproceedings{dearden2011a,
author = {{Dearden R.}, N Friedman and 1998, S Russell.},
booktitle = {Proceedings of the 28th International Conference on machine learning},
pages = {465--472},
title = {{Bayesian Q-learning}},
volume = {11}
}
@incollection{olah2017a,
author = {Olah, C and Mordvintsev, A and Schubert, L},
title = {{Feature Visualization}},
url = {https://distill.pub/2017/feature-visualization.}
}
@inproceedings{Lavrenko2001,
address = {New York, New York, USA},
author = {Lavrenko, Victor and Croft, W. Bruce},
doi = {10.1145/383952.383972},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Lavrenko, Croft - 2001 - Relevance based language models(2).pdf:pdf},
isbn = {1581133316},
pages = {120--127},
publisher = {ACM Press},
title = {{Relevance based language models}},
url = {http://portal.acm.org/citation.cfm?doid=383952.383972},
year = {2001}
}
@misc{boularias2011a,
author = {Boularias, A and Kober, J and Peters, J},
pages = {182--189},
title = {{“Relative Entropy Inverse Reinforcement Learning.” In: AISTATS}}
}
@article{Pattanaik2018,
author = {Pattanaik, Anay and Tang, Zhenyi and Liu, Shuijing and Bommannan, Gautham and Chowdhary, Girish},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Pattanaik et al. - 2018 - Robust Deep Reinforcement Learning with Adversarial Attacks(2).pdf:pdf},
journal = {undefined},
title = {{Robust Deep Reinforcement Learning with Adversarial Attacks}},
url = {https://www.semanticscholar.org/paper/Robust-Deep-Reinforcement-Learning-with-Adversarial-Pattanaik-Tang/3b6c891fbccaa564ea4fd8914a5e3952fcf42ee3},
year = {2018}
}
@incollection{bennett2013a,
author = {Bennett, C C and Hauser, K},
booktitle = {Artificial intelligence in medicine},
number = {1},
pages = {9--19},
title = {{Artificial intelligence framework for simulating clinical decision-making: A Markov decision process approach}},
volume = {57}
}
@book{russek2017a,
author = {Russek, E M and Momennejad, I and Botvinick, M M and Gershman, S J and Daw, N D},
publisher = {bioRxiv: 083857},
title = {{Predictive representations can link model-based reinforcement learning to model-free mechanisms}}
}
@incollection{duan2016a,
author = {Duan, Y and Chen, X and Houthooft, R and Schulman, J and Abbeel, P},
booktitle = {International Conference on Machine Learning},
pages = {1329--1338},
title = {{Benchmarking deep reinforcement learning for continuous control}}
}
@techreport{Watkins1992,
abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states. This paper presents and proves in detail a convergence theorem for Q,-learning based on that outlined in Watkins (1989). We show that Q-learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where many Q values can be changed each iteration, rather than just one.},
author = {Watkins, Christopher J C H and Dayan, Peter},
title = {{No Title}}
}
@incollection{fran2016b,
author = {Fran{\c{c}}ois-Lavet, V and Taralla, D and Ernst, D and Fonteneau, R},
booktitle = {European Workshop on Reinforcement Learning},
title = {{Deep Reinforcement Learning Solutions for Energy Microgrids Management}}
}
@misc{bengio2015a,
author = {Bengio, Y and Lee, D.-H. and Bornschein, J and Mesnard, T and Lin, Z},
edition = {arXiv prep},
title = {{Towards biologically plausible deep learning}}
}
@article{james2003a,
author = {James, G M},
journal = {Machine Learning},
number = {2},
pages = {115--135},
title = {{Variance and bias for general loss functions}},
volume = {51}
}
@article{rumelhart1988a,
author = {Rumelhart, D E and Hinton, G E and Williams, R J},
journal = {Cognitive modeling},
number = {3},
pages = {1},
title = {{Learning representations by back-propagating errors}},
volume = {5}
}
@inproceedings{gal2016a,
address = {2016, New York City, NY, USA},
author = {Gal, Y and Ghahramani, Z},
booktitle = {Proceedings of the 33nd International Conference on Machine Learning, ICML},
pages = {1050},
title = {{Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning}}
}
@incollection{nagabandi2018a,
author = {Nagabandi, A and Kahn, G and Fearing, R S and Levine, S},
booktitle = {2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE},
pages = {7559--7566},
title = {{Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning}}
}
@article{And2018,
author = {Lydakis, Andreas},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Lydakis - 2018 - Optimization of Control Measures Against Invasive Species BY Andreas Lydakis PROJECT Submitted to the University of (3).pdf:pdf},
title = {{Optimization of Control Measures Against Invasive Species BY Andreas Lydakis PROJECT Submitted to the University of New Hampshire In Partial Fulfilment of Master of Science in Computer Science February , 2018 Dr Marek Petik ( supervisor )}},
year = {2018}
}
@misc{duan2017a,
author = {Duan, Y and Andrychowicz, M and Stadie, B and Ho, J and Schneider, J and Sutskever, I and Abbeel, P and Zaremba, W},
edition = {arXiv prep},
title = {{One-Shot Imitation Learning}}
}
@misc{warnell2017a,
author = {Warnell, G and Waytowich, N and Lawhern, V and Stone, P},
edition = {arXiv prep},
title = {{Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces}}
}
@misc{stadie2015a,
author = {Stadie, B C and Levine, S and Abbeel, P},
edition = {arXiv prep},
title = {{Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models}}
}
@misc{dabney2017a,
author = {Dabney, W and Rowland, M and Bellemare, M G and Munos, R},
edition = {arXiv prep},
title = {{Distributional Reinforcement Learning with Quantile Regression}}
}
@article{Fithian2013,
abstract = {Statistical modeling of presence-only data has attracted much recent attention in the ecological literature, leading to a proliferation of methods, including the inhomogeneous Poisson process (IPP) model, maximum entropy (Maxent) modeling of species distributions and logistic regression models. Several recent articles have shown the close relationships between these methods. We explain why the IPP intensity function is a more natural object of inference in presence-only studies than occurrence probability (which is only defined with reference to quadrat size), and why presence-only data only allows estimation of relative, and not absolute intensity of species occurrence. All three of the above techniques amount to parametric density estimation under the same exponential family model (in the case of the IPP, the fitted density is multiplied by the number of presence records to obtain a fitted intensity). We show that IPP and Maxent give the exact same estimate for this density, but logistic regression in general yields a different estimate in finite samples. When the model is misspecified - as it practically always is - logistic regression and the IPP may have substantially different asymptotic limits with large data sets. We propose ``infinitely weighted logistic regression,'' which is exactly equivalent to the IPP in finite samples. Consequently, many already-implemented methods extending logistic regression can also extend the Maxent and IPP models in directly analogous ways using this technique.},
archivePrefix = {arXiv},
arxivId = {1207.6950},
author = {Fithian, William and Hastie, Trevor},
doi = {10.1214/13-AOAS667},
eprint = {1207.6950},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Fithian, Hastie - 2013 - Finite-sample equivalence in statistical models for presence-only data(3).pdf:pdf},
isbn = {1932-6157},
issn = {19326157},
journal = {Annals of Applied Statistics},
keywords = {Case-control sampling,Logistic regression,Maximum entropy,Poisson process models,Presence-only data,Species modeling},
number = {4},
pages = {1917--1939},
pmid = {25493106},
title = {{Finite-sample equivalence in statistical models for presence-only data}},
volume = {7},
year = {2013}
}
@article{szegedy2016a,
author = {Szegedy, C and Ioffe, S and Vanhoucke, V and Alemi, A A},
edition = {arXiv prep},
journal = {In: AAAI},
number = {12},
title = {{Inception-v4, inception-resnet and the impact of residual connections on learning}},
volume = {4}
}
@article{Douglas1993,
author = {Douglas, Craig C. and Douglas, Jim},
doi = {10.1137/0730007},
issn = {00361429},
journal = {SIAM Journal on Numerical Analysis},
month = {jul},
number = {1},
pages = {136--158},
publisher = {Society for Industrial and Applied Mathematics},
title = {{Unified convergence theory for abstract multigrid or multilevel algorithms, serial and parallel}},
volume = {30},
year = {1993}
}
@misc{neu2012a,
author = {Neu, G and Szepesv{\'{a}}ri, C},
edition = {arXiv prep},
title = {{Apprenticeship learning using inverse reinforcement learning and gradient methods}}
}
@book{bouckaert2004a,
address = {In},
author = {Bouckaert, R R and Frank, E},
pages = {3--12},
publisher = {PAKDD. Springer},
title = {{Evaluating the replicability of significance tests for comparing learning algorithms}}
}
@inproceedings{he2016a,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
pages = {770--778},
title = {{Deep residual learning for image recognition}},
year = {2016}
}
@misc{watter2015a,
author = {Watter, M and Springenberg, J and Boedecker, J and Riedmiller, M},
title = {{“Embed to control: A locally linear latent dynamics model for control from raw images”. In: Advances in neural information processing systems. 2746–2754}}
}
@incollection{islam2017a,
author = {Islam, R and Henderson, P and Gomrokchi, M and Precup, D},
booktitle = {ICML Reproducibility in Machine Learning Workshop},
title = {{Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control}}
}
@inproceedings{langley,
author = {Langley, P},
booktitle = {Proceedings of AAAI-92},
title = {{Systematic and Nonsystematic Search Strategies}},
year = {1992}
}
@incollection{dietterich2009a,
author = {Dietterich, T G},
booktitle = {Asian Conference on Machine Learning. Springer},
pages = {1--5},
title = {{Machine learning and ecosystem informatics: challenges and opportunities}}
}
@article{hochreiter1997a,
author = {Hochreiter, S and Schmidhuber, J},
journal = {Neural},
number = {8},
pages = {1735--1780},
title = {{Long short-term memory}},
volume = {9}
}
@article{jaquette1973a,
author = {Jaquette, S C},
journal = {The Annals of Statistics},
number = {3},
pages = {496--505},
title = {{Markov decision processes with a new optimality criterion: Discrete time}},
volume = {1}
}
@misc{hessel2017a,
author = {Hessel, M and Modayil, J and van Hasselt, H and Schaul, T and Ostrovski, G and Dabney, W and Horgan, D and Piot, B and Azar, M and Silver, D},
edition = {arXiv prep},
title = {{Rainbow: Combining Improvements in Deep Reinforcement Learning}}
}
@article{peng2017b,
author = {Peng, X B and Berseth, G and Yin, K and van de Panne, M},
journal = {ACM Transactions on Graphics},
number = {4},
publisher = {Proc. SIGGRAPH},
title = {{DeepLoco: Dynamic Locomotion Skills Using Hierarchical Deep Reinforcement Learning}},
volume = {36}
}
@incollection{xu2015a,
author = {Xu, K and Ba, J and Kiros, R and Cho, K and Courville, A and Salakhudinov, R and Zemel, R and Bengio, Y},
booktitle = {International Conference on Machine Learning},
pages = {2048--2057},
title = {{Show, attend and tell: Neural image caption generation with visual attention}}
}
@incollection{ernst2005a,
author = {Ernst, D and Geurts, P and Wehenkel, L},
booktitle = {Journal of Machine Learning Research},
pages = {503--556},
title = {{Tree-based batch mode reinforcement learning}}
}
@incollection{heess2015a,
author = {Heess, N and Wayne, G and Silver, D and Lillicrap, T and Erez, T and Tassa, Y},
booktitle = {Advances in Neural Information Processing Systems},
pages = {2944--2952},
title = {{Learning continuous control policies by stochastic value gradients}}
}
@misc{liu2017a,
author = {Liu, Y and Gupta, A and Abbeel, P and Levine, S},
edition = {arXiv prep},
title = {{Imitation from Observation: Learning to Imitate Behaviors from Raw Video via Context Translation}}
}
@article{Phillips2004,
abstract = {We study the problem of modeling species geographic distributions, a critical problem in conservation biology. We propose the use of maximum-entropy techniques for this problem, specifically, sequential-update algorithms that can handle a very large number of features. We describe experiments comparing maxent with a standard distribution-modeling tool, called GARP, on a dataset containing observation data for North American breeding birds. We also study how well maxent performs as a function of the number of training examples and training time, analyze the use of regularization to avoid overfitting when the number of examples is small, and explore the interpretability of models constructed using maxent.},
author = {Phillips, Steven J and Dud{\'{i}}k, M and Schapire, R E},
doi = {10.1145/1015330.1015412},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Phillips, Dud{\'{i}}k, Schapire - 2004 - A maximum entropy approach to species distribution modeling(3).pdf:pdf},
isbn = {1581138285},
issn = {00147672},
journal = {Twentyfirst international conference on Machine learning ICML 04},
pages = {83},
pmid = {6379},
title = {{A maximum entropy approach to species distribution modeling}},
url = {http://portal.acm.org/citation.cfm?doid=1015330.1015412},
volume = {69},
year = {2004}
}
@misc{barto1983a,
author = {Barto, A G and Sutton, R S and Anderson, C W},
pages = {834--846},
title = {{“Neuronlike adaptive elements that can solve difficult learning control problems”. IEEE transactions on systems, man, and cybernetics}}
}
@article{dhariwal2017a,
author = {Dhariwal, P and Hesse, C and Plappert, M and Radford, A and Schulman, J and Sidor, S and Wu, Y},
journal = {Neural computation},
number = {7},
pages = {1895--1923},
title = {{OpenAI Baselines}},
volume = {10}
}
@book{sutton1998a,
author = {Sutton, R S and Barto, A G},
number = {1},
publisher = {MIT press Cambridge},
title = {{Reinforcement learning: An introduction}},
volume = {1}
}
@misc{haarnoja2017a,
author = {Haarnoja, T and Tang, H and Abbeel, P and Levine, S},
edition = {arXiv prep},
title = {{Reinforcement learning with deep energy-based policies}}
}
@article{story2014a,
author = {Story, G and Vlaev, I and Seymour, B and Darzi, A and Dolan, R},
journal = {Frontiers in behavioral neuroscience},
pages = {76},
title = {{Does temporal discounting explain unhealthy behavior? A systematic review and reinforcement learning perspective}},
volume = {8}
}
@misc{neelakantan2015a,
author = {Neelakantan, A and Le, Q V and Sutskever, I},
edition = {arXiv prep},
title = {{Neural programmer: Inducing latent programs with gradient descent}}
}
@book{stone2000a,
author = {Stone, P and Veloso, M},
pages = {369--381},
publisher = {Machine Learning: ECML},
title = {{Layered learning}}
}
@article{Roy2016,
abstract = {In this paper a framework for Automatic Query Expansion (AQE) is proposed using distributed neural language model word2vec. Using semantic and contextual relation in a distributed and unsupervised framework, word2vec learns a low dimensional embedding for each vocabulary entry. Using such a framework, we devise a query expansion technique, where related terms to a query are obtained by K-nearest neighbor approach. We explore the performance of the AQE methods, with and without feedback query expansion, and a variant of simple K-nearest neighbor in the proposed framework. Experiments on standard TREC ad-hoc data (Disk 4, 5 with query sets 301-450, 601-700) and web data (WT10G data with query set 451-550) shows significant improvement over standard term-overlapping based retrieval methods. However the proposed method fails to achieve comparable performance with statistical co-occurrence based feedback method such as RM3. We have also found that the word2vec based query expansion methods perform similarly with and without any feedback information.},
archivePrefix = {arXiv},
arxivId = {1606.07608},
author = {Roy, Dwaipayan and Paul, Debjyoti and Mitra, Mandar and Garain, Utpal},
eprint = {1606.07608},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Roy et al. - 2016 - Using Word Embeddings for Automatic Query Expansion(2).pdf:pdf},
month = {jun},
title = {{Using Word Embeddings for Automatic Query Expansion}},
url = {http://arxiv.org/abs/1606.07608},
year = {2016}
}
@misc{kaplan2017a,
author = {Kaplan, R and Sauer, C and Sosa, A},
edition = {arXiv prep},
title = {{Beating Atari with Natural Language Guided Reinforcement Learning}}
}
@techreport{Bellemare2017,
abstract = {In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman's equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.},
annote = {From Duplicate 1 (A Distributional Perspective on Reinforcement Learning - Bellemare, Marc G; Dabney, Will; Munos, R{\'{e}}mi)

From Duplicate 1 (A Distributional Perspective on Reinforcement Learning - Bellemare, Marc G; Dabney, Will; Munos, R{\'{e}}mi)

Distributional Bellman Operator


From Duplicate 1 (A Distributional Perspective on Reinforcement Learning - Bellemare, Marc G; Dabney, Will; Munos, R{\'{e}}mi)

Distributional Bellman Operator},
archivePrefix = {arXiv},
arxivId = {1707.06887v1},
author = {Bellemare, Marc G and Dabney, Will and Munos, R{\'{e}}mi},
eprint = {1707.06887v1},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Bellemare, Dabney, Munos - 2017 - A Distributional Perspective on Reinforcement Learning(5).pdf:pdf;:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Bellemare, Dabney, Munos - 2017 - A Distributional Perspective on Reinforcement Learning(6).pdf:pdf},
title = {{A Distributional Perspective on Reinforcement Learning}},
url = {https://arxiv.org/pdf/1707.06887.pdf http://proceedings.mlr.press/v70/bellemare17a/bellemare17a.pdf},
year = {2017}
}
@incollection{mankowitz2016a,
author = {Mankowitz, D J and Mann, T A and Mannor, S},
booktitle = {Advances in Neural Information Processing Systems},
pages = {1588--1596},
title = {{Adaptive Skills Adaptive Partitions (ASAP)}}
}
@incollection{tamar2016a,
author = {Tamar, A and Levine, S and Abbeel, P and WU, Y and Thomas, G},
booktitle = {Advances in Neural Information Processing Systems},
pages = {2146--2154},
title = {{Value iteration networks}}
}
@article{Warton2010,
abstract = {Presence-only data, point locations where a species has been recorded as being present, are often used in modeling the distribution of a species as a function of a set of explanatory variables-whether to map species occurrence , to understand its association with the environment, or to predict its response to environmental change. Currently, ecologists most commonly analyze presence-only data by adding randomly chosen "pseudo-absences" to the data such that it can be analyzed using logistic regression, an approach which has weaknesses in model specification, in interpretation, and in implementation. To address these issues, we propose Poisson point process model-ing of the intensity of presences. We also derive a link between the proposed approach and logistic regression-specifically, we show that as the number of pseudo-absences increases (in a regular or uniform random arrangement), logistic regression slope parameters and their standard errors converge to those of the corresponding Poisson point process model. We discuss the practical implications of these results. In particular, point process modeling offers a framework for choice of the number and location of pseudo-absences, both of which are currently chosen by ad hoc and sometimes ineffective methods in ecology, a point which we illustrate by example.},
author = {Warton, David I and Shepherd, Leah C},
doi = {10.1214/10-AOAS331},
journal = {The Annals of Applied Statistics},
keywords = {Habitat modeling,occurrence data,pseudo-absences,quadrature points,species distribution modeling},
number = {3},
pages = {1383--1402},
title = {{POISSON POINT PROCESS MODELS SOLVE THE "PSEUDO-ABSENCE PROBLEM" FOR PRESENCE-ONLY DATA IN ECOLOGY 1}},
url = {https://projecteuclid.org/download/pdfview{\_}1/euclid.aoas/1287409378},
volume = {4},
year = {2010}
}
@misc{chebotar2018a,
author = {Chebotar, Y and Handa, A and Makoviychuk, V and Macklin, M and Issac, J and Ratliff, N and Fox, D},
edition = {arXiv prep},
title = {{Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience}}
}
@misc{paine2018a,
author = {Paine, T L and Colmenarejo, S G and Wang, Z and Reed, S and Aytar, Y and Pfaff, T and Hoffman, M W and Barth-Maron, G and Cabi, S and Budden, D},
edition = {arXiv prep},
title = {{One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL}}
}
@misc{real2017a,
author = {Real, E and Moore, S and Selle, A and Saxena, S and Suematsu, Y L and Le, Q and Kurakin, A},
edition = {arXiv prep},
title = {{Large-Scale Evolution of Image Classifiers}}
}
@book{niv2009b,
address = {In},
author = {Niv, Y and Montague, P R},
pages = {331--351},
publisher = {Neuroeconomics. Elsevier},
title = {{Theoretical and empirical studies of learning}}
}
@misc{gordon1996a,
author = {Gordon, G J},
title = {{“Stable fitted reinforcement learning”. In: Advances in neural information processing systems. 1052–1058}}
}
@article{turing1953a,
author = {Turing, A M},
journal = {Faster than thought},
title = {{Digital computers applied to games}}
}
@misc{garnelo2016a,
author = {Garnelo, M and Arulkumaran, K and Shanahan, M},
edition = {arXiv prep},
title = {{Towards Deep Symbolic Reinforcement Learning}}
}
@article{singh2000a,
author = {Singh, S and Jaakkola, T and Littman, M L and Szepesv{\'{a}}ri, C},
journal = {Machine learning},
number = {3},
pages = {287--308},
title = {{Convergence results for single-step on-policy reinforcement-learning algorithms}},
volume = {38}
}
@incollection{thomas2016a,
author = {Thomas, P S and Brunskill, E},
booktitle = {International Conference on Machine Learning},
title = {{Data-efficient off-policy policy evaluation for reinforcement learning}}
}
@techreport{Dietz,
abstract = {This presentation will provide an overview of the challenges of high power proton accelerators such as SNS, J-PARC, etc., and what we have learned from recent experiences. Beam loss mechanisms and methods to mitigate beam loss will also be discussed.},
author = {Dietz, Laura and Verma, Manisha and Radlinski, Filip and Craswell, Nick},
booktitle = {Trec},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Dietz et al. - 2017 - TREC Complex Answer Retrieval Overview.pdf:pdf},
pages = {1--13},
title = {{TREC Complex Answer Retrieval Overview}},
url = {http://trec-car.cs.unh.edu},
year = {2017}
}
@misc{duan-a,
author = {Duan, Y and Schulman, J and Chen, X and Bartlett, P L and Sutskever, I and P.},
edition = {arXiv prep},
title = {{Abbeel. 2016b. “RL2: Fast Reinforcement Learning via Slow Reinforcement Learning”}}
}
@article{Kos2017,
abstract = {Adversarial examples have been shown to exist for a variety of deep learning architectures. Deep reinforcement learning has shown promising results on training agent policies directly on raw inputs such as image pixels. In this paper we present a novel study into adversarial attacks on deep reinforcement learning polices. We compare the effectiveness of the attacks using adversarial examples vs. random noise. We present a novel method for reducing the number of times adversarial examples need to be injected for a successful attack, based on the value function. We further explore how re-training on random noise and FGSM perturbations affects the resilience against adversarial examples.},
archivePrefix = {arXiv},
arxivId = {1705.06452},
author = {Kos, Jernej and Song, Dawn Xiaodong},
eprint = {1705.06452},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Kos, Song - 2017 - Delving into adversarial attacks on deep policies(2).pdf:pdf},
journal = {undefined},
month = {may},
title = {{Delving into adversarial attacks on deep policies}},
url = {http://arxiv.org/abs/1705.06452 https://www.semanticscholar.org/paper/Delving-into-adversarial-attacks-on-deep-policies-Kos-Song/cf8ed2793bc6aec88da5306fe2de560dc0be9b15},
year = {2017}
}
@book{sutton2017a,
author = {Sutton, R S and Barto, A G},
edition = {2},
publisher = {MIT Press},
title = {{Reinforcement Learning: An Introductionin progress)}}
}
@incollection{vezhnevets2016a,
author = {Vezhnevets, A and Mnih, V and Osindero, S and Graves, A and Vinyals, O and Agapiou, J},
booktitle = {Advances in Neural Information Processing Systems},
pages = {3486--3494},
title = {{Strategic attentive writer for learning macro-actions}}
}
@misc{mirowski2016a,
author = {Mirowski, P and Pascanu, R and Viola, F and Soyer, H and Ballard, A and Banino, A and Denil, M and Goroshin, R and Sifre, L and Kavukcuoglu, K},
edition = {arXiv prep},
title = {{Learning to navigate in complex environments}}
}
@inproceedings{min-aaai,
author = {Minton, S and Johnston, M and Philips, A B and Laird, P},
booktitle = {Proceedings of AAAI-90},
title = {{Solving Large Scale Constraint Satisfaction and Scheduling Problems Using a Heuristic Repair Method}},
year = {1990}
}
@article{singh1996a,
author = {Singh, S P and Sutton, R S},
journal = {Machine learning},
number = {1-3},
pages = {123--158},
title = {{Reinforcement learning with replacing eligibility traces}},
volume = {22}
}
@inproceedings{ginsberg,
author = {Ginsberg, M L and Harvey, W D},
booktitle = {Proceedings of AAAI-91},
title = {{Iterative Broadening}},
year = {1990}
}
@misc{savinov2018a,
author = {Savinov, N and Raichuk, A and Marinier, R and Vincent, D and Pollefeys, M and Lillicrap, T and Gelly, S},
edition = {arXiv prep},
title = {{Episodic Curiosity through Reachability}}
}
@article{Szegedy2014,
author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian J. and Fergus, Rob},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Szegedy et al. - 2014 - Intriguing properties of neural networks(2).pdf:pdf},
journal = {undefined},
title = {{Intriguing properties of neural networks}},
url = {https://www.semanticscholar.org/paper/Intriguing-properties-of-neural-networks-Szegedy-Zaremba/83bfdd6a2b28106b9fb66e52832c45f08b828541},
year = {2014}
}
@article{Nogueira2019,
abstract = {Recently, neural models pretrained on a language modeling task, such as ELMo (Peters et al., 2017), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et al., 2018), have achieved impressive results on various natural language processing tasks such as question-answering and natural language inference. In this paper, we describe a simple re-implementation of BERT for query-based passage re-ranking. Our system is the state of the art on the TREC-CAR dataset and the top entry in the leaderboard of the MS MARCO passage retrieval task, outperforming the previous state of the art by 27{\%} (relative) in MRR@10. The code to reproduce our results is available at https://github.com/nyu-dl/dl4marco-bert},
archivePrefix = {arXiv},
arxivId = {1901.04085},
author = {Nogueira, Rodrigo and Cho, Kyunghyun},
eprint = {1901.04085},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Nogueira, Cho - 2019 - Passage Re-ranking with BERT(4).pdf:pdf},
month = {jan},
title = {{Passage Re-ranking with BERT}},
url = {http://arxiv.org/abs/1901.04085},
year = {2019}
}
@article{Goodfellow2014,
archivePrefix = {arXiv},
arxivId = {1412.6572},
author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
eprint = {1412.6572},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Goodfellow, Shlens, Szegedy - 2014 - Explaining and Harnessing Adversarial Examples(2).pdf:pdf},
month = {dec},
title = {{Explaining and Harnessing Adversarial Examples}},
url = {http://arxiv.org/abs/1412.6572},
year = {2014}
}
@book{schulman2016a,
address = {In},
author = {Schulman, J and Ho, J and Lee, C and Abbeel, P},
pages = {339--354},
publisher = {Robotics Research. Springer},
title = {{Learning from demonstrations through the use of non-rigid registration}}
}
@incollection{browne2012a,
author = {Browne, C B and Powley, E and Whitehouse, D and Lucas, S M and Cowling, P I and Rohlfshagen, P and Tavener, S and Perez, D and Samothrakis, S and Colton, S},
booktitle = {IEEE Transactions on Computational Intelligence and AI in games},
number = {1},
pages = {1--43},
title = {{A survey of monte carlo tree search methods}},
volume = {4}
}
@misc{rusu2016a,
edition = {arXiv prep},
title = {{Sim-to-real robot learning from pixels with progressive nets}}
}
@book{precup2000a,
author = {Precup, D},
publisher = {Computer Science Department Faculty Publication Series: 80},
title = {{Eligibility traces for off-policy policy evaluation}}
}
@incollection{levine2013a,
author = {Levine, S and Koltun, V},
booktitle = {International Conference on Machine Learning},
pages = {1--9},
title = {{Guided policy search}}
}
@article{Bean1987,
author = {Bean, James C and Birge, John R and Smith, Robert L},
issn = {0030364X, 15265463},
journal = {Operations Research},
month = {jun},
number = {2},
pages = {215--220},
publisher = {INFORMS},
title = {{Aggregation in Dynamic Programming}},
volume = {35},
year = {1987}
}
@incollection{unknown2001a,
address = {Columbia, Canada},
booktitle = {Information Processing Systems 14 [Neural Information Processing Systems: Natural and Synthetic, NIPS},
pages = {1531--1538},
publisher = {Vancouver, British},
title = {{No Title}}
}
@article{Narasimhan2016,
archivePrefix = {arXiv},
arxivId = {1603.07954},
author = {Narasimhan, Karthik and Yala, Adam and Barzilay, Regina},
eprint = {1603.07954},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Narasimhan, Yala, Barzilay - 2016 - Improving Information Extraction by Acquiring External Evidence with Reinforcement Learning(2).pdf:pdf},
month = {mar},
title = {{Improving Information Extraction by Acquiring External Evidence with Reinforcement Learning}},
url = {http://arxiv.org/abs/1603.07954},
year = {2016}
}
@inproceedings{dearden1999a,
author = {Dearden, R and Friedman, N and Andre, D},
booktitle = {Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence},
pages = {150--159},
publisher = {Morgan Kaufmann Publishers Inc},
title = {{Model based Bayesian exploration}}
}
@misc{watkins1989a,
address = {King's College, Cambridge},
author = {Watkins, C.J.C.H.},
title = {{“Learning from delayed rewards”. PhD thesis}}
}
@misc{schulman2017b,
author = {Schulman, J and Wolski, F and Dhariwal, P and Radford, A and Klimov, O},
edition = {arXiv prep},
title = {{Proximal policy optimization algorithms}}
}
@misc{peng2017a,
author = {Peng, P and Yuan, Q and Wen, Y and Yang, Y and Tang, Z and Long, H and Wang, J},
edition = {arXiv prep},
title = {{Multiagent Bidirectionally-Coordinated Nets for Learning to Play StarCraft Combat Games}}
}
@misc{bengio2017a,
author = {Bengio, Y},
edition = {arXiv prep},
title = {{The Consciousness Prior}}
}
@incollection{bertsekas1995a,
address = {Belmont, MA},
author = {Bertsekas, D P and Bertsekas, D P and Bertsekas, D P and Bertsekas, D P},
number = {2},
title = {{Dynamic programming and optimal control}},
volume = {1}
}
@inproceedings{littman1994a,
author = {Littman, M L},
booktitle = {Proceedings of the eleventh international conference on machine learning},
pages = {157--163},
title = {{Markov games as a framework for multi-agent reinforcement learning}},
volume = {157}
}
@misc{fox2015a,
author = {Fox, R and Pakman, A and Tishby, N},
edition = {arXiv prep},
title = {{Taming the noise in reinforcement learning via soft updates}}
}
@article{Ward2009,
author = {Ward, Gill and Hastie, Trevor and Barry, Simon and Elith, Jane and Leathwick, John R.},
doi = {10.1111/j.1541-0420.2008.01116.x},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Ward et al. - 2009 - Presence-Only Data and the EM Algorithm(5).pdf:pdf;:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Ward et al. - 2009 - Presence-Only Data and the EM Algorithm(6).pdf:pdf},
issn = {0006341X},
journal = {Biometrics},
keywords = {Boosted trees,EM algorithm,Logistic model,Presence‐only data,Use‐availability data},
month = {jun},
number = {2},
pages = {554--563},
publisher = {Wiley/Blackwell (10.1111)},
title = {{Presence-Only Data and the EM Algorithm}},
url = {http://doi.wiley.com/10.1111/j.1541-0420.2008.01116.x},
volume = {65},
year = {2009}
}
@book{ortner2014a,
author = {Ortner, R and Maillard, O.-A. and Ryabko, D},
pages = {140--154},
publisher = {Springer},
title = {{“Selecting nearoptimal approximate state representations in reinforcement learning”. In: International Conference on Algorithmic Learning Theory}}
}
@article{Miller2012,
annote = {Tax Collection Optimization},
author = {Miller, Gerard and Weatherwax, Melissa and Gardinier, Timothy and Abe, Naoki and Melville, Prem and Pendus, Cezar and Jensen, David and Reddy, Chandan K and Thomas, Vince and Bennett, James and Anderson, Gary and Cooley, Brent},
doi = {10.1287/inte.1110.0618},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Miller et al. - 2012 - Tax Collections Optimization for New York State(4).pdf:pdf},
title = {{Tax Collections Optimization for New York State}},
url = {http://pubsonline.informs.org74-84.https//doi.org/10.1287/inte.1110.0618http://www.informs.org},
year = {2012}
}
@book{brassard,
author = {Brassard, G and Bratley, P},
publisher = {Englewood Cliffs, NJ: Prentice Hall},
title = {{Algorithmics - Theory and Practice}},
year = {1988}
}
@misc{kakade2003a,
author = {Kakade, S and Kearns, M and Langford, J},
editor = {I.C.M.L.},
pages = {306--312},
title = {{Exploration in metric state spaces}},
volume = {3}
}
@misc{burda2018a,
author = {Burda, Y and Edwards, H and Storkey, A and Klimov, O},
edition = {arXiv prep},
title = {{Exploration by Random Network Distillation}}
}
@misc{oh2017a,
author = {Oh, J and Singh, S and Lee, H},
edition = {arXiv prep},
title = {{Value Prediction Network}}
}
@article{fortunato2017a,
author = {Fortunato, M and Azar, M G and Piot, B and Menick, J and Osband, I and Graves, A and Mnih, V and Munos, R and Hassabis, D and O.},
edition = {arXiv prep},
journal = {Pietquin, et al},
title = {{Noisy networks for exploration}}
}
@misc{zhu2016a,
author = {Zhu, Y and Mottaghi, R and Kolve, E and Lim, J J and Gupta, A and FeiFei, L and Farhadi, A},
edition = {arXiv prep},
title = {{Target-driven visual navigation in indoor scenes using deep reinforcement learning}}
}
@article{sunehag2017a,
author = {Sunehag, P and Lever, G and Gruslys, A and Czarnecki, W M and Zambaldi, V and Jaderberg, M and Lanctot, M and Sonnerat, N and Leibo, J Z and K.},
edition = {arXiv prep},
journal = {Tuyls, et al},
title = {{Value-Decomposition Networks For Cooperative Multi-Agent Learning}}
}
@misc{fran2015a,
author = {Fran{\c{c}}ois-Lavet, V and Fonteneau, R and Ernst, D},
edition = {arXiv prep},
title = {{How to Discount Deep Reinforcement Learning: Towards New Dynamic Strategies}}
}
@article{giusti2016a,
author = {Giusti, A and Guzzi, J and Cireşan, D C and He, F.-L. and Rodriguez, J P and Fontana, F and Faessler, M and Forster, C and Schmidhuber, J and Caro, G.Di},
journal = {IEEE Robotics and Automation Letters},
number = {2},
pages = {661--667},
title = {{A machine learning approach to visual perception of forest trails for mobile robots}},
volume = {1}
}
@misc{synnaeve2016a,
author = {Synnaeve, G and Nardelli, N and Auvolat, A and Chintala, S and Lacroix, T and Lin, Z and Richoux, F and Usunier, N},
edition = {arXiv prep},
title = {{TorchCraft: a Library for Machine Learning Research on Real-Time Strategy Games}}
}
@misc{lipton2016a,
author = {Lipton, Z C and Gao, J and Li, L and Li, X and Ahmed, F and Deng, L},
edition = {arXiv prep},
title = {{Efficient exploration for dialogue policy learning with BBQ networks {\&} replay buffer spiking}}
}
@article{moore1990a,
author = {Moore, A W},
journal = {Computers {\&} Chemical Engineering},
number = {4-5},
pages = {667--682},
title = {{Efficient memory-based learning for robot control}},
volume = {23}
}
@techreport{Watkins1992,
abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states. This paper presents and proves in detail a convergence theorem for Q,-learning based on that outlined in Watkins (1989). We show that Q-learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where many Q values can be changed each iteration, rather than just one.},
author = {Watkins, Christopher J C H and Dayan, Peter},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Watkins, Dayan - 1992 - Technical Note Q,-Learning(3).pdf:pdf},
keywords = {Q-learning,asynchronous dynamic programming,reinforcement learning,temporal differences},
pages = {279--292},
title = {{Technical Note Q,-Learning}},
url = {http://www.gatsby.ucl.ac.uk/{~}dayan/papers/cjch.pdf},
volume = {8},
year = {1992}
}
@article{Huang2017,
abstract = {Machine learning classifiers are known to be vulnerable to inputs maliciously constructed by adversaries to force misclassification. Such adversarial examples have been extensively studied in the context of computer vision applications. In this work, we show adversarial attacks are also effective when targeting neural network policies in reinforcement learning. Specifically, we show existing adversarial example crafting techniques can be used to significantly degrade test-time performance of trained policies. Our threat model considers adversaries capable of introducing small perturbations to the raw input of the policy. We characterize the degree of vulnerability across tasks and training algorithms, for a subclass of adversarial-example attacks in white-box and black-box settings. Regardless of the learned task or training algorithm, we observe a significant drop in performance, even with small adversarial perturbations that do not interfere with human perception. Videos are available at http://rll.berkeley.edu/adversarial.},
archivePrefix = {arXiv},
arxivId = {1702.02284},
author = {Huang, Sandy and Papernot, Nicolas and Goodfellow, Ian and Duan, Yan and Abbeel, Pieter},
eprint = {1702.02284},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Huang et al. - 2017 - Adversarial Attacks on Neural Network Policies(2).pdf:pdf},
month = {feb},
title = {{Adversarial Attacks on Neural Network Policies}},
url = {http://arxiv.org/abs/1702.02284},
year = {2017}
}
@misc{farquhar2017a,
author = {Farquhar, G and Rockt{\"{a}}schel, T and Igl, M and Whiteson, S},
edition = {arXiv prep},
title = {{TreeQN and ATreeC: Differentiable Tree Planning for Deep Reinforcement Learning}}
}
@incollection{cou2017a,
author = {Cou{\"{e}}toux},
booktitle = {9th International Conference on Agents and Artificial Intelligence (ICAART},
title = {{Approximate Bayes Optimal Policy Search using Neural Networks}}
}
@misc{schulman2017a,
author = {Schulman, J and Abbeel, P and Chen, X},
edition = {arXiv prep},
title = {{Equivalence Between Policy Gradients and Soft Q-Learning}}
}
@article{Rogers1991,
author = {Rogers, David F. and Plante, Robert D. and Wong, Richard T. and Evans, James R.},
doi = {10.1287/opre.39.4.553},
issn = {0030-364X},
journal = {Operations Research},
month = {aug},
number = {4},
pages = {553--582},
publisher = {Institute for Operations Research and the Management Sciences (INFORMS)},
title = {{Aggregation and Disaggregation Techniques and Methodology in Optimization}},
volume = {39},
year = {1991}
}
@inproceedings{peng1994a,
author = {Peng, J and Williams, R J},
booktitle = {Machine Learning Proceedings},
pages = {226--232},
publisher = {Elsevier},
title = {{Incremental multi-step Q-learning}}
}
@article{shannon-a,
author = {Shannon, C},
journal = {Philosophical Magazine},
number = {314},
title = {{1950. “Programming a Computer for Playing Chess”}},
volume = {41}
}
@techreport{Mnih2016b,
abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
author = {Mnih, Volodymyr and {Puigdom{\`{e}}nech Badia}, Adri{\`{a}} and Mirza, Mehdi and Graves, Alex and Harley, Tim and Lillicrap, Timothy P and Silver, David and Kavukcuoglu, Koray},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learning(4).pdf:pdf},
title = {{Asynchronous Methods for Deep Reinforcement Learning}},
url = {http://proceedings.mlr.press/v48/mniha16.pdf},
year = {2016}
}
@misc{van2016a,
author = {{Van Hasselt}, H and Guez, A and Silver, D},
title = {{“Deep Reinforcement Learning with Double Q-Learning.” In: AAAI. 2094–2100}}
}
@incollection{sukhbaatar2016a,
author = {Sukhbaatar, S and Szlam, A and Fergus, R},
booktitle = {Advances in Neural Information Processing Systems},
pages = {2244--2252},
title = {{Learning multiagent communication with backpropagation}}
}
@inproceedings{finn-a,
author = {{Finn C.}, S Levine and 2016b, P Abbeel.},
booktitle = {Proceedings of the 33rd International Conference on Machine Learning},
title = {{Guided cost learning: Deep inverse optimal control via policy optimization}},
volume = {48}
}
@article{watkins1992a,
author = {Watkins, C J and Dayan, P},
journal = {Machine learning},
number = {3-4},
pages = {279--292},
title = {{Q-learning}},
volume = {8}
}
@article{papernot2016technical,
abstract = {CleverHans is a software library that provides standardized reference implementations of adversarial example construction techniques and adversarial training. The library may be used to develop more robust machine learning models and to provide standardized benchmarks of models' performance in the adversarial setting. Benchmarks constructed without a standardized implementation of adversarial example construction are not comparable to each other, because a good result may indicate a robust model or it may merely indicate a weak implementation of the adversarial example construction procedure. This technical report is structured as follows. Section 1 provides an overview of adversarial examples in machine learning and of the CleverHans software. Section 2 presents the core functionalities of the library: namely the attacks based on adversarial examples and defenses to improve the robustness of machine learning models to these attacks. Section 3 describes how to report benchmark results using the library. Section 4 describes the versioning system.},
archivePrefix = {arXiv},
arxivId = {1610.00768},
author = {Papernot, Nicolas and Faghri, Fartash and Carlini, Nicholas and Goodfellow, Ian and Feinman, Reuben and Kurakin, Alexey and Xie, Cihang and Sharma, Yash and Brown, Tom and Roy, Aurko and Matyasko, Alexander and Behzadan, Vahid and Hambardzumyan, Karen and Zhang, Zhishuai and Juang, Yi-Lin and Li, Zhi and Sheatsley, Ryan and Garg, Abhibhav and Uesato, Jonathan and Gierke, Willi and Dong, Yinpeng and Berthelot, David and Hendricks, Paul and Rauber, Jonas and Long, Rujun and McDaniel, Patrick},
eprint = {1610.00768},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Papernot et al. - 2016 - Technical Report on the CleverHans v2.1.0 Adversarial Examples Library(2).pdf:pdf},
month = {oct},
title = {{Technical Report on the CleverHans v2.1.0 Adversarial Examples Library}},
url = {http://arxiv.org/abs/1610.00768},
year = {2016}
}
@inproceedings{mandel2014a,
author = {Mandel, T and Liu, Y.-E. and Levine, S and Brunskill, E and Popovic, Z},
title = {{“Offline policy evaluation across representations with applications to educational games”. In: Proceedings of the 2014 international conference on Autonomous agents and multi-agent systems. International Foundation for Autonomous Agents and Multiagent Sys}}
}
@misc{sutton2000a,
author = {Sutton, R S and McAllester, D A and Singh, S P and Mansour, Y},
title = {{“Policy gradient methods for reinforcement learning with function approximation”. In: Advances in neural information processing systems. 1057–1063}}
}
@article{geman1992a,
author = {Geman, S and Bienenstock, E and Doursat, R},
journal = {Neural computation},
number = {1},
pages = {1--58},
title = {{Neural networks and the bias/variance dilemma}},
volume = {4}
}
@inproceedings{adorf,
author = {Adorf, H M and Johnston, M D},
booktitle = {Proceedings of the International Joint Conference on Neural Networks},
title = {{A Discrete Stochastic Neural Network Algorithm for Constraint Satisfaction Problems}},
year = {1990}
}
@article{brafman2003a,
author = {Brafman, R I and Tennenholtz, M},
journal = {The Journal of Machine Learning Research},
pages = {213--231},
title = {{R-max-a general polynomial time algorithm for near-optimal reinforcement learning}},
volume = {3}
}
@article{tesauro1995a,
author = {Tesauro, G},
journal = {Communications of the ACM},
number = {3},
pages = {58--68},
title = {{Temporal difference learning and TD-Gammon}},
volume = {38}
}
@misc{teh2017a,
author = {Teh, Y W and Bapst, V and Czarnecki, W M and Quan, J and Kirkpatrick, J and Hadsell, R and Heess, N and Pascanu, R},
edition = {arXiv prep},
title = {{Distral: Robust Multitask Reinforcement Learning}}
}
@article{Peters2018,
abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
archivePrefix = {arXiv},
arxivId = {1802.05365},
author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
eprint = {1802.05365},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Peters et al. - 2018 - Deep contextualized word representations(2).pdf:pdf},
month = {feb},
title = {{Deep contextualized word representations}},
url = {http://arxiv.org/abs/1802.05365},
year = {2018}
}
@inproceedings{kolter2009a,
author = {Kolter, J Z and Ng, A Y},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning. ACM},
pages = {513--520},
title = {{Near-Bayesian exploration in polynomial time}}
}
@techreport{Lagoudakis2003,
annote = {LSPI paper},
author = {Lagoudakis, Michail G and Parr, Ronald},
booktitle = {Journal of Machine Learning Research},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Lagoudakis, Parr - 2003 - Least-Squares Policy Iteration(3).pdf:pdf},
pages = {1107--1149},
title = {{Least-Squares Policy Iteration}},
volume = {4},
year = {2003}
}
@article{hassabis2017a,
author = {Hassabis, D and Kumaran, D and Summerfield, C and Botvinick, M},
journal = {Neuron},
number = {2},
pages = {245--258},
title = {{Neuroscience-inspired artificial intelligence}},
volume = {95}
}
@misc{wang2015a,
author = {Wang, Z and de Freitas, N and Lanctot, M},
edition = {arXiv prep},
title = {{Dueling network architectures for deep reinforcement learning}}
}
@article{Knuth2019,
author = {Knuth, Kevin H.},
doi = {10.1016/j.dsp.2019.102581},
issn = {10512004},
journal = {Digital Signal Processing: A Review Journal},
month = {dec},
pages = {102581},
publisher = {Elsevier Inc.},
title = {{Optimal data-based binning for histograms and histogram-based probability density models}},
volume = {95},
year = {2019}
}
@article{Ciosek2015,
archivePrefix = {arXiv},
arxivId = {1501.03959},
author = {Ciosek, Kamil and Silver, David},
eprint = {1501.03959},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Ciosek, Silver - 2015 - Value Iteration with Options and State Aggregation.pdf:pdf},
month = {jan},
title = {{Value Iteration with Options and State Aggregation}},
year = {2015}
}
@misc{fran2018a,
author = {Fran{\c{c}}ois-Lavet, V and Bengio, Y and Precup, D and Pineau, J},
edition = {arXiv prep},
title = {{Combined Reinforcement Learning via Abstract Representations}}
}
@misc{bellemare2016a,
author = {Bellemare, M G and Srinivasan, S and Ostrovski, G and Schaul, T and Saxton, D and Munos, R},
edition = {arXiv prep},
title = {{Unifying Count-Based Exploration and Intrinsic Motivation}}
}
@misc{hausknecht2015a,
author = {Hausknecht, M and Stone, P},
edition = {arXiv prep},
title = {{Deep recurrent Q-learning for partially observable MDPs}}
}
@misc{gregor2016a,
author = {Gregor, K and Rezende, D J and Wierstra, D},
edition = {arXiv prep},
title = {{Variational Intrinsic Control}}
}
@article{schaul2010a,
author = {Schaul, T and Bayer, J and Wierstra, D and Sun, Y and Felder, M and Sehnke, F and R{\"{u}}ckstie{\ss}, T and Schmidhuber, J},
journal = {The Journal of Machine Learning Research},
pages = {743--746},
title = {{PyBrain}},
volume = {11}
}
@misc{mohamed2015a,
author = {Mohamed, S and Rezende, D J},
title = {{“Variational information maximisation for intrinsically motivated reinforcement learning”. In: Advances in neural information processing systems. 2125–2133}}
}
@article{bellman-a,
author = {Bellman, R 1957a},
journal = {Journal of Mathematics and Mechanics},
pages = {679--684},
title = {{A Markovian decision process}}
}
@misc{tan2018a,
author = {Tan, J and Zhang, T and Coumans, E and Iscen, A and Bai, Y and Hafner, D and Bohez, S and Vanhoucke, V},
edition = {arXiv prep},
title = {{Sim-to-Real: Learning Agile Locomotion For Quadruped Robots}}
}
@techreport{Boyan1998,
author = {Boyan, Justin A},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Boyan - 1998 - Learning Evaluation Functions for Global Optimization(3).pdf:pdf},
title = {{Learning Evaluation Functions for Global Optimization}},
url = {https://pdfs.semanticscholar.org/61d4/897dbf7ced83a0eb830a8de0dd64abb58ebd.pdf},
year = {1998}
}
@misc{johnson2017a,
author = {Johnson, J and Hariharan, B and van der Maaten, L and Hoffman, J and FeiFei, L and Zitnick, C L and Girshick, R},
edition = {arXiv prep},
title = {{Inferring and Executing Programs for Visual Reasoning}}
}
@article{davis2017a,
author = {Davis, K.Waugh and Johanson, M and Bowling, M},
journal = {Science},
number = {6337},
pages = {508--513},
title = {{DeepStack: Expert-level artificial intelligence in heads-up no-limit poker}},
volume = {356}
}
@incollection{kempka2016a,
author = {Kempka, M and Wydmuch, M and Runc, G and Toczek, J and Ja{\'{s}}kowski, W},
booktitle = {Computational Intelligence and Games (CIG), 2016 IEEE Conference on. IEEE},
pages = {1--8},
title = {{Vizdoom: A doom-based ai research platform for visual reinforcement learning}}
}
@incollection{sun2011a,
author = {Sun, Y and Gomez, F and Schmidhuber, J},
booktitle = {Artificial General Intelligence. Springer},
pages = {41--51},
title = {{Planning to be surprised: Optimal bayesian exploration in dynamic environments}}
}
@book{Puterman1994,
abstract = {The Wiley-Interscience Paperback Series consists of selected books that have$\backslash$nbeen made more accessible to consumers in an effort to increase global appeal$\backslash$nand general circulation. With these new unabridged softcover volumes, Wiley$\backslash$nhopes to extend the lives of these works by making them available to future$\backslash$ngenerations of statisticians, mathematicians, and scientists.$\backslash$n$\backslash$n"This text is unique in bringing together so many results hitherto found only$\backslash$nin part in other texts and papers. . . . The text is fairly self-contained,$\backslash$ninclusive of some basic mathematical results needed, and provides a rich diet$\backslash$nof examples, applications, and exercises. The bibliographical material at the$\backslash$nend of each chapter is excellent, not only from a historical perspective, but$\backslash$nbecause it is valuable for researchers in acquiring a good perspective of the$\backslash$nMDP research potential."$\backslash$n$\backslash$n-Zentralblatt fur Mathematik$\backslash$n$\backslash$n". . . it is of great value to advanced-level students, researchers, and$\backslash$nprofessional practitioners of this field to have now a complete volume (with$\backslash$nmore than 600 pages) devoted to this topic. . . . Markov Decision Processes:$\backslash$nDiscrete Stochastic Dynamic Programming represents an up-to-date, unified, and$\backslash$nrigorous treatment of theoretical and computational aspects of discrete-time$\backslash$nMarkov decision processes."$\backslash$n$\backslash$n-Journal of the American Statistical Association},
author = {Puterman, Martin L},
isbn = {0471727822},
keywords = {book,puterman},
publisher = {Wiley},
title = {{Markov Decision Processes: Discrete Stochastic Dynamic Programming}},
year = {1994}
}
@article{Nogueira2017,
abstract = {Search engines play an important role in our everyday lives by assisting us in finding the information we need. When we input a complex query, however, results are often far from satisfactory. In this work, we introduce a query reformulation system based on a neural network that rewrites a query to maximize the number of relevant documents returned. We train this neural network with reinforcement learning. The actions correspond to selecting terms to build a reformulated query, and the reward is the document recall. We evaluate our approach on three datasets against strong baselines and show a relative improvement of 5-20{\%} in terms of recall. Furthermore, we present a simple method to estimate a conservative upper-bound performance of a model in a particular environment and verify that there is still large room for improvements.},
archivePrefix = {arXiv},
arxivId = {1704.04572},
author = {Nogueira, Rodrigo and Cho, Kyunghyun},
eprint = {1704.04572},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Nogueira, Cho - 2017 - Task-Oriented Query Reformulation with Reinforcement Learning(2).pdf:pdf},
month = {apr},
title = {{Task-Oriented Query Reformulation with Reinforcement Learning}},
url = {http://arxiv.org/abs/1704.04572},
year = {2017}
}
@book{norris1998a,
author = {Norris, J R},
publisher = {Cambridge university press},
title = {{Markov chains. No. 2}}
}
@techreport{Bernstein2008,
author = {Bernstein, Andrey and Shimkin, Nahum},
booktitle = {undefined},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Bernstein, Shimkin - 2008 - Adaptive Aggregation for Reinforcement Learning with Efficient Exploration Deterministic Domains.pdf:pdf},
title = {{Adaptive Aggregation for Reinforcement Learning with Efficient Exploration: Deterministic Domains}},
year = {2008}
}
@misc{wang2016a,
author = {Wang, Z and Bapst, V and Heess, N and Mnih, V and Munos, R and Kavukcuoglu, K and de Freitas, N},
edition = {arXiv prep},
title = {{Sample efficient actor-critic with experience replay}}
}
@misc{bostrom2017a,
annote = {Superintelligence. Dunod},
author = {Bostrom, N},
title = {{No Title}}
}
@article{Strehl2008a,
abstract = {Several algorithms for learning near-optimal policies in Markov Decision Processes have been analyzed and proven efficient. Empirical results have suggested that Model-based Interval Estimation (MBIE) learns efficiently in practice, effectively balancing exploration and exploitation. This paper presents a theoretical analysis of MBIE and a new variation called MBIE-EB, proving their efficiency even under worst-case conditions. The paper also introduces a new performance metric, average loss, and relates it to its less "online" cousins from the literature. {\textcopyright} 2008 Elsevier Inc. All rights reserved.},
annote = {RiverSwim
SixArms},
author = {Strehl, Alexander L. and Littman, Michael L.},
doi = {10.1016/j.jcss.2007.08.009},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Strehl, Littman - 2008 - An analysis of model-based Interval Estimation for Markov Decision Processes(3).pdf:pdf},
issn = {00220000},
journal = {Journal of Computer and System Sciences},
keywords = {Learning theory,Markov Decision Processes,Reinforcement learning},
month = {dec},
number = {8},
pages = {1309--1331},
title = {{An analysis of model-based Interval Estimation for Markov Decision Processes}},
volume = {74},
year = {2008}
}
@book{anderson1958a,
address = {New York},
author = {Anderson, T W and Anderson, T W and Anderson, T W and Anderson, T W and Math{\'{e}}maticien, E.-U.},
publisher = {Wiley},
title = {{An introduction to multivariate statistical analysis}},
volume = {2}
}
@incollection{kakade2001a,
author = {Kakade, S},
booktitle = {Advances in Neural},
title = {{A Natural Policy Gradient}}
}
@book{Busoniu2010,
author = {Buşoniu, Lucian and Babu{\v{s}}ka, Robert and {De Schutter}, Bart and Ernst, Damien},
booktitle = {Reinforcement Learning and Dynamic Programming Using Function Approximators},
doi = {10.1201/9781439821091},
isbn = {9781439821091},
title = {{Reinforcement learning and dynamic programming using function approximators}},
year = {2010}
}
@misc{parisotto2015a,
author = {Parisotto, E and Ba, J L and Salakhutdinov, R},
edition = {arXiv prep},
title = {{Actor-mimic: Deep multitask and transfer reinforcement learning}}
}
@inproceedings{duchesne2017a,
author = {Duchesne, L and Karangelos, E and Wehenkel, L},
title = {{“Machine learning of real-time power systems reliability management response”. PowerTech Manchester 2017 Proceedings}}
}
@inproceedings{li2011a,
author = {Li, L and Chu, W and Langford, J and Wang, X},
booktitle = {Proceedings of the fourth ACM international conference on},
pages = {297--306},
title = {{Unbiased offline evaluation of contextual-bandit-based news article recommendation algorithms}}
}
@article{Tsitsiklis1996,
author = {Tsitsiklis, John N. and {Van Roy}, Benjamin},
doi = {10.1007/BF00114724},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Tsitsiklis, Van Roy - 1996 - Feature-based methods for large scale dynamic programming.pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
number = {1-3},
pages = {59--94},
publisher = {Springer Netherlands},
title = {{Feature-based methods for large scale dynamic programming}},
volume = {22},
year = {1996}
}
@article{bartlett2002a,
author = {Bartlett, P L and Mendelson, S},
journal = {Journal of Machine Learning Research},
pages = {463--482},
title = {{Rademacher and Gaussian complexities: Risk bounds and structural results}},
volume = {3}
}
@inproceedings{hauskrecht1998a,
author = {Hauskrecht, M and Meuleau, N and Kaelbling, L P and Dean, T and Boutilier, C},
booktitle = {Proceedings of the Fourteenth conference on Uncertainty in artificial intelligence},
pages = {220--229},
publisher = {Morgan Kaufmann Publishers Inc},
title = {{Hierarchical solution of Markov decision processes using macro-actions}}
}
@article{Bengio2009,
abstract = {Theoretical results suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g., in vision, language, and other AI-level tasks), one may need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This monograph discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.},
archivePrefix = {arXiv},
arxivId = {submit/0500581},
author = {Bengio, Y.},
doi = {10.1561/2200000006},
eprint = {0500581},
isbn = {2200000006},
issn = {1935-8237},
journal = {Foundations and Trends{\textregistered} in Machine Learning},
pmid = {17348934},
primaryClass = {submit},
title = {{Learning Deep Architectures for AI}},
year = {2009}
}
@misc{lowe2017a,
author = {Lowe, R and Wu, Y and Tamar, A and Harb, J and Abbeel, P and Mordatch, I},
edition = {arXiv prep},
title = {{Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments}}
}
@misc{tzeng2015a,
author = {Tzeng, E and Devin, C and Hoffman, J and Finn, C and Abbeel, P and Levine, S and Saenko, K and Darrell, T},
edition = {arXiv prep},
title = {{Adapting deep visuomotor representations with weak pairwise constraints}}
}
@article{Royle2012,
abstract = {1.Understanding the factors affecting species occurrence is a pre-eminent focus of applied ecological research. However, direct information about species occurrence is lacking for many species. Instead, researchers sometimes have to rely on so-called presence-only data (i.e. when no direct information about absences is available), which often results from opportunistic, unstructured sampling. maxent is a widely used software program designed to model and map species distribution using presence-only data. 2.We provide a critical review of maxent as applied to species distribution modelling and discuss how it can lead to inferential errors. A chief concern is that maxent produces a number of poorly defined indices that are not directly related to the actual parameter of interest – the probability of occurrence ($\psi$). This focus on an index was motivated by the belief that it is not possible to estimate $\psi$ from presence-only data; however, we demonstrate that $\psi$ is identifiable using conventional likelihood methods under the assumptions of random sampling and constant probability of species detection. 3.The model is implemented in a convenient r package which we use to apply the model to simulated data and data from the North American Breeding Bird Survey. We demonstrate that maxent produces extreme under-predictions when compared to estimates produced by logistic regression which uses the full (presence/absence) data set. We note that maxent predictions are extremely sensitive to specification of the background prevalence, which is not objectively estimated using the maxent method. 4.As with maxent, formal model-based inference requires a random sample of presence locations. Many presence-only data sets, such as those based on museum records and herbarium collections, may not satisfy this assumption. However, when sampling is random, we believe that inference should be based on formal methods that facilitate inference about interpretable ecological quantities instead of vaguely defined indices.},
author = {Royle, J. Andrew and Chandler, Richard B. and Yackulic, Charles and Nichols, James D.},
doi = {10.1111/j.2041-210X.2011.00182.x},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Royle et al. - 2012 - Likelihood analysis of species occurrence probability from presence-only data for modelling species distributio(3).pdf:pdf},
isbn = {2041-210X},
issn = {2041210X},
journal = {Methods in Ecology and Evolution},
keywords = {Bayes' rule,Detection probability,Logistic regression,Occupancy model,Occurrence probability,Presence-only data,Species distribution model,maxent},
number = {3},
pages = {545--554},
title = {{Likelihood analysis of species occurrence probability from presence-only data for modelling species distributions}},
volume = {3},
year = {2012}
}
@book{riedmiller2005a,
address = {In},
author = {Riedmiller, M},
pages = {317--328},
publisher = {Machine Learning: ECML},
title = {{Neural fitted Q iteration–first experiences with a data efficient neural reinforcement learning method}}
}
@incollection{coumans2016a,
author = {Coumans, E and Bai, Y},
edition = {Barto.2012},
title = {{Bullet}},
url = {http://pybullet.org/.}
}
@misc{todorov2012a,
author = {Todorov, E and Erez, T and Tassa, Y},
title = {{“MuJoCo: A physics engine for model-based control”. In: Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on. IEEE. 5026–5033}}
}
@article{VanRoy2006,
author = {{Van Roy}, Benjamin},
doi = {10.1287/moor.1060.0188},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Van Roy - 2006 - Performance Loss Bounds for Approximate Value Iteration with State Aggregation.pdf:pdf},
journal = {Mathematics of Operations Research},
number = {2},
pages = {234},
title = {{Performance Loss Bounds for Approximate Value Iteration with State Aggregation}},
volume = {31},
year = {2006}
}
@book{gordon1999a,
author = {Gordon, G J},
publisher = {Robotics Institute: 228},
title = {{Approximate solutions to Markov decision processes}}
}
@article{schultz1997a,
author = {Schultz, W and Dayan, P and Montague, P R},
journal = {Science},
number = {5306},
pages = {1593--1599},
title = {{A neural substrate of prediction and reward}},
volume = {275}
}
@article{piketty2013a,
author = {Piketty, T},
journal = {In: IJCAI},
pages = {1025--1032},
title = {{Capital in the Twenty-First Century}},
volume = {3}
}
@misc{kalashnikov2018a,
author = {Kalashnikov, D and Irpan, A and Pastor, P and Ibarz, J and Herzog, A and Jang, E and Quillen, D and Holly, E and Kalakrishnan, M and Vanhoucke, V and Levine, S},
edition = {arXiv prep},
title = {{Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation}}
}
@misc{gruslys2017a,
author = {Gruslys, A and Azar, M G and Bellemare, M G and Munos, R},
edition = {arXiv prep},
title = {{The Reactor: A Sample-Efficient Actor-Critic Architecture}}
}
@misc{zoph2016a,
author = {Zoph, B and Le, Q V},
edition = {arXiv prep},
title = {{Neural architecture search with reinforcement learning}}
}
@misc{fazel-zarandi2017a,
author = {Fazel-Zarandi, M and Li, S.-W. and Cao, J and Casale, J and Henderson, P and Whitney, D and Geramifard, A},
edition = {arXiv prep},
title = {{Learning Robust Dialog Policies in Noisy Environments}}
}
@book{pavlov1927a,
author = {Pavlov, I P},
publisher = {Oxford University Press},
title = {{Conditioned reflexes}}
}
@article{sondik1978a,
author = {Sondik, E J},
journal = {Operations research},
number = {2},
pages = {282--304},
title = {{The optimal control of partially observable Markov processes over the infinite horizon: Discounted costs}},
volume = {26}
}
@misc{fran2016a,
author = {Fran{\c{c}}ois-Lavet, V},
institution = {thesis. University of Liege, Belgium},
title = {{DeeR”. https://deer.readthedocs.io/. Fran{\c{c}}ois-Lavet, V.2017.“Contributions to deep reinforcement learning and its applications in smartgrids}},
type = {PhD}
}
@article{cohen2007a,
author = {Cohen, J D and McClure, S M and Angela, J Y},
journal = {Philosophical Transactions of the Royal Society of London B: Biological Sciences},
number = {1481},
pages = {933--942},
title = {{Should I stay or should I go? How the human brain manages the trade-off between exploitation and exploration}},
volume = {362}
}
@misc{plappert2017a,
author = {Plappert, M and Houthooft, R and Dhariwal, P and Sidor, S and Chen, R Y and Chen, X and Asfour, T and Abbeel, P and Andrychowicz, M},
edition = {arXiv prep},
title = {{Parameter Space Noise for Exploration}}
}
@misc{gauci2018a,
author = {Gauci, J and Conti, E and Liang, Y and Virochsiri, K and He, Y and Kaden, Z and Narayanan, V and Ye, X},
edition = {arXiv prep},
title = {{Horizon: Facebook's Open Source Applied Reinforcement Learning Platform}}
}
@incollection{mordatch2015a,
author = {Mordatch, I and Lowrey, K and Andrew, G and Popovic, Z and Todorov, E V},
booktitle = {Advances in Neural Information Processing Systems},
pages = {3132--3140},
title = {{Interactive control of diverse complex characters with neural networks}}
}
@misc{gelly2006a,
author = {Gelly, S and Wang, Y and Munos, R and Teytaud, O},
title = {{Modification of UCT with patterns in Monte-Carlo Go}}
}
@misc{macglashan2017a,
author = {MacGlashan, J and Ho, M K and Loftin, R and Peng, B and Roberts, D and Taylor, M E and Littman, M L},
edition = {arXiv prep},
title = {{Interactive Learning from PolicyDependent Human Feedback}}
}
@misc{wahlstroem2015a,
author = {Wahlstr{\"{o}}m, N and Sch{\"{o}}n, T B and Deisenroth, M P},
edition = {arXiv prep},
title = {{From pixels to torques: Policy learning with deep dynamical models}}
}
@misc{johnson2016a,
author = {Johnson, M and Hofmann, K and Hutton, T and Bignell, D},
title = {{“The Malmo Platform for Artificial Intelligence Experimentation.” In: IJCAI. 4246–4247}}
}
@misc{ostrovski2017a,
author = {Ostrovski, G and Bellemare, M G and v. d. Oord, A and Munos, R},
edition = {arXiv prep},
title = {{Count-based exploration with neural density models}}
}
@misc{chen2017a,
author = {Chen, X and Liu, C and Song, D},
edition = {arXiv prep},
title = {{Learning Neural Programs To Parse Programs}}
}
@incollection{gu2017a,
author = {Gu, S and Holly, E and Lillicrap, T and Levine, S},
booktitle = {Robotics and Automation (ICRA), 2017 IEEE International Conference on. IEEE},
pages = {3389--3396},
title = {{Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates}}
}
@article{silver2016a,
author = {Silver, D and Huang, A and Maddison, C J and Guez, A and Sifre, L and Driessche, G.Van Den and Schrittwieser, J and Antonoglou, I and Panneershelvam, V and Lanctot, M},
journal = {Nature},
number = {7587},
pages = {484--489},
title = {{Mastering the game of Go with deep neural networks and tree search}},
volume = {529}
}
@article{hafner2011a,
author = {Hafner, R and Riedmiller, M},
journal = {Machine learning},
number = {1-2},
pages = {137--169},
title = {{Reinforcement learning in feedback control}},
volume = {84}
}
@misc{ruder2017a,
author = {Ruder, S},
edition = {arXiv prep},
title = {{An overview of multi-task learning in deep neural networks}}
}
@techreport{Henderson,
abstract = {In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.},
archivePrefix = {arXiv},
arxivId = {1709.06560v2},
author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
eprint = {1709.06560v2},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Henderson et al. - Unknown - Deep Reinforcement Learning that Matters(5).pdf:pdf;:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Henderson et al. - Unknown - Deep Reinforcement Learning that Matters(6).pdf:pdf},
keywords = {Machine Learning Methods Track},
title = {{Deep Reinforcement Learning that Matters}},
url = {www.aaai.org}
}
@misc{mccallum1996a,
author = {McCallum, A K},
institution = {thesis. University of Rochester},
title = {{Reinforcement learning with selective perception and hidden state}},
type = {PhD}
}
@inproceedings{Hasselt2010,
abstract = {In some stochastic environments the well-known reinforcement learning algo-rithm Q-learning performs very poorly. This poor performance is caused by large overestimations of action values. These overestimations result from a positive bias that is introduced because Q-learning uses the maximum action value as an approximation for the maximum expected action value. We introduce an alter-native way to approximate the maximum expected value for any set of random variables. The obtained double estimator method is shown to sometimes under-estimate rather than overestimate the maximum expected value. We apply the double estimator to Q-learning to construct Double Q-learning, a new off-policy reinforcement learning algorithm. We show the new algorithm converges to the optimal policy and that it performs well in some settings in which Q-learning per-forms poorly due to its overestimation.},
archivePrefix = {arXiv},
arxivId = {1606.04615},
author = {Hasselt, Hado Van and Group, Adaptive Computation and Wiskunde, Centrum and {Van Hasselt}, Hado},
booktitle = {Neural Information Proceeding Systems},
doi = {10.1016/j.tws.2009.08.006},
eprint = {1606.04615},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Hasselt et al. - 2010 - Double Q-learning(2).pdf:pdf},
isbn = {9781617823800},
issn = {02638231},
pmid = {26150344},
title = {{Double Q-learning}},
url = {https://papers.nips.cc/paper/3964-double-q-learning.pdf},
year = {2010}
}
@incollection{hochreiter2001a,
author = {Hochreiter, S and Younger, A S and Conwell, P R},
booktitle = {International Conference on Artificial Neural Networks. Springer},
pages = {87--94},
title = {{Learning to learn using gradient descent}}
}
@misc{finn2017a,
author = {Finn, C and Abbeel, P and Levine, S},
edition = {arXiv prep},
title = {{Model-agnostic metalearning for fast adaptation of deep networks}}
}
@article{Phillips2008,
abstract = {1 The overall functional capacity of the liver was evaluated using [35S]-bromosulphophthalein (BSP, 100 mg/kg, i.v.) in biliary fistulated adult rats pretreated orally with different doses of paracetamol (APAP) for varying time intervals. 2 The maximal hepatic damage occurred between 12-18 h after single doses of APAP (0.5 or 1 g/kg); hepatic excretory function returned to control levels by 48-72 hours. 3 Administration of either 0.5 or 1 g/kg APAP 18 h before BSP caused a dose-dependent inhibition of the choleretic effect of BSP and of the 60 min cumulative excretion of the dye, but conversely, produced a significant increase in the liver and plasma concentrations of 35S. 4 Following acute (0.25 g/kg), or subacute (0.5 g/kg, twice daily for 7 days) treatment with APAP, the total excretion of 35S in bile and the retention of 35S in the liver or plasma remained essentially the same as that for the controls. 5 In rats given single doses of 1 g/kg APAP, the hepatic uptake of the dye was significantly increased during the early stages of intoxication, while the opposite effect was observed at late periods. 6 The bile flow appeared to be inversely related to the excretion of unchanged BSP, and directly related to the excretion of the major BSP conjugate in bile. 7 The hepatic clearance of BSP was more rapid in rats treated subacutely with 0.5 or 1 g/kg APAP, than in those treated acutely with equal doses, suggesting that the intensity of APAP-induced hepatotoxicity became less severe after the repeated administration of this drug. 8 It is concluded that the hepatic uptake, metabolism and excretion of BSP are reversibly impaired following APAP-induced liver injury.},
archivePrefix = {arXiv},
arxivId = {10.1111/j.2007.0906-7590.05203.x},
author = {Phillips, Steven J. and Dud{\'{i}}k, Miroslav},
doi = {10.1111/j.2007.0906-7590.05203.x},
eprint = {j.2007.0906-7590.05203.x},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Phillips, Dud{\'{i}}k - 2008 - Modeling of species distribution with Maxent new extensions and a comprehensive evalutation(2).pdf:pdf},
isbn = {0906-7590},
issn = {0007-1188},
journal = {Ecograpy},
keywords = {05203,0906-7590,10,1111,161 {\'{a}} 175,2007,2007 at,2007 ecography,2008,accepted 13 december 2007,doi,graphy 31,inc,j,jo,journal compilation,t,x},
number = {December 2007},
pages = {161--175},
pmid = {3245},
primaryClass = {10.1111},
title = {{Modeling of species distribution with Maxent: new extensions and a comprehensive evalutation}},
volume = {31},
year = {2008}
}
@incollection{ho2016a,
author = {Ho, J and Ermon, S},
booktitle = {Advances in Neural Information Processing Systems},
pages = {4565--4573},
title = {{Generative adversarial imitation learning}}
}
@article{deng2017a,
author = {Deng, Y and Bao, F and Kong, Y and Ren, Z and Dai, Q},
journal = {IEEE transactions on neural networks and learning systems},
number = {3},
pages = {653--664},
title = {{Deep direct reinforcement learning for financial signal representation and trading}},
volume = {28}
}
@misc{schaarschmidt2017a,
author = {Schaarschmidt, M and Kuhnle, A and Fricke, K},
title = {{TensorForce: A TensorFlow library for applied reinforcement learning}}
}
@incollection{pathak2017a,
author = {Pathak, D and Agrawal, P and Efros, A A and Darrell, T},
booktitle = {International Conference on Machine Learning (ICML)},
title = {{Curiositydriven exploration by self-supervised prediction}},
volume = {Vol.}
}
@incollection{florensa2018a,
author = {Florensa, C and Held, D and Geng, X and Abbeel, P},
booktitle = {International Conference on Machine Learning},
pages = {1514--1523},
title = {{Automatic goal generation for reinforcement learning agents}}
}
@misc{pascanu2017a,
author = {Pascanu, R and Li, Y and Vinyals, O and Heess, N and Buesing, L and Racani{\`{e}}re, S and Reichert, D and Weber, T and Wierstra, D and Battaglia, P},
edition = {arXiv prep},
title = {{Learning model-based planning from scratch}}
}
@misc{sutton1996a,
author = {Sutton, R S},
title = {{“Generalization in reinforcement learning: Successful examples using sparse coarse coding”. Advances in neural information processing systems: 1038–1044}}
}
@incollection{oh2015a,
author = {Oh, J and Guo, X and Lee, H and Lewis, R L and Singh, S},
booktitle = {Advances in Neural Information Processing Systems},
pages = {2863--2871},
title = {{Actionconditional video prediction using deep networks in atari games}}
}
@article{Mendelssohn1982,
author = {Mendelssohn, Roy},
doi = {10.1287/opre.30.1.62},
issn = {0030364X},
journal = {Operations Research},
number = {1},
pages = {62--73},
publisher = {INFORMS},
title = {{An Iterative Aggregation Procedure for Markov Decision Processes}},
url = {https://www.jstor.org/stable/170309},
volume = {30},
year = {1982}
}
@misc{dosovitskiy2016a,
author = {Dosovitskiy, A and Koltun, V},
edition = {arXiv prep},
title = {{Learning to act by predicting the future}}
}
@incollection{bellman1957a,
address = {Bello},
author = {Bellman, R},
title = {{Dynamic Programming}}
}
@misc{henderson2017b,
author = {Henderson, P and Islam, R and Bachman, P and Pineau, J and Precup, D and Meger, D},
edition = {arXiv prep},
title = {{Deep Reinforcement Learning that Matters}}
}
@article{holroyd2002a,
author = {Holroyd, C B and Coles, M G},
journal = {Psychological review},
number = {4},
pages = {679},
title = {{The neural basis of human error processing: reinforcement learning, dopamine, and the error-related negativity.}},
volume = {109}
}
@misc{brys2014a,
author = {Brys, T and Harutyunyan, A and Vrancx, P and Taylor, M E and Kudenko, D and Now{\'{e}}, A},
title = {{“Multi-objectivization of reinforcement learning problems by reward shaping”. In: Neural Networks (IJCNN), 2014 International Joint Conference on. IEEE. 2315–2322}}
}
@misc{zhang2016a,
author = {Zhang, C and Bengio, S and Hardt, M and Recht, B and Vinyals, O},
edition = {arXiv prep},
title = {{Understanding deep learning requires rethinking generalization}}
}
@book{ziebart2010a,
author = {Ziebart, B D},
publisher = {Carnegie Mellon University},
title = {{Modeling purposeful adaptive behavior with the principle of maximum causal entropy}}
}
@misc{gu-b,
author = {Gu, S and Lillicrap, T and Ghahramani, Z and Turner, R E and Sch{\"{o}}lkopf, B and Levine, S},
edition = {arXiv prep},
title = {{2017c. “Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning”}}
}
@article{Lillicrap2015,
abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
archivePrefix = {arXiv},
arxivId = {1509.02971},
author = {Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
eprint = {1509.02971},
journal = {ICLR},
title = {{Continuous control with deep reinforcement learning: Deep Deterministic Policy Gradients (DDPG)}},
year = {2015}
}
@article{Iyengar2005,
abstract = {In this paper we propose a robust formulation for discrete time dynamic programming (DP). The objective of the robust formulation is to systematically mitigate the sensitivity of the DP optimal policy to ambiguity in the underlying transition probabilities. The ambiguity is modeled by associating a set of conditional measures with each state-action pair. Consequently, in the robust formulation each policy has a set of measures associated with it. We prove that when this set of measures has a certain "rectangularity" property, all of the main results for finite and infinite horizon DP extend to natural robust counterparts. We discuss techniques from Nilim and El Ghaoui [17] for constructing suitable sets of conditional measures that allow one to efficiently solve for the optimal robust policy. We also show that robust DP is equivalent to stochastic zero-sum games with perfect information. 1. Introduction. This paper is concerned with sequential decision making in uncertain environments. Decisions are made in stages and each decision, in addition to providing an immediate reward, changes the context of future decisions; thereby affecting the future rewards. Due to the uncertain nature of the environment, there is limited information about both the immediate reward from each decision and the resulting future state. In order to achieve a good performance over all the stages, the decision maker has to trade-off the immediate payoff with future payoffs. Dynamic programming (DP) is the mathematical framework that allows the decision maker to efficiently compute a good overall strategy by succinctly encoding the evolving information state. In the DP formalism the uncertainty in the environment is modeled by a Markov process whose transition probability depends both on the information state and the action taken by the decision maker. It is assumed that the transition probability corresponding to each state-action pair is known to the decision maker, and the goal is to choose a policy, i.e., a rule that maps states to actions, that maximizes some performance measure. Puterman [20] provides a excellent introduction to the DP formalism and its various applications. In this paper, we assume that the reader has some prior knowledge of DP. The DP formalism encodes information in the form of a "reward-to-go" function (see Puterman [20] for details) and chooses an action that maximizes the sum of the immediate reward and the expected "reward-to-go." Thus, to compute the optimal action in any given state the "reward-to-go" function for all the future states must be known. In many applications of DP, the number of states and actions available in each state are large; consequently , the computational effort required to compute the optimal policy for a DP can be overwhelming-Bellman's "curse of dimensionality." For this reason, considerable recent research effort has focused on developing algorithms that compute an approximately optimal policy efficiently (Bertsekas and Tsitsiklis [5], de Farias and Van Roy [8]). Fortunately, for many applications the DP optimal policy can be computed with a modest computational effort. In this paper we restrict attention to this class of DPs. Typically, the transition probability of the underlying Markov process is estimated from historical data and is, therefore, subject to statistical errors. In current practice, these errors are ignored and the optimal policy is computed assuming that the estimate is, indeed, the true transition probability. The DP optimal policy is quite sensitive to perturbations in the transition probability},
author = {Iyengar, Garud N},
doi = {10.1287/moor.1040.0129},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Iyengar - 2005 - Robust Dynamic Programming(3).pdf:pdf},
issn = {1526-5471},
journal = {Robust Dynamic Programming. Mathematics of Operations Research},
keywords = {90C25 OR/MS subject classificati,Markov decision processes,ambiguity MSC2000 subject classification: Primary:,dynamic programming,robust optimization,secondary: 90C40,secondary: probability-Markov processes},
number = {2},
pages = {257--280},
title = {{Robust Dynamic Programming}},
url = {http://pubsonline.informs.org.https//doi.org/10.1287/moor.1040.0129http://www.informs.orghttp://www.columbia.edu/∼gi10},
volume = {30},
year = {2005}
}
@misc{singh1994a,
author = {Singh, S P and Jaakkola, T S and Jordan, M I},
pages = {284--292},
title = {{“Learning Without State-Estimation in Partially Observable Markovian Decision Processes.” In: ICML}}
}
@techreport{erhan2009a,
author = {Erhan, D and Bengio, Y and Courville, A and Vincent, P},
institution = {University of Montreal},
number = {3},
pages = {1},
title = {{Visualizing higher-layer features of a deep network}},
volume = {1341}
}
@misc{zamora2016a,
author = {Zamora, I and Lopez, N G and Vilches, V M and Cordero, A H},
edition = {arXiv prep},
title = {{Extending the OpenAI Gym for robotics: a toolkit for reinforcement learning using ROS and Gazebo}}
}
@article{camerer2005a,
author = {Camerer, C and Loewenstein, G and Prelec, D},
journal = {Journal of economic Literature},
number = {1},
pages = {9--64},
title = {{Neuroeconomics: How neuroscience can inform economics}},
volume = {43}
}
@misc{unknown-a,
edition = {arXiv prep},
title = {{Asymmetric Actor Critic for Image-Based Robot Learning”}}
}
@article{nguyen1990a,
author = {Nguyen, D H and Widrow, B},
journal = {IEEE Control systems magazine},
number = {3},
pages = {18--23},
title = {{Neural networks for self-learning control systems}},
volume = {10}
}
@article{bubeck2011a,
author = {Bubeck, S and Munos, R and Stoltz, G},
journal = {Theoretical Computer Science},
number = {19},
pages = {1832--1852},
title = {{Pure exploration in finitely-armed and continuous-armed bandits}},
volume = {412}
}
@inproceedings{morimura2010a,
author = {Morimura, T and Sugiyama, M and Kashima, H and Hachiya, H and Tanaka, T},
booktitle = {Proceedings of the 27th International Conference on Machine Learning},
pages = {799--806},
title = {{Nonparametric return distribution approximation for reinforcement learning}},
volume = {10}
}
@incollection{dayan2008b,
author = {Dayan, P and Niv, Y},
booktitle = {Current opinion in neurobiology},
number = {2},
pages = {185--196},
title = {{Reinforcement learning: the good, the bad and the ugly}},
volume = {18}
}
@techreport{braziunas2003a,
author = {Braziunas, D},
institution = {University of Toronto, Tech. Rep},
title = {{POMDP solution methods}}
}
@article{fonteneau2013a,
author = {Fonteneau, R and Murphy, S A and Wehenkel, L and Ernst, D},
journal = {Annals of operations research},
number = {1},
pages = {383--416},
title = {{Batch mode reinforcement learning based on the synthesis of artificial trajectories}},
volume = {208}
}
@incollection{thomas2014a,
author = {Thomas, P},
booktitle = {International Conference on Machine Learning},
pages = {441--448},
title = {{Bias in natural actor-critic algorithms}}
}
@misc{tobin2017a,
author = {Tobin, J and Fong, R and Ray, A and Schneider, J and Zaremba, W and Abbeel, P},
edition = {arXiv prep},
title = {{Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World}}
}
@book{rasmussen2004a,
author = {Rasmussen, C E},
pages = {63--71},
publisher = {Springer},
title = {{“Gaussian processes in machine learning”. In: Advanced lectures on machine learning}}
}
@misc{graves2014a,
author = {Graves, A and Wayne, G and Danihelka, I},
edition = {arXiv prep},
title = {{Neural turing machines}}
}
@article{geurts2006a,
author = {Geurts, P and Ernst, D and Wehenkel, L},
journal = {Machine learning},
number = {1},
pages = {3--42},
title = {{Extremely randomized trees}},
volume = {63}
}
@misc{schaul2015a,
author = {Schaul, T and Quan, J and Antonoglou, I and Silver, D},
edition = {arXiv prep},
title = {{Prioritized Experience Replay}}
}
@misc{florensa2017a,
author = {Florensa, C and Duan, Y and Abbeel, P},
edition = {arXiv prep},
title = {{Stochastic neural networks for hierarchical reinforcement learning}}
}
@article{munos2002a,
author = {Munos, R and Moore, A},
journal = {Machine learning},
number = {2},
pages = {291--323},
title = {{Variable resolution discretization in optimal control}},
volume = {49}
}
@article{pedregosa2011a,
author = {Pedregosa, F and Varoquaux, G and Gramfort, A and Michel, V and Thirion, B and Grisel, O and Blondel, M and Prettenhofer, P and Weiss, R and Dubourg, V},
journal = {Journal of Machine Learning Research},
pages = {2825--2830},
title = {{Scikit-learn: Machine learning in Python}},
volume = {12}
}
@article{leffler2007a,
author = {Leffler, B R and Littman, M L and Edmunds, T},
journal = {In: AAAI},
pages = {572--577},
title = {{Efficient reinforcement learning with relocatable action models}},
volume = {7}
}
@article{FitzpatrickMC;GotelliNJ;Ellison2013,
abstract = {MaxEnt is one of the most widely used tools in ecology, biogeography, and evolution for modeling and mapping species distributions using presence-only occurrence records and associated environmental covariates. Despite its popularity, the exponential model implemented by MaxEnt does not directly estimate occurrence probability, the natural quantity of interest when modeling species distributions. Instead, MaxEnt generates an index of relative habitat suitability. MaxLike, a newly introduced maximum-likelihood technique, has been shown to overcome the problem of directly estimating the probability of occurrence using presence-only data. However, the performance and relative merits of MaxEnt and MaxLike remain largely untested, especially when modeling species with relatively few occurrence data that encompass only a portion of the geographic range of the species. Using geo- referenced occurrence records for six species of ants in New England, we provide comparisons of MaxEnt and MaxLike. We show that by most quantitative metrics, the performance of MaxLike exceeds that of MaxEnt, regardless of whether MaxEnt models account for sampling bias and include greater model complexity than implemented in MaxLike. More importantly, for most species, the relative suitability index estimated by MaxEnt oftenwas poorly correlated with the probability of occurrence estimated by MaxLike, suggesting that the two methods are estimating different quantities. For species distribution modeling, MaxLike, and similar models that are based on an explicit sampling process and that directly estimate probability of occurrence, should be considered as important alternatives to the widely-used MaxEnt framework.},
author = {{Fitzpatrick, MC; Gotelli, NJ; Ellison}, Am},
doi = {10.1890/es13-00066.1},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Fitzpatrick, MC Gotelli, NJ Ellison - 2013 - MaxEnt vs. MaxLike Empirical comparisons with ant species distributions(2).pdf:pdf},
isbn = {2150-8925},
issn = {2150-8925},
journal = {Ecosphere},
keywords = {ecological niche modeling,myrmecology,new england,occurrence probability,presence-only data,species distribution modeling},
number = {May},
pages = {1--15},
title = {{MaxEnt vs. MaxLike: Empirical comparisons with ant species distributions}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:MaxEnt+versus+MaxLike+:+empirical+comparisons+with+ant+species+distributions{\#}0},
volume = {4},
year = {2013}
}
@misc{silver2013a,
author = {Silver, D L and Yang, Q and Li, L},
pages = {5},
title = {{“Lifelong Machine Learning Systems: Beyond Learning Algorithms.” In: AAAI Spring Symposium: Lifelong Machine Learning}},
volume = {13}
}
@misc{kalchbrenner2016a,
author = {Kalchbrenner, N and v. d. Oord, A and Simonyan, K and Danihelka, I and Vinyals, O and Graves, A and Kavukcuoglu, K},
edition = {arXiv prep},
title = {{Video pixel networks}}
}
@book{silver2014a,
address = {In},
author = {Silver, D and Lever, G and Heess, N and Degris, T and Wierstra, D and Riedmiller, M},
publisher = {ICML},
title = {{Deterministic Policy Gradient Algorithms}}
}
@article{Radford,
author = {Radford, A and Narasimhan, K and https://s3-us-west-2 {\ldots}, T Salimans - URL and undefined 2018 and https://s3-us-west-2 {\ldots}, T Salimans - U R L and undefined 2018},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Radford et al. - Unknown - Improving language understanding by generative pre-training.pdf:pdf},
journal = {cs.ubc.ca},
title = {{Improving language understanding by generative pre-training}},
url = {https://www.cs.ubc.ca/{~}amuham01/LING530/papers/radford2018improving.pdf}
}
@inproceedings{abbeel2004a,
author = {Abbeel, P and Ng, A Y},
booktitle = {Proceedings of the twenty-first international conference on Machine learning. ACM},
title = {{Apprenticeship learning via inverse reinforcement learning}}
}
@article{Delage2010,
abstract = {Markov decision processes are an effective tool in modeling decision making in uncertain dynamic environments. Because the parameters of these models typically are estimated from data or learned from experience, it is not surprising that the actual performance of a chosen strategy often differs significantly from the designer's initial expectations due to unavoidable modeling ambiguity. In this paper, we present a set of percentile criteria that are conceptually natural and representative of the trade-off between optimistic and pessimistic views of the question. We study the use of these criteria under different forms of uncertainty for both the rewards and the transitions. Some forms are shown to be efficiently solvable and others highly intractable. In each case, we outline solution concepts that take parametric uncertainty into account in the process of decision making. {\textcopyright} 2010 INFORMS.},
author = {Delage, Erick and Mannor, Shie},
doi = {10.1287/opre.1080.0685},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Delage, Mannor - 2010 - Percentile optimization for Markov decision processes with parameter uncertainty.pdf:pdf},
issn = {0030364X},
journal = {Operations Research},
keywords = {Chance-constrained optimization,Finite state,Markov decision processes,Parameter uncertainty,Stochastic model applications,Stochastic programming,Value at risk},
month = {jan},
number = {1},
pages = {203--213},
publisher = {INFORMS},
title = {{Percentile optimization for Markov decision processes with parameter uncertainty}},
volume = {58},
year = {2010}
}
@article{sandve2013a,
author = {Sandve, G K and Nekrutenko, A and Taylor, J and Hovig, E},
journal = {PLoS computational biology},
number = {10},
pages = {1003285},
title = {{Ten simple rules for reproducible computational research}},
volume = {9}
}
@book{fukushima1982a,
author = {Fukushima, K and Miyake, S},
pages = {267--285},
publisher = {Springer},
title = {{“Neocognitron: A self-organizing neural network model for a mechanism of visual pattern recognition”. In: Competition and cooperation in neural nets}}
}
@misc{wang-a,
author = {Wang, J X and Kurth-Nelson, Z and Tirumala, D and Soyer, H and Leibo, J Z and Munos, R and Blundell, C and Kumaran, D and M.},
edition = {arXiv prep},
title = {{Botvinick. 2016a. “Learning to reinforcement learn”}}
}
@misc{silver2016b,
author = {Silver, D and van Hasselt, H and Hessel, M and Schaul, T and Guez, A and Harley, T and Dulac-Arnold, G and Reichert, D and Rabinowitz, N and Barreto, A},
edition = {arXiv prep},
title = {{The predictron: End-to-end learning and planning}}
}
@inproceedings{jiang-a,
author = {{Jiang N.}, A Kulesza and 2015a, S Singh.},
booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
pages = {179--188},
title = {{Abstraction selection in model-based reinforcement learning}},
volume = {15}
}
@misc{lecun1995a,
address = {3361(10},
author = {LeCun, Y and Bengio, Y},
title = {{“Convolutional networks for images, speech, and time series”. The handbook of brain theory and neural networks}}
}
@misc{chen2015a,
author = {Chen, T and Goodfellow, I and Shlens, J},
edition = {arXiv prep},
title = {{Net2net: Accelerating learning via knowledge transfer}}
}
@inproceedings{Strehl2004,
abstract = {This paper takes an empirical approach to evaluating three model-based reinforcement-learning methods. All methods intend to speed the learning process by mixing exploitation of learned knowledge with exploration of possibly promising alternatives. We consider $\epsilon$-greedy exploration, which is computationally cheap and popular, but unfocused in its exploration effort; R-Max exploration, a simplification of an exploration scheme that comes with a theoretical guarantee of efficiency; and a well-grounded approach, model-based interval estimation, that better integrates exploration and exploitation. Our experiments indicate that effective exploration can result in dramatic improvements in the observed rate of learning. {\textcopyright} 2004 IEEE.},
author = {Strehl, Alexander L. and Littman, Michael L.},
booktitle = {Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI},
doi = {10.1109/ICTAI.2004.28},
isbn = {076952236X},
issn = {10823409},
title = {{An empirical evaluation of interval estimation for Markov decision processes}},
year = {2004}
}
@incollection{boyan1995a,
author = {Boyan, J A and Moore, A W},
booktitle = {Advances in neural information processing systems},
pages = {369--376},
title = {{Generalization in reinforcement learning: Safely approximating the value function}}
}
@article{salge2014a,
author = {Salge, C and Glackin, C and Polani, D},
journal = {Entropy},
number = {5},
pages = {2789--2819},
title = {{Changing the environment based on empowerment as intrinsic motivation}},
volume = {16}
}
@article{schmidhuber2015a,
author = {Schmidhuber, J},
journal = {Neural Networks},
pages = {85--117},
title = {{Deep learning in neural networks: An overview}},
volume = {61}
}
@article{Silver2017,
abstract = {Starting from zero knowledge and without human data, AlphaGo Zero was able to teach itself to play Go and to develop novel strategies that provide new insights into the oldest of games.},
annote = {Solving Go},
author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature24270},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Silver et al. - 2017 - Mastering the game of Go without human knowledge(2).pdf:pdf},
issn = {0028-0836},
journal = {Nature},
keywords = {Computational science,Computer science,Reward},
month = {oct},
number = {7676},
pages = {354--359},
publisher = {Nature Publishing Group},
title = {{Mastering the game of Go without human knowledge}},
url = {http://www.nature.com/doifinder/10.1038/nature24270},
volume = {550},
year = {2017}
}
@misc{gandhi2017a,
author = {Gandhi, D and Pinto, L and Gupta, A},
edition = {arXiv prep},
title = {{Learning to Fly by Crashing}}
}
@article{niv2009a,
author = {Niv, Y},
journal = {Journal of Mathematical Psychology},
number = {3},
pages = {139--154},
title = {{Reinforcement learning in the brain}},
volume = {53}
}
@article{bellemare2013a,
author = {Bellemare, M G and Naddaf, Y and Veness, J and Bowling, M},
journal = {Journal of Artificial Intelligence Research},
pages = {253--279},
title = {{The Arcade Learning Environment: An evaluation platform for general agents.}},
volume = {47}
}
@misc{santoro2017a,
author = {Santoro, A and Raposo, D and Barrett, D G and Malinowski, M and Pascanu, R and Battaglia, P and Lillicrap, T},
edition = {arXiv prep},
title = {{A simple neural network module for relational reasoning}}
}
@misc{vaswani2017a,
author = {Vaswani, A and Shazeer, N and Parmar, N and Uszkoreit, J and Jones, L and Gomez, A N and Kaiser, L and Polosukhin, I},
edition = {arXiv prep},
title = {{Attention Is All You Need}}
}
@misc{guo2017a,
author = {Guo, Z D and Brunskill, E},
edition = {arXiv prep},
title = {{Sample efficient feature selection for factored mdps}}
}
@article{Mnih2015,
abstract = {An artificial agent is developed that learns to play a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Mnih et al. - 2015 - Human-level control through deep reinforcement learning(3).pdf:pdf},
issn = {0028-0836},
journal = {Nature},
keywords = {Computer science},
month = {feb},
number = {7540},
pages = {529--533},
publisher = {Nature Publishing Group},
title = {{Human-level control through deep reinforcement learning}},
url = {http://www.nature.com/articles/nature14236},
volume = {5aaIA18},
year = {2015}
}
@inproceedings{jiang-b,
booktitle = {Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems. International Foundation for},
title = {{The Dependence of Effective Planning Horizon on Model Accuracy}}
}
@misc{ranzato2015a,
author = {Ranzato, M and Chopra, S and Auli, M and Zaremba, W},
edition = {arXiv prep},
title = {{Sequence level training with recurrent neural networks}}
}
@techreport{Sutton1998,
abstract = {This introductory textbook on reinforcement learning is targeted toward engineers and scientists in artificial intelligence, operations research, neural networks, and control systems, and we hope it will also be of interest to psychologists and neuroscientists. If you would like to order a copy of the book, or if you are qualified instructor and would like to see an examination copy, please see the MIT Press home page for this book. Or you might be interested in the reviews at amazon.com. There is also a Japanese translation available. The table of contents of the book is given below, with associated HTML. The HTML version has a number of presentation problems, and its text is slightly different from the real book, but it may be useful for some purposes. q Preface Part I: The Problem q 1 Introduction r 1.1 Reinforcement Learning r 1.2 Examples r 1.3 Elements of Reinforcement Learning r 1.4 An Extended Example: Tic-Tac-Toe r 1.5 Summary r 1.6 History of Reinforcement Learning r 1.7 Bibliographical Remarks q 2 Evaluative Feedback r 2.1 An n-armed Bandit Problem r 2.2 Action-Value Methods r 2.3 Softmax Action Selection r 2.4 Evaluation versus Instruction r 2.5 Incremental Implementation r 2.6 Tracking a Nonstationary Problem r 2.7 Optimistic Initial Values r 2.8 Reinforcement Comparison r 2.9 Pursuit Methods r 2.10 Associative Search r 2.11 Conclusion r 2.12 Bibliographical and Historical Remarks q 3 The Reinforcement Learning Problem r 3.1 The Agent-Environment Interface r 3.2 Goals and Rewards r 3.3 Returns r 3.4 A Unified Notation for Episodic and Continual Tasks r 3.5 The Markov Property r 3.6 Markov Decision Processes r 3.7 Value Functions r 3.8 Optimal Value Functions r 3.9 Optimality and Approximation r 3.10 Summary r 3.11 Bibliographical and Historical Remarks Part II: Elementary Methods},
archivePrefix = {arXiv},
arxivId = {1603.02199},
author = {Sutton, Richard S. and Barto, Andrew G.},
booktitle = {MIT press},
doi = {10.1109/TNN.1998.712192},
eprint = {1603.02199},
isbn = {0262193981},
issn = {10459227},
keywords = {reinforcement learning theory},
pmid = {18255791},
title = {{Sutton {\&} Barto Book: Reinforcement Learning: An Introduction}},
year = {1998}
}
@misc{sutton1984a,
author = {Sutton, R S},
title = {{Temporal credit assignment in reinforcement learning}}
}
@article{campbell2002a,
author = {Campbell, M and Hoane, A J and Hsu, F.-h},
journal = {Artificial},
number = {1-2},
pages = {57--83},
title = {{Deep blue}},
volume = {134}
}
@article{schmidhuber2010a,
author = {Schmidhuber, J},
journal = {IEEE Transactions on Autonomous Mental Development},
number = {3},
pages = {230--247},
title = {{Formal theory of creativity, fun, and intrinsic motivation (1990–2010)}},
volume = {2}
}
@misc{bojarski2016a,
author = {Bojarski, M and Testa, D.Del and Dworakowski, D and Firner, B and Flepp, B and Goyal, P and Jackel, L D and Monfort, M and Muller, U and Zhang, J},
edition = {arXiv prep},
title = {{End to end learning for self-driving cars}}
}
@inproceedings{Fernaandez2000,
author = {Fern{\'{a}}andez, Fernando and Borrajo, Daniel},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/3-540-45327-x_24},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Fern{\'{a}}andez, Borrajo - 2000 - VQQL. Applying vector quantization to reinforcement learning.pdf:pdf},
isbn = {9783540410430},
issn = {16113349},
pages = {292--303},
publisher = {Springer Verlag},
title = {{VQQL. Applying vector quantization to reinforcement learning}},
volume = {1856},
year = {2000}
}
@article{tsitsiklis1997a,
author = {Tsitsiklis, J N and Roy, B.Van},
journal = {Automatic Control, IEEE Transactions on},
number = {5},
pages = {674--690},
title = {{An analysis of temporaldifference learning with function approximation}},
volume = {42}
}
@article{sutton1988a,
author = {Sutton, R S},
journal = {Machine learning},
number = {1},
pages = {9--44},
title = {{Learning to predict by the methods of temporal differences}},
volume = {3}
}
@article{lecun1998a,
author = {LeCun, Yann and Bottou, L{\'{e}}on and Bengio, Yoshua and Haffner, Patrick and Others},
journal = {Proceedings of the IEEE},
number = {11},
pages = {2278--2324},
publisher = {Taipei, Taiwan},
title = {{Gradient-based learning applied to document recognition}},
volume = {86},
year = {1998}
}
@article{Chatelin1982,
author = {Chatelin, Fran{\c{c}}oise and Miranker, Willard L.},
doi = {10.1016/0024-3795(82)90242-7},
file = {:Users/shi-on/Library/Application Support/Mendeley Desktop/Downloaded/Chatelin, Miranker - 1982 - Acceleration by aggregation of successive approximation methods.pdf:pdf},
issn = {00243795},
journal = {Linear Algebra and Its Applications},
month = {mar},
number = {C},
pages = {17--47},
publisher = {North-Holland},
title = {{Acceleration by aggregation of successive approximation methods}},
volume = {43},
year = {1982}
}
